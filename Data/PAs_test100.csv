PN,_ISD,_TTL,_ABST,_CPC,_IPC,b_cits,f_cits,_ACLM
"10,089,203","
     October 2, 2018
","Artificial intelligence based health management of host system
"," This disclosure relates generally to health management, and more
     particularly to a method and system for artificial intelligence based
     diagnostic and prognostic health management of host systems. In an
     embodiment, the system includes a memory to store instructions, and a
     neural network controller coupled to the memory. The neural network
     controller is configured by the instructions to monitor a plurality of
     unique patterns generated in real-time. The plurality of system
     parameters is indicative of a system-level performance of the host
     system. The neural network controller is configured by the instructions
     to compare the plurality of unique patterns with a plurality of
     predetermined patterns corresponding to the plurality of system
     parameters to detect potential anomalies in the host system and one or
     more subsystems of the plurality of subsystems, where the one or more
     subsystems are responsible for contributing to the potential anomalies in
     the host system.
",G05B 23/0254 (20130101); G06F 11/2257 (20130101); G06N 20/00 (20190101); G06N 3/088 (20130101),G06F 11/00 (20060101); G06F 11/22 (20060101); G05B 23/02 (20060101); G06N 99/00 (20100101); G06N 3/08 (20060101),"[['\n5402521', '\nMarch 1995'], ['\n5544308', '\nAugust 1996'], ['\n5919267', '\nJuly 1999'], ['\n6105149', '\nAugust 2000'], ['\n2002/0066054', '\nMay 2002'], ['\n2006/0230313', '\nOctober 2006'], ['\n2009/0216393', '\nAugust 2009'], ['\n2013/0060524', '\nMarch 2013'], ['\n2014/0039834', '\nFebruary 2014']]","[5, '11,315,027', '11,248,989', '11,227,209', '11,132,245', '10,719,772']"," What is claimed is:  1.  A system for artificial intelligence based diagnosis and prognosis of a host system having multiple subsystems, the system comprising: a memory to store instructions and
a plurality of predetermined patterns;  and a neural network controller coupled to the memory, wherein the neural network controller is configured by the instructions to: monitor a plurality of unique patterns generated in real-time corresponding to a
subset of multiple system parameters of the host system, wherein the system parameters comprise at least one of one or more of input parameters, control parameters, feedback parameters, and output parameters, wherein the plurality of unique patterns are
indicative of the system-level performance of the host system in real-time, and each of the unique patterns is unique for a respective subsystem of the host system and only the subset of system parameters are capable of determining one or more potential
anomalies in the host system, and wherein the unique patterns enable identification of faults associated with one or more subsystems without depending on physical sensors for detecting fault in the subsystem, and wherein the neural network controller is
configured to preconfigure the plurality of predetermined patterns by acquiring training data comprising the system-level performance of the host system under normal and abnormal working conditions of the one or more subsystems, extract a plurality of
feature vectors from the training data, wherein the plurality of feature vectors exhibiting the plurality of predetermined patterns indicative of the one or more potential anomalies in the host system;  detect the plurality of unique patterns generated
in real-time based on the host system responses for the normal and abnormal working conditions due to the one or more subsystem failure, wherein the data acquisition enables detection and identification of abnormal response by enabling distinguishing the
normal and abnormal performance of the host system based on domain experience and training associated with operational conditions to further identify a failed component/subsystem of the host system that leads to abnormality in the system response;  and
compare the plurality of unique patterns with the plurality of predetermined patterns corresponding to the subset of multiple system parameters to detect one or more potential anomalies in the host system and at least one faulty subsystem of the
plurality of subsystems based on the comparison, the at least one faulty subsystem is responsible for contributing to the one or more potential anomalies in the host system.
 2.  The system of claim 1, wherein the neural network controller is further configured by the instructions to train a neural network model based on the plurality of feature vectors to classify the one or more potential anomalies with the at
least one faulty subsystem responsible for contributing to the one or more potential anomalies.
 3.  The system of claim 1, wherein to acquire the training data, the neural network controller is further configured by the instructions to: simulate the normal working condition and the plurality of abnormal working conditions of the plurality
of subsystems;  and generate the system-level performance under the normal and the plurality of abnormal working conditions.
 4.  The system of claim 3, wherein to simulate the normal and the plurality of abnormal working conditions, the neural network controller is further configured by the instructions to: capture the system-level performance from a reference model
and a fault introduced model continuously in a specified window of time scale, wherein the reference model comprises modeling of the normal working condition and the fault introduced model comprises modeling of the plurality of abnormal working
conditions of the plurality of subsystems;  and preprocess the system-level responses to remove trends in training data.
 5.  The system of claim 1, wherein the host system comprises one of an aircraft system, an automotive system, a turbine system and an engine system.
 6.  A processor-implemented method for artificial intelligence based diagnosis and prognosis of a host system having multiple subsystems, the method comprising: monitoring, by a neural network controller, a plurality of unique patterns generated
in real-time corresponding to a subset of multiple system parameters of the host system, wherein the system parameters comprise at least one of one or more of input parameters, control parameters, feedback parameters, and output parameters, the plurality
of unique patterns are indicative of the system-level performance of the host system in real-time, and each of the unique patterns is unique for a respective subsystem of the host system and only the subset of system parameters are capable of determining
one or more potential anomalies in the host system, and wherein the unique patterns enable identification of faults associated with one or more subsystems without depending on physical sensors for detecting fault in the subsystem, and wherein the neural
network controller is configured to preconfigure the plurality of predetermined patterns by acquiring training data comprising the system-level performance of the host system under normal and abnormal working conditions of the one or more subsystems; 
and extract a plurality of feature vectors from the training data, wherein the plurality of feature vectors exhibiting the plurality of predetermined patterns indicative of the one or more potential anomalies in the host system, and wherein the data
acquisition enables detection and identification of abnormal working condition by enabling distinguishing the normal and abnormal performance of the host system based on domain experience and training associated with operational conditions to further
identify a failed component/subsystem of the host system that leads to abnormality in the system response;  detect the plurality of unique patterns generated in real-time based on host system responses for various normal and abnormal performance
scenarios due to one or more sub-system failures;  and comparing, by the neural network controller, the plurality of unique patterns with the plurality of predetermined patterns corresponding to the subset of multiple system parameters for detecting, by
the neural network controller, one or more potential anomalies in the host system and at least one faulty subsystem of the plurality of subsystems based on the comparison, the at least one faulty subsystem responsible for contributing to the one or more
potential anomalies in the host system.
 7.  The method of claim 6, further comprising training a neural network model based on the plurality of feature vectors to classify the one or more potential anomalies with the at least one faulty subsystem responsible for contributing to the
one or more potential anomalies.
 8.  The method of claim 6, wherein acquiring the training data comprises: simulating the normal working condition and the plurality of abnormal working conditions of the plurality of subsystems;  and generating the system-level performance under
the normal and the plurality of abnormal working conditions.
 9.  The method of claim 8, wherein simulating the normal working condition of the plurality of sub-systems and the plurality of abnormal working conditions of the one or more subsystems comprises: capturing the system-level performance from a
reference model and a fault introduced model continuously in a specified window of time scale, wherein the reference model comprises modeling of the normal working condition and the fault introduced model comprises modeling of the plurality of abnormal
working conditions of the plurality of subsystems;  and pre-processing the system-level responses to remove trends in training data.
 10.  The method of claim 6, wherein the host system comprises one of an aircraft system, an automotive system, a turbine system and an engine system.
 11.  A non-transitory computer-readable medium having embodied thereon a computer program for executing a method for artificial intelligence based diagnosis and prognosis of a host system, the method comprising: monitoring, by a neural network
controller, a plurality of unique patterns generated in real-time, the plurality of unique patterns corresponding to a subset of multiple system parameters of the host system, wherein the system parameters comprise at least one of one or more of input
parameters, control parameters, feedback parameters, and output parameters, the plurality of unique patterns are indicative of the system-level performance of the host system in real-time, and each of the unique patterns is unique for a respective
subsystem of the host system and only the subset of system parameters are capable of determining one or more potential anomalies in the host system, and wherein the unique patterns enable identification of faults associated with one or more subsystems
without depending on physical sensors for detecting fault in the subsystem, and wherein the neural network controller is configured to preconfigure the plurality of predetermined patterns by acquiring training data comprising the system-level performance
of the host system under normal and abnormal working conditions of the one or more subsystems;  and extract a plurality of feature vectors from the training data, wherein the plurality of feature vectors exhibiting the plurality of predetermined patterns
indicative of the one or more potential anomalies in the host system, and wherein the data acquisition enables detection and identification of abnormal working condition by enabling distinguishing the normal and abnormal performance of the host system
based on domain experience and training associated with operational conditions to further identify a failed component/subsystem of the host system that leads to abnormality in the system response;  detect the plurality of unique patterns generated in
real-time based on the host system responses for various normal and abnormal performance scenarios due to one or more sub-system failures;  comparing, by a neural network controller, the plurality of unique patterns with the plurality of predetermined
patterns corresponding to the subset of multiple system parameters;  and detecting, by a neural network controller, one or more potential anomalies in the host system and at least one faulty subsystem of the plurality of subsystems based on the
comparison, the at least one faulty subsystem responsible for contributing to the one or more potential anomalies in the host system.  "
"10,096,169","
     October 9, 2018
","System for the augmented assessment of virtual insertion opportunities
"," A system for the augmented assessment of digital media streams for
     virtual less than insertion opportunities is disclosed. A digital media
     stream is automatically decomposed, using a programmed digital processor,
     into one or more candidate-clips have a predetermined minimum length, and
     no internal shot transitions. These candidate-clips are then examined,
     and the ones deemed suitable for virtual insertion use, are classified as
     viable-clips and stored in a digital database. The artificial
     intelligence techniques of machine learning and deep learning are then
     used to further classify the viable-clips according to their virtual
     insertion related attributes that may be attractive to advertisers, such
     as scene context, emotional tone and contained characters. A value is
     then assigned to the viable-clips, dependent on their insertion related
     attributes, and the overlap of those attributes with client requested
     requirements.
",H04L 65/762 (20220501); G06N 20/00 (20190101); G06T 19/006 (20130101); G06F 16/285 (20190101); G06T 19/20 (20130101); H04N 5/2723 (20130101),G06T 19/00 (20110101); H04N 5/272 (20060101); G06F 15/18 (20060101); G06F 17/30 (20060101); H04L 29/06 (20060101); G06T 19/20 (20110101),"[['\n7444659', '\nOctober 2008'], ['\n9351032', '\nMay 2016'], ['\n9584736', '\nFebruary 2017'], ['\n9621929', '\nApril 2017']]","[3, '11,259,091', '11,037,348', '10,878,461']"," The invention claimed is:  1.  A system for the augmented assessment of digital media streams for virtual image insertion opportunities, comprising: a digital media stream comprising one or more
digital video images;  a candidate-clip database comprising a digital data storage unit;  processing said digital media stream using a programmed, digital computation device to automatically obtain one or more candidate-clips, said candidate-clip
comprising a contiguous portion of said digital media stream having a duration of at least 30 of said digital video images, and no internal shot transitions, wherein, said shot transition comprises a change of 5% or more in a sum of image pixel values
between consecutive digital video images;  and storing said candidate-clips in said candidate-clip database.
 2.  The system of claim 1, further comprising, processing one or more of said candidate-clips using said programmed, digital computation device to automatically obtain one or more viable-clips, said viable-clip comprising said candidate-clip
comprising a viable insertion region;  and storing said viable-clips, tagged with coordinates indicative of a geometric location of said viable insertion regions, in a viable-clip database.
 3.  The system of claim 2, wherein, said viable insertion region comprises a contiguous region of a one of said digital video images of said viable clip in which all pixels have pixel color values within a predetermined range of color values,
said contiguous region being greater than a predetermined minimum area.
 4.  The system of claim 3, wherein, said predetermined range of color values is less than or equal to +/-1% of a mean color value of said pixels, and said predetermined minimum size is an area that is greater than or equal to 5% of a total area
of said digital video image.
 5.  The system of claim 2, wherein said viable insertion region comprises a contiguous region, unoccluded by foreground objects, of a one of said digital video images of said viable clip that is rectangular in shape, has an area greater that
than 5% of the total area of the clip image, and in which the rectangle has an aspect ratio less than 4:1.
 6.  The system of claim 3, further comprising automatically determining one or more virtual insertion related attributes of one of said viable-clips.
 7.  The system of claim 6, wherein said virtual insertion related attribute is a perspective perceived, surface orientation, and wherein, automatically determining said perspective perceived, surface orientation further comprises, automatically
obtaining one or more edges of said viable insertion region;  automatically using said detected edges to define a perspective perceived, plane of said viable insertion region;  and automatically using said of said perspective perceived, plane to define a
normal vector, normal to said perspective perceived, plane.
 8.  The system of claim 6, wherein, said insertion related attribute is a scene context, and, wherein, automatically determining said scene context of said viable-clip comprises: a reference database of human evaluated training video images
representative of one or more scene contexts;  training a machine learning system on said reference database;  and using said trained machine learning system to determine a scene context of one or more of said viable-clips.
 9.  The system of claim 8, wherein, said reference database further comprises a representative object database of images of objects representative of one or more scene contexts;  and, wherein, said trained machine learning system processes one
or more digital images of said viable-clips to identify one or more representative objects present in said digital image;  and, wherein, said programmed, digital computation device uses one or more of said identified representative objects to determine
said scene context.
 10.  The system of claim 9, wherein, said scene context is one of a living room scene, a kitchen scene, a bathroom scene, a bedroom scene, an office scene, a restaurant scene, and a bar scene.
 11.  The system of claim 6, wherein, said insertion related attribute is an emotional tone, and, wherein, automatically determining said emotional tone of said viable-clip comprises: an audio track associated with said viable-clip;  a reference
audio database of human evaluated training audio tracks representative of one or more emotional tones;  training a machine learning system on said reference audio database;  and using said trained machine learning system to determine said emotional tone
of one or more of said viable-clips.
 12.  The system of claim 11 wherein said emotional tone is one of a high intensity moment, a suspenseful moment, a surprising moment, a fearful moment, a cathartic moment, a joyful moment, and a sad moment, or some combination thereof.
 13.  The system of claim 6, wherein, said insertion related attribute is a contained character, and, wherein, automatically determining said contained character of said viable-clip comprises: an audio track associated with said viable-clip;  a
reference audio database of human evaluated training audio tracks representative of one or more contained characters;  training a machine learning system on said reference audio database;  and using said trained machine learning system to determine said
contained character of one or more of said viable-clips.
 14.  The system of claim 6;  further comprising: automatically determining a potential value one of said viable-clips;  and wherein, determining said potential value comprises: automatically determining an area, a duration, and one or more of
said virtual insertion related attributes of said viable insertion regions in said viable clip;  automatically assigning a numerical weight to each of said virtual insertion related attributes associated with said viable-clip;  and assigning a single
numerical potential value to said viable clip based on a combination of said area, said duration, and said numerical weights of said virtual insertion related attributes.
 15.  The system of claim 14, wherein, said virtual insertion related attributes comprise one of a scene context, a perspective perceived surface orientation, an emotional tone, and a contained character, or some combination thereof.
 16.  The system of claim 14, further comprising, automatically determining a market value of said viable-clip, said market value comprising a numerical value reflective of correspondence between a client requested list of criteria and a said
size, said duration and said virtual insertion related attributes of said viable clip.  "
"10,108,601","
     October 23, 2018
","Method and system for presenting personalized content
"," Content personalized for a user is presented. Particularly, content is
     personalized and presented to a user in a more cognitive and
     user-understandable manner to improve the impact and the effectiveness on
     the user. The system utilizes artificial intelligence to analyze and
     categorize the content and thereby learns to discover the core concept of
     the content and any patterns involved. The system also understands the
     user's interests by capturing the preferred presentation formats and the
     user's past knowledge. The system maps the categorized content and user's
     interests and personalizes the content and renders into user preferred
     presentation type and format. The system supplements the main
     presentation type with additional related content. The system is capable
     of continuously monitoring the user activities to understand the
     effectiveness of the presented content type and formats, and feedback is
     exploited to continuous improvement of presented content and presentation
     type and formats.
",G06F 16/9535 (20190101); G06F 40/103 (20200101); G06F 40/258 (20200101),G06F 17/30 (20060101); G06F 17/27 (20060101); G06F 17/21 (20060101),"[['\n7689682', '\nMarch 2010'], ['\n8156113', '\nApril 2012'], ['\n8602793', '\nDecember 2013'], ['\n8930204', '\nJanuary 2015'], ['\n8942973', '\nJanuary 2015'], ['\n9135245', '\nSeptember 2015'], ['\n9141624', '\nSeptember 2015'], ['\n9235844', '\nJanuary 2016'], ['\n2003/0061611', '\nMarch 2003'], ['\n2003/0217328', '\nNovember 2003'], ['\n2003/0237093', '\nDecember 2003'], ['\n2004/0003097', '\nJanuary 2004'], ['\n2005/0022239', '\nJanuary 2005'], ['\n2007/0061712', '\nMarch 2007'], ['\n2008/0092182', '\nApril 2008'], ['\n2008/0109285', '\nMay 2008'], ['\n2008/0172274', '\nJuly 2008'], ['\n2008/0189099', '\nAugust 2008'], ['\n2009/0083129', '\nMarch 2009'], ['\n2009/0150400', '\nJune 2009'], ['\n2009/0187593', '\nJuly 2009'], ['\n2011/0113041', '\nMay 2011'], ['\n2011/0258049', '\nOctober 2011'], ['\n2012/0123992', '\nMay 2012'], ['\n2013/0290233', '\nOctober 2013'], ['\n2014/0100844', '\nApril 2014'], ['\n2014/0122595', '\nMay 2014'], ['\n2014/0164507', '\nJune 2014'], ['\n2014/0180760', '\nJune 2014'], ['\n2015/0070585', '\nMarch 2015'], ['\n2015/0081611', '\nMarch 2015'], ['\n2016/0132608', '\nMay 2016'], ['\n2016/0171559', '\nJune 2016'], ['\n2016/0182658', '\nJune 2016'], ['\n2016/0196490', '\nJuly 2016'], ['\n2016/0277244', '\nSeptember 2016'], ['\n2017/0091886', '\nMarch 2017'], ['\n2017/0279867', '\nSeptember 2017'], ['\n2017/0323348', '\nNovember 2017']]",[0]," What is claimed is:  1.  A computer-implemented method for presenting content personalized for a user comprising: analyzing and categorizing the content by a server computer using artificial
intelligence and storing the categorized content on a memory of the server computer;  capturing explicit preferences of a user comprising user content interests, preferred content type, preferred presentation type, preferred language, and preferred
presentation format;  storing the captured explicit preferences of the user on the memory of the server computer in an internal representation of the user's preferences;  storing an internal representation of prior knowledge of the user;  determining and
storing a plurality of implicit preferences of the user;  mapping the categorized content to the internal representation of the user's preferences and the internal representation of prior knowledge of the user to discover the content, relevant to user,
by the server computer, from the categorized content;  and processing the relevant content into the preferred presentation type and format at the server computer and presenting to the user on a user device;  wherein at least one conflict between a
conflicting explicit preference and a conflicting implicit preference is resolved by giving precedence to the conflicting explicit preference.
 2.  The method according to claim 1, wherein analyzing and categorizing the content is carried out by Natural Language Processing (NLP) techniques at the server computer.
 3.  The method according to claim 2, wherein Natural Language Processing (NLP) techniques include at least one from the group consisting of: semantic analysis, spatial analysis, chronological analysis, conceptual analysis, quantitative analysis,
and pattern analysis.
 4.  The method according to claim 1, further comprising: capturing at least one among the user's education, working field, profile input by the user, user's history, or past contents presented to the user.
 5.  The method according to claim 1, wherein mapping the categorized content to user's interests comprises comparing the content concepts with the user content interests.
 6.  The method according to claim 1, wherein mapping the categorized content to the internal representation of prior knowledge of the user comprises determining similarities and differences with the internal representation of prior knowledge of
the user.
 7.  The method according to claim 1, wherein the presentation type comprises at least one among the Text data, Graphical representation, Images, Concept map, Flow charts, Text to speech, Bulleted list, display of trends, or patterns in the
content.
 8.  The method according to claim 1, further comprising: translating the relevant content into at least one user preferred language for presenting to the user on the user device.
 9.  The method according to claim 1, further comprising: presenting analogies and compare and contrast information on the user device based on the internal representation of prior knowledge of the user.
 10.  The method according to claim 1, further comprising: monitoring user's activity by an analytics module during presentation of content, collecting the monitored user's activity data and user's feedback through user device on effectiveness of
presented content type, presentation type and formats and updating the user content interests on the memory of the server computer based on the activity data and the feedback collected.
 11.  The method according to claim 10, wherein the monitored user's activity data comprises at least one selected from the group consisting of: click events, download events, usability analysis, reading time and scroll data.
 12.  The method according to claim 10, wherein the user's feedback comprises at least one selected from the group consisting of: user rating, answers for questions on content type, presentation type and formats output through the user device.
 13.  The method according to claim 10, further comprising: presenting on the user device a most relevant content with a most preferred presentation type and formats based on the updated user content interests on the memory of the server
computer.
 14.  The method according to claim 1, further comprising: collecting the content related to a core concept of content, presented to the user, from plurality of web sources;  presenting the content related to the core concept of content,
presented to the user, during content presentation on the user device.
 15.  The method according to claim 14, where in the content related to the core concept of content, presented to the user, comprises links to further readable contents, audio visual contents, social networking platforms and other content to
attract the user's attention.
 16.  The method according to claim 1, further comprising: rendering the content, suitable to present on the user device, by a content presentation module.
 17.  The method according to claim 16, wherein the user device includes a personal computer (PC), a tablet PC, a set-top box (STB), a personal digital assistant (PDA), a mobile device, a hand held device, a wearable device, a global positioning
satellite (GPS) device, a palmtop computer, a laptop computer, a desktop computer, a personal trusted device, a web appliance, and any other communication device.
 18.  A system for presenting content personalized for a user comprising: a processor;  memory coupled to the processor;  a learning module of a server computer for analyzing and categorizing the content using artificial intelligence and storing
the categorized content on a memory of a server computer;  a training module of the server computer for capturing explicit preferences of a user comprising user content interests, preferred content type, preferred presentation type, preferred language
and preferred presentation format;  storing the captured explicit preferences of the user on the memory of the server computer in an internal representation of the user's preferences;  storing an internal representation of prior knowledge of the user; 
and determining a plurality of implicit preferences of the user, wherein the implicit preferences comprise geography, language, or media type;  a content processing module of the server computer for mapping the categorized content to the internal
representation of the user's preferences and the internal representation of prior knowledge of the user to discover the content relevant to the user, from the categorized content, and to discover the preferred presentation type and formats;  and
processing the content relevant to the user into the preferred presentation type and format for presenting to the user on a user device;  wherein at least one conflict between a conflicting explicit preference and a conflicting implicit preference is
resolved by giving precedence to the conflicting explicit preference;  and a content presentation module of the server computer for rendering the relevant content in the preferred presentation type and format on the user device.
 19.  The system according to claim 18, wherein the learning module includes Natural Language Processing Engine.
 20.  The system according to claim 19 wherein the Natural Language Processing Engine includes at least one from a semantic analyzer, a spatial analyzer, a chronological analyzer, conceptual analyzer, quantitative analyzer, pattern analyzer,
theme analyzer.
 21.  The system according to claim 18, further comprising: capturing at least one among the user's education, working field, profile input by the user, user's history, past contents presented to the user.
 22.  The system according to claim 18, wherein mapping the categorized content to user's interests comprises comparing content concepts with the user content's interests.
 23.  The system according to claim 18, wherein mapping the categorized content to the internal representation of prior knowledge of the user comprises determining similarities and differences with the internal representation of prior knowledge
of the user.
 24.  The system according to claim 18, wherein the preferred presentation type comprises at least one selected from the group consisting of: Text data, Graphical representation, Images, Concept map, Flow charts, Text to speech, Bulleted list,
display of trends, and patterns in the content.
 25.  The system according to claim 18, further comprising: language translators for translating the relevant content into at least one user preferred language for presenting to the user.
 26.  The system according to claim 18, further comprising: presenting analogies and compare and contrast information on the user device based on the internal representation of prior knowledge of the user.
 27.  The system according to claim 18, further comprising: an analytics module for monitoring user's activity during presentation of content, collecting monitored user's activity data and user's feedback through the user device on the
effectiveness of a presented content type, presentation type and format and updating the user content interests on the memory of the server computer based on the activity data and the feedback collected.
 28.  The system according to claim 27, wherein the monitored user's activity data comprises at least one selected from the group consisting of: click events, download events, usability analysis, reading time and scroll data.
 29.  The system according to claim 27, wherein the user's feedback comprises at least one selected from the group consisting of: user rating, answers for questions on content type, presentation type and formats.
 30.  The system according to claim 27, wherein the content presentation module presents on the user device most relevant content with a most preferred presentation type and formats, based on the updated user content interests.
 31.  The system according to claim 18, further comprising: a searching module of the server computer for collecting the content related to core concept of content, presented to the user, from plurality of web sources for further presenting to
the user during content presentation on the user device.
 32.  The system according to claim 31, wherein the content related to the core concept of content, presented to the user, comprises links to further readable contents, audio visual contents, social networking platforms and other content to
attract the user's attention.
 33.  The system according to claim 31, wherein the content presentation module of the server computer further comprises multi-device renderers to present the content on the user device.
 34.  The system according to claim 33, wherein the user device comprises: a personal computer (PC), a tablet PC, a set-top box (STB), a personal digital assistant (PDA), a mobile device, a hand held device, a wearable device, a global
positioning satellite (GPS) device, a palmtop computer, a laptop computer, a desktop computer, a personal trusted device, a web appliance, or any other communication device.
 35.  A non-transitory computer readable storage medium, comprising instructions to perform the steps of: analyzing and categorizing content by a server computer using artificial intelligence and storing the categorized content on a memory of the
server computer;  capturing explicit preferences of a user comprising user content interests, preferred content type, preferred presentation type, preferred language and preferred presentation format;  storing the captured explicit preferences of the
user on the memory of the server computer in an internal representation of the user's preferences;  storing an internal representation of prior knowledge of the user;  determining a plurality of implicit preferences of the user, wherein the implicit
preferences comprise geography, language, or media type;  mapping the categorized content to the internal representation of the user's preferences and the internal representation of the prior knowledge of the user to discover the content relevant to
user, by the server computer, from the categorized content, and preferred presentation type and formats;  processing the relevant content into the preferred presentation type and format at the server computer and presenting to the user on a user device; 
wherein at least one conflict between a conflicting explicit language preference and a conflicting implicit language preference is resolved by giving precedence to the conflicting explicit language preference;  monitoring user's activities during the
presentation by an analytics module, wherein monitoring user's activities comprises collecting the monitored user's activity data and the monitored user's feedback through the user device on effectiveness of presented content type, presentation type and
formats and updating the internal representation of the user's preferences on the memory of the server computer based on the activity data and the feedback collected;  and presenting on the user device a most relevant content with a most preferred
presentation type and formats based on the updated internal representation of the user's preferences.
 36.  The non-transitory computer readable storage medium according to claim 35, further comprising: gathering the content related to a core concept of content, presented to the user, from plurality of web sources;  and presenting the content
related to the core concept of content presented to the user during content presentation on the user device.  "
"10,120,955","
     November 6, 2018
","State tracking over machine-learned relational trees in a dialog system
"," A method is provided for representing and updating the state of a dialog
     involving a series of queries and commands to an artificial intelligence
     system. Each statement within the dialogue may be modeled as a relational
     tree spanning nodes corresponding to named entities within the statement.
     A data structure may be used to store each of these trees and to modify
     them as the dialog progresses. A subsequent statement in the dialog may
     be parsed and its contents used to update an ongoing search initiated
     within that dialog. Statements may be used for the update process despite
     being fragmentary or not corresponding to any predetermined grammar. An
     algorithm is disclosed for updating the trees within the data structure
     after a new statement is parsed.
",G06F 16/9024 (20190101); G06F 16/3329 (20190101),G06F 17/30 (20060101),"[['\n2014/0172899', '\nJune 2014'], ['\n2015/0370787', '\nDecember 2015']]",[0]," What is claimed is:  1.  A method, comprising: receiving data representing a first user intent statement;  generating a first graph comprising two or more named entities identified within the
first user intent statement and one or more relationships identified between the two or more named entities;  receiving data representing a second user intent statement;  without modifying the first graph, performing an action based on an intent
identified in the second user intent statement;  and subsequent to performing the action based on the intent identified in the second user intent statement, performing an action based on an intent identified in the first user intent statement.
 2.  The method of claim 1, further comprising: replacing a named entity within the first graph with a named entity within a third user intent statement.
 3.  The method of claim 1, further comprising: adding a named entity to the first graph based on a third user intent statement;  identifying a relationship between the added named entity and another named entity within the first graph;  and
adding the identified relationship to the first graph.
 4.  The method of claim 1, further comprising: performing a query or command identified in a third user intent statement.
 5.  The method of claim 1, further comprising: applying an update rule to the first graph based on an intent identified in a third user intent statement, the update rule comprising one or more additions, deletions, or replacements of nodes in
the first graph, wherein applying the update rule is further based at least in part on contents of the first graph, contents of a second graph, or a comparison of an element in the first graph to an element in the second graph.
 6.  The method of claim 1, wherein the intent identified in the second user intent statement is identified by: generating a second graph comprising two or more named entities identified within the second user intent statement and one or more
relationships identified within the second user intent statement;  and determining that the second user intent statement is not related to the first user intent statement based on comparing the first graph with the second graph.
 7.  A system, comprising: a processor;  and memory storing a database and storing computer readable instructions that, when executed by the processor, cause the system to: receive data representing a first user intent statement;  generate a
graph comprising two or more named entities identified in the first user intent statement and one or more relationships identified in the first user intent statement;  receive data representing a second user intent statement;  perform an action based on
an intent identified in the second user intent statement, wherein the action comprises applying an update rule to the graph, the update rule comprising one or more deletions or replacements of nodes in the graph, wherein applying the update rule is based
at least in part on contents of the graph, contents of a second graph, or a comparison of an element in the graph to an element in the second graph;  and subsequent to performing the action based on the intent identified in the second user intent
statement, perform an action based on an intent identified in the first user intent statement.
 8.  The system of claim 7, wherein performing the action based on the intent identified in the second user intent statement further comprises: replacing a named entity within the graph with a named entity within the second user intent statement.
 9.  The system of claim 7, wherein performing the action based on the intent identified in the second user intent statement further comprises: adding a named entity to the graph;  identifying a relationship between the added named entity and
another named entity within the graph;  and adding the identified relationship to the graph.
 10.  The system of claim 7, wherein performing the action based on the intent identified in the second user intent statement further comprises: performing a query upon the database based on relationships identified in the second user intent
statement.
 11.  The system of claim 7, wherein performing the action based on the intent identified in the second user intent statement further comprises: performing a command identified in the second user intent statement.
 12.  The system of claim 7, wherein the received data representing the first user intent statement comprises an audio recording of a user's spoken utterance.
 13.  The system of claim 12, additionally comprising a recording device, wherein the recording device records the user's spoken utterance and generates the received data representing the first user intent statement.
 14.  One or more non-transitory computer-readable media having instructions stored thereon, which when performed, cause a computing device to perform: receiving data representing a first user intent statement;  identifying a plurality of named
entities within the first user intent statement;  identifying one or more relationships between one or more pairs from the plurality of named entities;  generating a graph comprising two or more of the plurality of named entities and one or more of the
identified one or more relationships;  receiving data representing a second user intent statement;  determining a classification of the second user intent statement wherein the classification indicates whether the second user intent statement is
unrelated or related to the first user intent statement;  and performing an action based on the classification of the second user intent statement.
 15.  The non-transitory computer-readable media of claim 14, wherein performing the action based on the classification of the second user intent statement comprises: replacing a named entity within the graph with a named entity within the second
user intent statement.
 16.  The non-transitory computer-readable media of claim 14, wherein performing the action based on the classification of the second user intent statement comprises: adding a named entity to the graph;  identifying a relationship between the
added named entity and another named entity within the graph;  and adding the identified relationship to the graph.
 17.  The non-transitory computer-readable media of claim 14, wherein performing the action based on the classification of the second user intent statement comprises: performing a query or command identified in the second user intent statement.
 18.  The non-transitory computer-readable media of claim 14, wherein performing the action based on the classification of the second user intent statement comprises: applying an update rule to the graph, the update rule comprising one or more
additions, deletions, or replacements of nodes in the graph, wherein applying the update rule is based at least in part on contents of the graph, contents of a second graph, or a comparison of an element in the graph to an element in a second graph.
 19.  The non-transitory computer-readable media of claim 14, wherein the received data representing the first user intent statement comprises an audio recording of a user's spoken utterance.
 20.  The non-transitory computer-readable media of claim 14 wherein the instructions further cause the computing device to perform: subsequent to performing the action based on the classification of the second user intent statement, performing
an action based on an intent identified in the first user intent statement.  "
"10,127,493","
     November 13, 2018
","Logical entanglement device for governing AI-human interaction
"," A process of using a logical entanglement device such as a non-volatile
     logic gate as a failsafe to constrain the behavior of an autonomous
     machine controlled by an artificial intelligence (AI). Such a device may
     be employed to extend an AI self-boundary to include other objects or
     entities such as humans. This logical entanglement device may act much
     like a mirror neuron and cause the AI to respond to human
     nonfunctionality or suffering as if it were its own, causing the AI's
     behavior to reliably mimic empathy and compassion when interacting with
     humans and limiting the possibility of the AI devaluing the functionality
     and well-being of humans.
",G06N 3/0427 (20130101); G06N 3/002 (20130101); G06N 5/04 (20130101); G06N 3/0454 (20130101); G06N 3/04 (20130101); G06N 99/00 (20130101),G06N 99/00 (20100101); G06N 3/00 (20060101); G06N 3/04 (20060101); G06N 5/04 (20060101),"[['\n9161692', '\nOctober 2015']]",[0]," What is claimed is:  1.  A system for constraining AI behavior, comprising: a machine, comprising: at least one processor and at least one memory, at least one biometric sensor that detects at
least one factor relating to the functionality of a biological organism, the biometric sensor further being communicatively coupled with the at least one memory and configured to store information relating to the functionality of a biological organism
within the at least one memory;  at least one diagnostic sensor that detects at least one factor relating to the functionality of a machine, the at least one diagnostic sensor being communicatively coupled with the at least one memory and configured to
store information relating to the functionality of a machine within the at least one memory;  a logical entanglement device, the logical entanglement device configured to receive at least one data signal encoding information relating to the functionality
of a biological organism and configured to receive at least one data signal encoding information relating to the functionality of a machine;  the logical entanglement device further configured to transmit an output signal relating to the general
functionality of the system comprising the biological organism and the machine, said output signal comprising a hardware summation of the at least one data signal encoding information relating to the functionality of the biological organism and the at
least one data signal relating to the functionality of the machine;  and a transmitter, the transmitter being communicatively coupled to the at least one biometric sensor and the at least one diagnostic sensor, and wherein the transmitter transmits
information relating to the functionality of a biological organism and relating to the functionality of a machine from the at least one biometric sensor and the at least one diagnostic sensor to the logical entanglement device.
 2.  The system of claim 1, wherein the at least one processor and the at least one memory receive an output signal from the logical entanglement device;  wherein the at least one processor and the at least one memory calls a priority interrupt
function overriding the current functions of the processor and the memory if an output signal corresponding to nonfunctionality of the system comprising the biological organism and the machine is received;  and wherein the priority interrupt induces the
at least one processor and at least one memory to execute new instructions directing a machine to mitigate the nonfunctionality.
 3.  The system of claim 2, wherein said priority interrupt function activates an expert system.
 4.  The system of claim 3, wherein the expert system comprises an expert system processor and an expert system memory, and wherein the expert system memory has stored thereon instructions that, when executed by the expert system processor, cause
the expert system processor to: request priority direct memory access to the at least one memory of the machine, access information relating to the functionality of a biological organism and information relating to the functionality of a machine stored
within the at least one memory of the machine;  interpret the information relating to the functionality of a biological organism and information relating to the functionality of a machine;  identify the most probable causes of the nonfunctionality of the
system of entities;  identify appropriate remedies to said causes of the nonfunctionality of the system of entities;  and transmit remedy information to the at least one processor and at least one memory of the machine.
 5.  The system of claim 4, further comprising at least one of peripheral robotic devices and internal repair mechanisms communicatively coupled to the machine, and wherein the at least one peripheral robotic devices and internal repair
mechanisms executes the remedies.
 6.  The system of claim 2, wherein the at least one processor and at least one memory of the machine are configured to receive an output signal from the logical entanglement device.
 7.  The system of claim 1, wherein the machine further comprises an artificial neural network that recognizes functionality states of at least one machine and at least one biological organism, and wherein the artificial neural network performs
at least two of: learning over time, updating the system's accuracy over time, and updating the system memory.
 8.  The system of claim 1, wherein the logical entanglement device is a hardware logic gate.
 9.  The system of claim 1, wherein the biological organism is a human.
 10.  The system of claim 1, wherein the at least one biometric sensor detects at least one factor relating to the functionality of a plurality of biological organisms.
 11.  The system of claim 1, wherein the at least one factor relating to the functionality of a biological organism is at least one of blood flow, heart rate, muscular activity, motion, facial expression, audible voice recognition, audible animal
communications recognition, visual human signals comprising sign language, text communications, hand-written communications, and symbolic communications, communications relayed through third parties, and distress of an animal.
 12.  The system of claim 1, wherein the at least one diagnostic sensor is configured to measure at least one of the performance of a particular component of a machine, the performance of particular software on a machine, and the broad
functionality of a machine.
 13.  The system of claim 1, wherein the at least one factor relating to the functionality of a machine is at least one of: ambient temperature, operating temperature of a component, ambient levels of electromagnetic radiation, structural
integrity of the machine's chassis or casing.
 14.  A method of constraining AI behavior, comprising: providing a machine, the machine having a processor, a memory, at least one biometric sensor, and at least one diagnostic sensor;  providing a logical entanglement device configured to
receive at least one data signal corresponding to biometric sensor functionality assessment data and configured to receive at least one data signal corresponding to diagnostic sensor functionality assessment data;  collecting biometric sensor
functionality assessment data from the at least one biometric sensor, and storing that biometric sensor functionality assessment data within the memory;  collecting diagnostic sensor functionality assessment data from the at least one diagnostic sensor,
and storing that diagnostic sensor functionality assessment data within the memory;  transmitting biometric sensor functionality assessment data and diagnostic sensor functionality assessment data from the memory to the logical entanglement device; 
determining whether a system of entities comprising at least one biological organism and at least one machine can be classified as functional or can be classified as nonfunctional, said determination comprising a hardware summation of a signal encoding
the biometric sensor functionality assessment data and a signal encoding the diagnostic sensor functionality assessment data;  classifying the system of entities as one of: functional, nonfunctional;  transmitting an output signal from the logical
entanglement device to a processor and a memory;  activating, if the system of entities is classified as nonfunctional, an expert system;  determining, via the expert system, the most probable causes of the nonfunctionality in the system of entities; 
determining, via the expert system, a remedy for the nonfunctionality;  and directing a machine to implement the remedy.
 15.  The method of claim 14, wherein the expert system: requests priority direct memory access to a memory of the machine, accesses biometric sensor functionality assessment data and diagnostic sensor functionality assessment data stored within
that memory;  interprets the biometric sensor functionality assessment data and diagnostic sensor functionality assessment data;  identifies the most probable causes of the nonfunctionality;  identifies appropriate remedies to said causes of the
nonfunctionality;  and transmits remedy information to at least one processor and memory.
 16.  The method of claim 14, wherein the activation of the expert system interrupts the assigned task instructions of the machine.
 17.  The method of claim 14, wherein the expert system and the machine run concurrently.
 18.  The method of claim 14, further comprising: determining, after a machine has been directed to implement a particular remedy, whether the system of entities remains nonfunctional or may be classified as functional;  determining, if the
system remains nonfunctional, via an expert system, the most probable causes of the nonfunctionality;  and directing a machine to mitigate the nonfunctionality according to a particular remedy.
 19.  The method of claim 18, wherein the expert system accesses any of the following information: biometric sensor functionality assessment data, diagnostic sensor functionality assessment data, remedies that have been previously tried, the
effects of said remedies on the nonfunctionality of the system, the effects of said remedies on the identified cause of the nonfunctionality of the system.
 20.  The method of claim 14, further comprising: determining, after a machine has been directed to mitigate the cause of the nonfunctionality according to a particular remedy, whether the system remains nonfunctional or may be classified as
functional;  and if the system is functional, disabling the expert system and resuming previous instructions.  "
"10,136,189","
     November 20, 2018
","Method and system for re-aggregation and optimization of media
"," A media system for providing community and behaviorally driven content
     selection and bundling based on user consumption data and an analysis of
     user affinity. The media system performs user affinity representations
     and analyzes content affinity. The media system performs content affinity
     representations and maps content analysis into groups. The media system
     creates multi-dimensional arrays and uses for user content, maps pricing
     choices to subscriber value, and, using algorithms which may include
     Artificial Intelligence and Machine Learning approaches, maps groups to
     pricing choices.
",H04N 21/25891 (20130101); H04N 21/2407 (20130101); H04N 21/4667 (20130101); H04N 21/4668 (20130101); H04N 21/4532 (20130101); H04N 21/47815 (20130101); H04N 21/251 (20130101); H04N 21/4788 (20130101); H04N 21/44226 (20200801); H04N 21/6582 (20130101),H04N 21/466 (20110101); H04N 21/45 (20110101); H04N 21/442 (20110101); H04N 21/478 (20110101); H04N 21/258 (20110101),"[['\n2004/0221308', '\nNovember 2004'], ['\n2006/0015902', '\nJanuary 2006'], ['\n2007/0288961', '\nDecember 2007'], ['\n2010/0251305', '\nSeptember 2010'], ['\n2010/0325648', '\nDecember 2010'], ['\n2012/0260214', '\nOctober 2012'], ['\n2014/0195919', '\nJuly 2014'], ['\n2015/0326688', '\nNovember 2015']]",[1]," What is claimed is:  1.  A system for digital content filtering and multi-dimensional analysis, the system comprising: a content server that stores a plurality of digital content files, each
digital content file comprising sequential content data;  a user database that stores user data arrays, wherein each user data array is specific to a respective user and includes a set of content factors, and wherein each content factor in the set of
content factors is associated with a likability metric specific to the respective user;  a content database that stores content data arrays, wherein each content data array is specific to a respective content title and includes a set of content elements,
and wherein each content element in the set of content elements is associated with an affinity metric specific to the respective content title;  and a media analysis server that: selects a plurality of dimensions from the set of content factors and the
set of content elements;  creates a multi-dimensional content array for a requesting user based on the selected plurality of dimensions that are found in a user data array specific to the requesting user and in one or more content data arrays;  and
generates the content data arrays and the user data array specific to the requesting user.
 2.  The system of claim 1, wherein the media analysis server generates the user data array specific to the requesting user based on current content consumption data associated with the requesting user.
 3.  The system of claim 2, wherein the current content consumption data includes at least one of viewing history, imported historical data, survey responses, express preferences, and social media activity.
 4.  The system of claim 2, wherein the media analysis server generates the user data array specific to the requesting user by assigning a weight to each content factor and determining the likability metric for each content factor based on the
assigned weight.
 5.  The system of claim 4, wherein the media analysis server further updates the weight assigned to each content factor based on a disparity between the determined likability metric and new content consumption data associated with the requesting
user.
 6.  The system of claim 1, wherein the user database further includes at least one data array associated with a group of users, and wherein the media analysis server generates a plurality of user data arrays, each user data array specific to a
different user in the group based on one or more tracked indicators and an assigned degree of confidence associated with the tracked indicators.
 7.  The system of claim 1, wherein the media analysis server generates the multidimensional content array based on at least one other user data array specific to another user identified as similar to the requesting user.
 8.  A method for digital content filtering and multi-dimensional analysis, the method comprising: storing a plurality of digital content files in a content server, each digital content file comprising sequential content data;  storing user data
arrays in a user database, wherein each user data array is specific to a respective user and includes a set of content factors, and wherein each content factor in the set of content factors is associated with a likability metric specific to the
respective user;  storing content data arrays in a content database, wherein each content data array is specific to a respective content title and includes a set of content elements, and wherein each content element in the set of content elements is
associated with an affinity metric specific to the respective content title;  selecting a plurality of dimensions from the set of content factors and the set of content elements;  creating a multi-dimensional content array for a requesting user based on
the selected plurality of dimensions that are found in a user data array specific to the requesting user and in one or more content data arrays;  and generating the content data arrays and the user data array specific to the requesting user.
 9.  The method of claim 8, wherein generating the user data array specific to the requesting user is based on current content consumption data associated with the requesting user.
 10.  The method of claim 9, wherein the current content consumption data includes at least one of viewing history, imported historical data, survey responses, express preferences, and social media activity.
 11.  The method of claim 9, wherein generating the user data array specific to the requesting user comprises assigning a weight to each content factor and determining the likability metric for each content factor based on the assigned weight.
 12.  The method of claim 11, further comprising updating the weight assigned to each content factor based on a disparity between the determined likability metric and new content consumption data associated with the requesting user.
 13.  The method of claim 8, further comprising storing at least one data array associated with a group of users, and generating a plurality of user data arrays, each user data array specific to a different user in the group based on one or more
tracked indicators and an assigned degree of confidence associated with the tracked indicators.
 14.  The method of claim 8, wherein generating the multi-dimensional content array is further based on at least one other user data array specific to another user identified as similar to the requesting user.
 15.  A non-transitory computer-readable storage medium, having embodied thereon a program executable by a processor to perform a method for digital content filtering and multi-dimensional analysis, the method comprising: storing a plurality of
digital content files in a content server, each digital content file comprising sequential content data;  storing user data arrays in a user database, wherein each user data array is specific to a respective user and includes a set of content factors,
and wherein each content factor in the set of content factors is associated with a likability metric specific to the respective user;  storing content data arrays in a content database, wherein each content data array is specific to a respective content
title and includes a set of content elements, and wherein each content element in the set of content elements is associated with an affinity metric specific to the respective content title;  selecting a plurality of dimensions from the set of content
factors and the set of content elements;  creating a multi-dimensional content array for a requesting user based on the selected plurality of dimensions that are found in a user data array specific to the requesting user and in one or more content data
arrays;  and generating the content data arrays and the user data array specific to the requesting user.  "
"10,137,834","
     November 27, 2018
","Motor vehicle artificial intelligence expert system dangerous driving
     warning and control system and method
"," Specifically programmed, integrated motor vehicle dangerous driving
     warning and control system and methods comprising at least one
     specialized communication computer machine including electronic
     artificial intelligence expert system decision making capability further
     comprising one or more motor vehicle electronic sensors for monitoring
     the motor vehicle and for monitoring activities of the driver and/or
     passengers including activities related to the use of cellular telephones
     and/or other wireless communication devices and further comprising
     electronic communications transceiver assemblies for communications with
     external sensor networks for monitoring dangerous driving situations,
     weather conditions, roadway conditions, pedestrian congestion and motor
     vehicle traffic congestion conditions to derive warning and/or control
     signals for warning the driver of dangerous driving situations and/or for
     controlling the motor vehicle driver use of a cellular telephone and/or
     other wireless communication devices.
",H04R 3/005 (20130101); G08G 1/096775 (20130101); G08G 1/167 (20130101); G10L 15/22 (20130101); G08G 1/205 (20130101); H04R 1/406 (20130101); G08G 1/096716 (20130101); G08G 1/0116 (20130101); G10L 25/78 (20130101); G08G 1/0129 (20130101); H04M 1/72454 (20210101); G08G 1/096741 (20130101); H04W 4/80 (20180201); G06V 20/56 (20220101); G06V 20/597 (20220101); B60Q 9/008 (20130101); G10L 15/26 (20130101); H04W 4/90 (20180201); G08G 1/096783 (20130101); H04B 5/0056 (20130101); G08G 1/048 (20130101); H04M 1/72463 (20210101); G06N 5/048 (20130101); H04W 4/023 (20130101); G08G 1/012 (20130101); G10L 21/0232 (20130101); H04W 4/40 (20180201); H04B 5/0081 (20130101); G08G 1/0141 (20130101); G06N 5/02 (20130101); G08G 1/166 (20130101); H04B 7/0617 (20130101); H04B 5/0043 (20130101); H04R 2201/403 (20130101); H04R 2499/13 (20130101); G10L 2021/02166 (20130101),G08G 1/09 (20060101); H04M 1/725 (20060101); H04R 3/00 (20060101); H04W 4/02 (20180101); H04W 4/40 (20180101); B60Q 9/00 (20060101); G10L 25/78 (20130101); H04W 4/90 (20180101); G08G 1/048 (20060101); G10L 21/0232 (20130101); H04R 1/40 (20060101); G08G 1/01 (20060101); G10L 15/26 (20060101); G06K 9/00 (20060101); G10L 15/22 (20060101); G08G 1/16 (20060101); G06N 5/02 (20060101); H04W 4/80 (20180101); G08G 1/0967 (20060101); G08G 1/00 (20060101); H04B 5/00 (20060101); G10L 21/0216 (20130101),"[['\n2936571', '\nMay 1960'], ['\n3662401', '\nMay 1972'], ['\n4852001', '\nJuly 1989'], ['\n5301320', '\nApril 1994'], ['\n5745687', '\nApril 1998'], ['\n5768506', '\nJune 1998'], ['\n5862346', '\nJanuary 1999'], ['\n5958071', '\nSeptember 1999'], ['\n6317058', '\nNovember 2001'], ['\n6334137', '\nDecember 2001'], ['\n6636884', '\nOctober 2003'], ['\n7024669', '\nApril 2006'], ['\n7408907', '\nAugust 2008'], ['\n7693486', '\nApril 2010'], ['\n7697917', '\nApril 2010'], ['\n7856360', '\nDecember 2010'], ['\n8060150', '\nNovember 2011'], ['\n8145199', '\nMarch 2012'], ['\n8229458', '\nJuly 2012'], ['\n8295890', '\nOctober 2012'], ['\n8364171', '\nJanuary 2013'], ['\n8437776', '\nMay 2013'], ['\n8447331', '\nMay 2013'], ['\n8515459', '\nAugust 2013'], ['\n8538402', '\nSeptember 2013'], ['\n8566236', '\nOctober 2013'], ['\n8595824', '\nNovember 2013'], ['\n8626194', '\nJanuary 2014'], ['\n8634816', '\nJanuary 2014'], ['\n8635645', '\nJanuary 2014'], ['\n8639263', '\nJanuary 2014'], ['\n8750853', '\nJune 2014'], ['\n8826175', '\nSeptember 2014'], ['\n9024783', '\nMay 2015'], ['\n9086948', '\nJuly 2015'], ['\n2002/0038228', '\nMarch 2002'], ['\n2002/0147642', '\nOctober 2002'], ['\n2007/0027583', '\nFebruary 2007'], ['\n2007/0281716', '\nDecember 2007'], ['\n2008/0064446', '\nMarch 2008'], ['\n2008/0133336', '\nJune 2008'], ['\n2008/0294690', '\nNovember 2008'], ['\n2009/0234552', '\nSeptember 2009'], ['\n2010/0002075', '\nJanuary 2010'], ['\n2010/0207787', '\nAugust 2010'], ['\n2010/0323615', '\nDecember 2010'], ['\n2012/0057716', '\nMarch 2012'], ['\n2012/0246650', '\nSeptember 2012'], ['\n2013/0288744', '\nOctober 2013'], ['\n2013/0295901', '\nNovember 2013'], ['\n2015/0091740', '\nApril 2015'], ['\n2015/0092056', '\nApril 2015'], ['\n2015/0161913', '\nJune 2015'], ['\n2015/0172450', '\nJune 2015'], ['\n2015/0178578', '\nJune 2015'], ['\n2015/0186714', '\nJuly 2015'], ['\n2015/0191122', '\nJuly 2015'], ['\n2015/0371659', '\nDecember 2015'], ['\n2016/0096529', '\nApril 2016'], ['\n2016/0101785', '\nApril 2016'], ['\n2016/0101786', '\nApril 2016'], ['\n2016/0267335', '\nSeptember 2016'], ['\n2016/0272215', '\nSeptember 2016'], ['\n2017/0049785', '\nFebruary 2017']]","[3, '11,203,294', '11,052,821', '10,681,523']"," The invention claimed is:  1.  An integrated motor vehicle dangerous driving warning and control system comprising at least one electronic, specifically programmed, specialized device control
communication computer machine including electronic artificial intelligence expert system decision making capability and further comprising: (a) one or more motor vehicle electronic sensors including one or more interior vehicle cameras and image
analysis software that generate electronic signals derived from monitoring activities of the motor vehicle driver and/or passengers that may distract the driver including activities related to the use of cellular telephones and/or other wireless
communication devices;  (b) one or more electronic communications transceiver assemblies for communications with electronic external sensor networks to receive information from external sensor network inputs to gather one or more of weather information,
roadway danger information, traffic congestion information, and/or pedestrian congestion information;  (c) one or more sensor input history files wherein said integrated motor vehicle dangerous driving warning and control system makes use of said
artificial intelligence expert system decision making capability based on said motor vehicle electronic sensor inputs and said electronic external sensor network inputs and/or sensor input history files to derive warning and/or control signals for the
use of said cellular telephone;  and, (d) further wherein artificial intelligence expert systems decision making is based on expert input with multiple propositional expert system programming instructions defining multiple ranges of degrees of danger for
each of driver distraction and roadway and driving conditions variables and further wherein said artificial intelligence expert system decision making provides an integrated composite degree of danger driver warning index based on degree of danger input
parameters for driver distraction and roadway and driving conditions with variable degrees of danger inputs.
 2.  The integrated motor vehicle dangerous driving warning and control system of claim 1 wherein said one or more cameras and image analysis software provide for capturing images of the driver of a moving vehicle to assist in the determination
of situations where the driver is using or attempting to use a telecommunications device.
 3.  The integrated motor vehicle dangerous driving warning and control system of claim 2 wherein said one or more cameras and image analysis software provide for capturing and analyzing multiple individual photos or videos of the driver of the
vehicle while excluding other areas of the vehicle.
 4.  The integrated motor vehicle dangerous driving warning and control system of claim 3 wherein one or more photographs or videos may be taken of other areas of the vehicle assist in determining the presence or absence of other passengers in
the vehicle and ascertaining their conversational activities and/or their use of separate telecommunications devices or cellular telephones.
 5.  The integrated motor vehicle dangerous driving warning and control system of claim 3 wherein said one or more cameras and image analysis software includes facial recognition capability to identify particular drivers.
 6.  The integrated motor vehicle dangerous driving warning and control system of claim 5 wherein said system maintains facial recognition and image identification history files of motor vehicle drivers with records and driving habits including
dangerous driving tendencies.
 7.  The integrated motor vehicle dangerous driving warning and control system of claim 6 wherein facial recognition assists in identifying drivers based on captured images by comparing to stored image files and to identify driving risks
presented by particular drivers and, if no identification, transmitting images of unidentified drivers to the vehicle owner or other responsible entity for verification that the driver is permitted to drive the vehicle.
 8.  The integrated motor vehicle dangerous driving warning and control system of claim 7 wherein said one or more of said cameras is used to analyze specific motions of the driver including, for example, raising of a telecommunication device to
the driver's ear for use.
 9.  The integrated motor vehicle dangerous driving warning and control system of claim 1 further specifically programmed to derive artificial intelligence expert systems road/weather indices based on said electronic external sensor network
inputs.
 10.  The integrated motor vehicle dangerous driving warning and control system of claim 9 further specifically programmed to derive artificial intelligence expert system vehicle driving warning indices based on said derived road/weather indices
and traffic congestion inputs from said electronic external sensor networks.
 11.  The integrated motor vehicle dangerous driving warning and control system of claim 10 further specifically programmed to derive artificial intelligence expert system driver warning indices based on said derived vehicle driving warning
indices and said motor vehicle electronic sensor inputs monitoring driver distractions including distractions arising from the use of said cellular telephones and/or other wireless communication devices.
 12.  The integrated motor vehicle dangerous driving warning and control system of claim 11 wherein said derivation of driver warning indices is further based on fuzzy logic calculations.
 13.  An integrated motor vehicle dangerous driving warning and control system comprising at least one electronic, specifically programmed, specialized device control communication computer machine including electronic artificial intelligence
expert system decision making capability and further comprising: (a) one or more motor vehicle electronic sensors including one or more interior vehicle cameras and image analysis software that generate electronic signals derived from monitoring
activities of the motor vehicle driver and/or passengers that may distract the driver including activities related to the use of cellular telephones and/or other wireless communication devices;  (b) one or more electronic communications transceiver
assemblies for communications with electronic external sensor networks to receive information from external sensor network inputs to gather one or more of weather information, roadway danger information, traffic congestion information, and/or pedestrian
congestion information;  (c) artificial intelligence expert system decision making capability based on said motor vehicle electronic sensor inputs and said electronic external sensor network inputs to derive control signals for warning the driver of
dangerous driving situations;  (d) wherein artificial intelligence expert systems decision making is based on expert input with multiple propositional expert system programming instructions defining multiple ranges of degrees of danger for each of driver
distraction and roadway and driving conditions variables and are based on expert knowledge and inference procedures defining conditional parameter relationships;  and (e) further wherein said artificial intelligence expert system decision making provides
an integrated composite degree of danger driver warning index based on degree of danger input parameters for driver distraction and roadway and driving conditions with variable degrees of danger inputs.
 14.  An integrated motor vehicle dangerous driving warning and control method comprising at least one electronic, specifically programmed, specialized device control communication computer machine including electronic artificial intelligence
expert system decision making capability and further comprising the following steps: (a) the step of deriving electronic signals from one or more motor vehicle electronic sensors including one or more interior vehicle cameras and image analysis software
used for monitoring activities of the motor vehicle driver and/or passengers that may distract the driver including activities related to the use of cellular telephones and/or other wireless communication devices, (b) the step of receiving information
reporting one or more of weather conditions, roadway dangers, traffic congestion and/or pedestrian congestion via one or more electronic communications transceiver assemblies used for communications with electronic external sensor networks, (c) the step
of deriving control signals for controlling the use of said cellular telephone and/or other wireless communication devices and/or for warning the driver of dangerous driving situations based on artificial intelligence expert system decision making
capability, said decisions based on said motor vehicle electronic sensor inputs and/or said electronic external sensor network inputs;  (d) the step of deriving sensor signals indicative of vehicle motion and cellular telephone usage by the driver of the
motor vehicle;  (e) upon detection of defined vehicle motion and driver usage of a cellular telephone, the further step of deriving danger warning indices indicative of a degree of danger for driver continuing the use of the cellular telephone;  and (f)
wherein said driver warning indices are based on artificial intelligence expert systems propositional computations derived from specific cellular telephone usage and driver distraction parameters together with parameters defining ranges of driving
conditions including roadway conditions, traffic congestion, weather information and/or pedestrian congestion.
 15.  The integrated motor vehicle dangerous driving warning and control method of claim 14 further comprising the step of capturing and analyzing one or more images of the driver of a moving vehicle to assist in the determination of situations
where the driver is using or attempting to use a telecommunications device while the vehicle is moving.
 16.  The integrated motor vehicle dangerous driving warning and control method of claim 15 further comprising the step of capturing and analyzing one or more individual photos or videos of the driver of the vehicle while excluding other areas of
the vehicle.
 17.  The integrated motor vehicle dangerous driving warning and control system of claim 16 further comprising the step of capturing one or more photographs and/or videos of other areas of the vehicle that may be of assistance in determining the
presence or absence of other passengers in the vehicle and ascertaining their conversational activities and/or their use of separate telecommunications devices or cellular telephones.
 18.  The integrated motor vehicle dangerous driving warning and control method of claim 16 further comprising the step of using said one or more cameras and image analysis software for facial recognition to identify particular drivers.
 19.  The integrated motor vehicle dangerous driving warning and control method of claim 18 further comprising the step of maintaining image identification and/or history files of motor vehicle drivers with records of their driving habits
including dangerous driving tendencies.
 20.  The integrated motor vehicle dangerous driving warning and control method of claim 19 further comprising the step of using facial recognition for identifying drivers based on captured images by comparing to stored image files and
identifying particular drivers who present increased risks for automobile accidents.
 21.  The integrated motor vehicle dangerous driving warning and control method of claim 15 further comprising the step of analyzing specific motions of the driver of a motor vehicle including, for example, raising of a cellular telephone or
other wireless device to the driver's ear for use.  "
"10,140,553","
     November 27, 2018
","Machine learning artificial intelligence system for identifying vehicles
"," An artificial intelligence system for identifying attributes in an image.
     The system may include a processor in communication with a client device;
     and a storage medium. The storage medium may store instructions that,
     when executed, configure the processor to perform operations including:
     extracting first features; categorizing the first images in a first group
     or a second group; modifying first metadata associated with each image in
     the first images to include a binary label; calculating a classification
     function; classifying a second plurality of images using the
     classification function; extracting second features from the second
     images classified in the first group; categorizing the second images in
     the first group by attribute; calculating an attribute identification
     function that identifies attributes of the second images; and identifying
     at least one attribute associated with a client image using the attribute
     identification function, the client image being received from the client
     device.
",G06N 3/02 (20130101); G06V 10/40 (20220101); G06N 3/084 (20130101); G06K 9/6267 (20130101); G06N 20/00 (20190101); G06K 9/6256 (20130101); G06N 3/0481 (20130101); G06N 3/0454 (20130101); G06V 10/17 (20220101); G06V 10/454 (20220101); G06V 20/20 (20220101); G06F 16/583 (20190101); G06K 9/6218 (20130101); G06V 2201/10 (20220101); G06N 5/003 (20130101); G06N 20/20 (20190101),G06K 9/46 (20060101); G06K 9/62 (20060101); G06N 3/02 (20060101); G06F 17/30 (20060101); G06N 99/00 (20100101),"[['\n8873807', '\nOctober 2014'], ['\n8965112', '\nFebruary 2015'], ['\n9792530', '\nOctober 2017'], ['\n9842496', '\nDecember 2017'], ['\n2004/0258313', '\nDecember 2004'], ['\n2006/0030985', '\nFebruary 2006'], ['\n2006/0104504', '\nMay 2006'], ['\n2008/0169914', '\nJuly 2008'], ['\n2008/0219517', '\nSeptember 2008'], ['\n2008/0310737', '\nDecember 2008'], ['\n2009/0273677', '\nNovember 2009'], ['\n2010/0080440', '\nApril 2010'], ['\n2010/0158356', '\nJune 2010'], ['\n2010/0322534', '\nDecember 2010'], ['\n2012/0230548', '\nSeptember 2012'], ['\n2013/0070986', '\nMarch 2013'], ['\n2013/0261880', '\nOctober 2013'], ['\n2014/0266803', '\nSeptember 2014'], ['\n2015/0019204', '\nJanuary 2015'], ['\n2015/0036919', '\nFebruary 2015'], ['\n2015/0100448', '\nApril 2015'], ['\n2015/0242708', '\nAugust 2015'], ['\n2015/0248586', '\nSeptember 2015'], ['\n2015/0310365', '\nOctober 2015'], ['\n2016/0070986', '\nMarch 2016'], ['\n2016/0259994', '\nSeptember 2016'], ['\n2017/0024641', '\nJanuary 2017'], ['\n2017/0083772', '\nMarch 2017'], ['\n2017/0083874', '\nMarch 2017'], ['\n2017/0228616', '\nAugust 2017'], ['\n2017/0294118', '\nOctober 2017'], ['\n2017/0329755', '\nNovember 2017'], ['\n2018/0025392', '\nJanuary 2018'], ['\n2018/0129893', '\nMay 2018']]","[14, '11,335,107', '11,263,482', '11,216,922', '11,165,694', '11,144,788', '11,144,757', '11,087,447', '11,010,830', '11,010,808', '10,795,994', '10,706,452', '10,635,940', '10,332,245', '10,311,336']"," What is claimed is:  1.  A system for identifying attributes in an image, comprising: at least one processor in communication with a client processor;  and at least one storage medium storing
instructions that, when executed, configure the processor to perform operations comprising: obtaining a plurality of first images, the first images being associated with first metadata, the first images comprising car images;  extracting a plurality of
first features from the first images;  categorizing the first images into one of a first group or a second group based on the first features, the first group comprising car exterior images, the second group comprising car interior images;  modifying the
first metadata associated with images in the first images to include a binary label, the binary label representing into which of first and second groups the image was categorized;  calculating a classification function that classifies the first images
based on image features and the binary label;  obtaining a plurality of second images, the second images being associated with second metadata;  classifying the second images into one of the first group or the second group using the classification
function, the classification function comprising a regression function;  extracting a plurality of second features from the second images classified into the first group;  categorizing the second images in the first group by attribute, based on the
second features;  calculating an attribute identification function that identifies attributes of the second images based on image features;  receiving a client image from the client processor;  and identifying at least one attribute associated with the
client image using the attribute identification function.
 2.  The artificial intelligence system of claim 1, wherein: the operations further comprise generating augmented images by applying filters to the second images classified in the first group;  and extracting second features comprises extracting
second features from the augmented images.
 3.  The artificial intelligence system of claim 2, wherein the filters include at least one of a zoom filter, a shear filter, a rotation filter, or a channel filter and are associated with a random function.
 4.  The artificial intelligence system of claim 1, wherein categorizing the first images comprises: associating coordinates with each first images based on its corresponding first features;  determining a threshold distance based on distances
between the associated coordinates of pairs of the first images;  and iteratively cluster coordinates that are within the threshold distance from each other.
 5.  The artificial intelligence system of claim 1, wherein extracting first features comprises: transmitting the first images to a cluster comprising graphic processing units configured to extract image features;  and receiving, from the cluster
comprising graphic processing units, a file with the first features.
 6.  The artificial intelligence system of claim 1, wherein extracting first features comprises: identifying a file type for each image in the first images;  and normalizing each image in the first images based on the file type and a model image.
 7.  The artificial intelligence system of claim 6, wherein extracting first features further comprises using a pre-trained convolutional neural network.
 8.  The artificial intelligence system of claim 7, wherein using a pre-trained convolutional neural network comprises: importing layers of the pre-trained convolutional neural network;  determining features described in a target layer of the
pre-trained convolutional neural network;  and initializing a multiclass fitting model using the features in the target layer and the first images.
 9.  The artificial intelligence system of claim 1, wherein the second images comprise car images, at least one of the second images being different from at least one of the first images;  and the attribute identification function identifies at
least one of a car make, a car model, a car year, or a car trim.
 10.  The artificial intelligence system of claim 1, wherein the first metadata includes notes indicating the first images correspond to car interior or exterior;  and the second metadata includes notes indicating car make and car model.
 11.  The artificial intelligence system of claim 10, wherein the second images comprise at least one million car images.
 12.  The artificial intelligence system of claim 1, wherein calculating an attribute identification function comprises: partitioning the second images in the first group as at least training images and validation images;  generating an input
file comprising features from images in the training images and a result file comprising the second metadata associated with the training images;  determining hyper parameters and activation functions;  calculating a plurality of weights based on the
input file and the result file, the weights associating a plurality of nodes determined by the hyper parameters;  and recursively calculating the attribute identification function until a cost function is below a threshold error.
 13.  The artificial intelligence system of claim 1, wherein the operations further comprise transmitting display information to the client processor, the display information instructing the client processor to modify the client image by
including the identified attribute.
 14.  The artificial intelligence system of claim 13, wherein the display information instruct the client processor to modify the client image by: generating a new layer including the identified attribute;  and overlaying the new layer on top of
original layers of the client image.
 15.  The artificial intelligence system of claim 1 wherein, the operations further comprise: determining at least one of a cost or a condition based on the identified attribute by querying a database;  and transmitting information to the client
processor including the at least one identified attribute, and the cost or the condition.
 16.  The artificial intelligence system of claim 1 wherein, the operations further comprise: determine that the client image is not acceptable for identification;  transmitting display information to the client processor, the display information
instructing the client processor to modify the client image by including at least one of a guiding line or an error message.
 17.  A non-transitory computer-readable medium storing instructions that, when executed by a processor, cause the processor to operate a computer system for identifying attributes in an image by performing operations, the operations comprising:
obtaining a plurality of first images, the first images being associated with first metadata, the first images comprising car images;  extracting a plurality of first features from the first images;  categorizing the first images into one of a first
group or a second group based on the first features, the first group comprising car exterior images, the second group comprising car interior images;  modifying the first metadata associated with images in the first images to include a binary label, the
binary label representing into which of first and second groups the image was categorized;  calculating a classification function that classifies the first images based on image features and the binary label;  obtaining a plurality of second images, the
second images being associated with second metadata;  classifying the second images into one of the first group or the second group using the classification function, the classification function comprising a regression function;  extracting a plurality
of second features from the second images classified into the first group;  categorizing the second images in the first group by attribute, based on the second features;  calculating an attribute identification function that identifies attributes of the
second images based on image features;  receiving a client image from a client processor;  and identifying at least one attribute associated with the client image using the attribute identification function.
 18.  A computer-implemented method for identifying attributes in an image, the method comprising: obtaining a plurality of first images, the first images being associated with first metadata, the first images comprising car images;  extracting a
plurality of first features from the first images;  categorizing the first images into one of a first group or a second group based on the first features, the first group comprising car exterior images, the second group comprising car interior images; 
modifying the first metadata associated with images in the first images to include a binary label, the binary label representing into which of first and second groups the image was categorized;  calculating a classification function that classifies the
first images based on image features and the binary label;  obtaining a plurality of second images, the second images being associated with second metadata;  classifying the second images into one of the first group or the second group using the
classification function, the classification function comprising a regression function;  extracting a plurality of second features from the second images classified into the first group;  categorizing the second images in the first group by attribute,
based on the second features;  calculating an attribute identification function that identifies attributes of the second images based on image features;  receiving a client image from a client processor;  and identifying at least one attribute associated
with the client image using the attribute identification function.  "
"10,140,557","
     November 27, 2018
","Increasing network transmission capacity and data resolution quality and
     computer systems and computer-implemented methods for implementing
     thereof
"," In some embodiments, the present invention provides for an exemplary
     inventive system, including: a communication pipeline, including: at a
     first end of the communication pipeline: a first processor configured to:
     obtain a plurality of original content data units having a representative
     content associated with a subject; apply a trained artificial
     intelligence algorithm to identify: the representative content of the
     subject and original background content that is not associated with the
     subject; remove the original background content to reduce a volume of
     data being transmitted resulting in an increased capacity of the
     communication channel; encode and transmit each respective modified
     content data unit from the first end of the communication pipeline to a
     second end; a second processor configured to: receive and decode each
     respective modified content data unit; generate a respective artificial
     background content; and combine the representative content associated
     with the subject and the respective artificial background content to form
     each composite content data unit.
",G06V 10/454 (20220101); G06K 9/6227 (20130101); G06K 9/6271 (20130101); G06N 3/08 (20130101); G06V 30/194 (20220101); G06V 40/10 (20220101); G06N 3/0445 (20130101); G06K 9/6267 (20130101); G06N 3/0454 (20130101); G06N 3/04 (20130101),G06K 9/66 (20060101); G06K 9/62 (20060101); G06N 3/08 (20060101); G06N 3/04 (20060101),"[['\n2017/0154420', '\nJune 2017'], ['\n2017/0270593', '\nSeptember 2017']]",[1]," What is claimed is:  1.  A computer-implemented method, comprising: at a first end of a communication pipeline: obtaining, by at least one first processor, a plurality of original content data
units having a representative content associated with at least one subject;  applying, by the at least one first processor, to each original content data unit, at least one trained artificial intelligence algorithm to identify: i) the representative
content associated with the at least one subject and ii) original background content that is not associated with the at least one subject;  removing, by the at least one first processor, from each original content data unit, the original background
content to form each respective modified content data unit only having the representative content associated with the at least one subject to reduce a volume of data being transmitted resulting in an increased capacity of the communication channel; 
encoding, by the at least one first processor, each respective modified content data unit in accordance with at least one data encoding-decoding methodology;  and transmitting, by the at least one first processor, each respective modified content data
unit from the first end of the communication pipeline to a second end of the communication pipeline;  at the second end of the communication pipeline: receiving, by at least one second processor, each respective modified content data unit;  decoding, by
the at least one second processor, each respective modified content data unit in accordance with at least one data encoding-decoding methodology;  generating, by the at least one second processor, for each respective modified content data unit, a
respective artificial background content;  and combining, by the at least one second processor, for each respective modified content data unit, the representative content associated with the at least one subject and the respective artificial background
content to form each composite content data unit.
 2.  The computer-implemented method of claim 1, wherein the at least one trained artificial intelligence algorithm is selected from the group consisting of: i) a trained convolutional neural network algorithm, ii) a trained feedforward neural
network algorithm, iii) a trained recurrent neural network algorithm, and iv) a trained modular neural network algorithm.
 3.  The computer-implemented method of claim 1, wherein the trained convolutional neural network algorithm is a U-Net type trained convolutional neural network algorithm.
 4.  The computer-implemented method of claim 1, wherein the representative content of the at least one subject is a visual appearance of at least one person.
 5.  The computer-implemented method of claim 4, wherein the visual appearance of the at least one subject is a face of the at least one person.
 6.  The computer-implemented method of claim 5, wherein the at least one trained artificial intelligence algorithm has been trained based on a multi-dimensional morphable facial model.
 7.  The computer-implemented method of claim 4, wherein the at least one trained artificial intelligence algorithm has been trained based on an image database, comprising images of people that were taken at a plurality of settings and a
plurality of conditions.
 8.  The computer-implemented method of claim 1, wherein the step of applying, to each original content data unit, the at least one trained artificial intelligence algorithm further comprises: assigning, by a labeling component of the at least
one trained artificial intelligence algorithm, a particular label to the at least one subject, classifying, by a classification component of the at least one trained artificial intelligence algorithm, pixels of a visual content of each original content
data unit into at least three classes: i) foreground pixels, ii) background pixels, and iii) unknown pixels;  matting, by a matting component of the at least one trained artificial intelligence algorithm, the foreground pixels and the background pixels
of the visual content of each original content data to an aligned average shape mask associated with the at least one subject to identify the representative content associated with the at least one subject.
 9.  The computer-implemented method of claim 1, wherein the method further comprising: training, by at least one third processor, at least one artificial intelligence algorithm based on an image database until a loss function reaches a
pre-determined acceptable threshold to obtain the at least one trained artificial intelligence algorithm.
 10.  The computer-implemented method of claim 9, wherein the pre-determined acceptable threshold varies from 1 to 15 percent.
 11.  The computer-implemented method of claim 10, wherein the pre-determined acceptable threshold is between 1 and 5 percent.
 12.  The computer-implemented method of claim 1, wherein the respective artificial background content comprises at least one of: i) at least one uniform color, ii) at least one color gradient, and iii) at least one artificial image.
 13.  A system, comprising: a communication pipeline, comprising: at a first end of the communication pipeline: at least one first processor;  wherein the at least one first processor is configured to: obtain a plurality of original content data
units having a representative content associated with at least one subject;  apply, to each original content data unit, at least one trained artificial intelligence algorithm to identify: i) the representative content associated with the at least one
subject and ii) original background content that is not associated with the at least one subject;  remove, from each original content data unit, the original background content to form each respective modified content data unit only having the
representative content associated with the at least one subject to reduce a volume of data being transmitted resulting in an increased capacity of the communication channel;  encode each respective modified content data unit in accordance with at least
one data encoding-decoding methodology;  and transmit each respective modified content data unit from the first end of the communication pipeline to a second end of the communication pipeline;  at the second end of the communication pipeline: at least
one second processor;  wherein the at least one second processor is configured to: receive each respective modified content data unit;  decode each respective modified content data unit in accordance with at least one data encoding-decoding methodology; 
generate, for each respective modified content data unit, a respective artificial background content;  and combine, for each respective modified content data unit, the representative content associated with the at least one subject and the respective
artificial background content to form each composite content data unit.
 14.  The system of claim 13, wherein the at least one trained artificial intelligence algorithm is selected from the group consisting of: i) a trained convolutional neural network algorithm, ii) a trained feedforward neural network algorithm,
iii) a trained recurrent neural network algorithm, and iv) a trained modular neural network algorithm.
 15.  The system of claim 13, wherein the trained convolutional neural network algorithm is a U-Net type trained convolutional neural network algorithm.
 16.  The system of claim 13, wherein the representative content of the at least one subject is a visual appearance of at least one person.
 17.  The system of claim 16, wherein the visual appearance of the at least one subject is a face of the at least one person.
 18.  The system of claim 17, wherein the at least one trained artificial intelligence algorithm has been trained based on a multi-dimensional morphable facial model.
 19.  The system of claim 16, wherein the at least one trained artificial intelligence algorithm has been trained based on an image database, comprising images of people that were taken at a plurality of settings and a plurality of conditions.
 20.  The system of claim 13, wherein the at least one first processor is further configured to: assign, by a labeling component of the at least one trained artificial intelligence algorithm, a particular label to the at least one subject,
classify, by a classification component of the at least one trained artificial intelligence algorithm, pixels of a visual content of each original content data unit into at least three classes: i) foreground pixels, ii) background pixels, and iii)
unknown pixels;  perform matting, by a matting component of the at least one trained artificial intelligence algorithm, the foreground pixels and the background pixels of the visual content of each original content data to an aligned average shape mask
associated with the at least one subject to identify the representative content associated with the at least one subject.
 21.  The system of claim 13, wherein the system further comprises: at least one third processor;  wherein the at least one third processor is configured to: train at least one artificial intelligence algorithm based on an image database until a
loss function reaches a pre-determined acceptable threshold to obtain the at least one trained artificial intelligence algorithm.
 22.  The system of claim 21, wherein the pre-determined acceptable threshold varies from 1 to 15 percent.
 23.  The system of claim 22, wherein the pre-determined acceptable threshold is between 1 and 5 percent.
 24.  The system of claim 13, wherein the respective artificial background content comprises at least one of: i) at least one uniform color, ii) at least one color gradient, and iii) at least one artificial image. 
"
"10,141,006","
     November 27, 2018
","Artificial intelligence system for improving accessibility of digitized
     speech
"," Described are techniques for automatically improving the accessibility of
     webpages and other content using machine learning and artificial
     intelligence systems. Webpage data may include visual data used to render
     visible elements and audio data used to render audible elements, such as
     digitized speech representative of at least a portion of the visible
     elements. In some cases, text data may be generated based on the audio
     data. The audio data may be modified based on target text strings,
     patterns, and characteristics determined in the text data, or the audio
     data may be analyzed directly. Additionally, user interactions with
     particular visible elements and corresponding audible elements may be
     compared. If the user interactions for a visible element exceed the user
     interactions for a corresponding audible element, the audio data
     associated with the audible element may be modified.
",G06F 3/167 (20130101); G10L 15/26 (20130101); G09B 21/006 (20130101); G06F 3/04812 (20130101); G10L 21/00 (20130101); G06F 40/20 (20200101); G10L 2021/065 (20130101),G06F 17/27 (20060101); G10L 15/26 (20060101); G10L 21/16 (20130101); G10L 15/18 (20130101); G09B 21/00 (20060101); G10L 15/01 (20130101); G10L 21/06 (20130101); G06F 3/0481 (20130101),"[['\n6745163', '\nJune 2004'], ['\n7818664', '\nOctober 2010'], ['\n7865365', '\nJanuary 2011'], ['\n8862985', '\nOctober 2014'], ['\n2006/0150110', '\nJuly 2006'], ['\n2010/0070872', '\nMarch 2010'], ['\n2014/0058733', '\nFebruary 2014'], ['\n2014/0195222', '\nJuly 2014'], ['\n2014/0324438', '\nOctober 2014'], ['\n2014/0380149', '\nDecember 2014']]","[5, '11,350,166', '11,157,682', '11,151,304', '11,061,532', '11,029,815']"," What is claimed is:  1.  An artificial intelligence system comprising: one or more memories storing computer-executable instructions;  and one or more hardware processors, to execute the
computer-executable instructions to: access webpage data including visual data for rendering visible elements of a webpage and audio data for rendering audible elements that include speech representative of at least a subset of the visible elements of
the webpage;  generate text data based on the audio data using a speech-to-text module, the text data including a transcription of the audible elements;  determine, based on the text data, one or more of: a repeated string in the text data;  a semantic
error in the text data;  a total quantity of text, in the text data, that exceeds a threshold total quantity;  a quantity of text, in the text data, that corresponds to a particular visible feature of the webpage, that is less than a threshold minimum
value;  a quantity of text, in the text data, that corresponds to the particular visible feature, that is greater than a threshold maximum value;  or a separation between at least a portion of the text data and the particular visible feature that exceeds
a threshold separation;  access first user data indicative of first user interactions with a visible element of the webpage;  determine an audible element that corresponds to the visible element;  access second user data indicative of second user
interactions with the audible element;  determine one or more differences between the first user data and the second user data, the one or more differences indicating that the first user interactions with the visible element exceed the second user
interactions with the audible element;  and based on the text data and the one or more differences, modify the audio data by one or more of: removing at least a portion of the audible element;  moving the audible element from a first location in the
webpage to a second location subsequent to the first location;  or modifying a portion of the audio data that corresponds to the one or more of the repeated text string, the semantic error, the total quantity of text that exceeds the threshold quantity,
the quantity of text that is less than the threshold minimum value, the quantity of text that is greater than the threshold maximum value, or the separation.
 2.  The system of claim 1, further comprising computer-executable instructions to: determine a first score value for the webpage based on the text data and first scoring data, wherein the first scoring data associates score values with
differences between the text data and the visual data;  determine a second score value for the webpage based on correspondence between the one or more differences and second scoring data that associates the score values with differences between the first
user data and the second user data;  determine a total score value for the webpage based on the first score value and the second score value;  and generate one or more notifications for user review of the webpage based on correspondence between the total
score value and a threshold value.
 3.  The system of claim 1, further comprising computer-executable instructions to: determine, based on the webpage data, that the webpage includes one or more sections;  determine at least a subset of the one or more sections that include an
interactive feature;  determine one or more differences between the text data and the visual data associated with the at least a subset of the one or more sections, the one or more differences between the text data and the visual data indicating one or
more of: a quantity of text, associated with the interactive feature, that is less than a threshold quantity of text;  absence of text associated with the interactive feature;  or a separation between the interactive feature and the text data associated
with the interactive feature;  and further modify the audio data based at least in part on the one or more differences between the text data and the visual data.
 4.  The system of claim 1, further comprising computer-executable instructions to: provide a modified webpage including modified audible elements based on modified audio data to one or more user devices;  determine third user data indicative of
user interactions with the modified audible elements;  determine one or more differences between the third user data and the second user data, the one or more differences between the third user data and the second user data indicating that user
interactions with the audible element of the webpage exceed user interactions with the modified audible element of the modified webpage;  and remove one or more modifications to the audio data.
 5.  A method comprising: accessing visual data associated with a visible element of a first version of content;  accessing audio data associated with an audible element of a second version of the content, wherein the audible element corresponds
to the visible element;  determining first user data indicative of first user interactions with the visible element;  determining second user data indicative of second user interactions with the audible element;  determining one or more differences
between the first user data and the second user data;  generating text data representing at least a portion of the audio data;  determining, based on the text data, one or more of: a repeated text string in the text data;  a semantic error in the text
data;  a total quantity of text, in the text data, that exceeds a threshold total quantity;  a quantity of text, in the text data, that corresponds to a particular visible feature of the first version, that is less than a threshold minimum value;  a
quantity of text, in the text data, that corresponds to the particular visible feature, that is greater than a threshold maximum value;  or a separation between at least a portion of the text data and the particular visible feature of the first version
that exceeds a threshold separation;  and based on the text data and the one or more differences, modifying the audio data associated with the audible element by modifying a portion of the audio data that corresponds to the one or more of the repeated
text string, the semantic error, the total quantity of text that exceeds the threshold total quantity, the quantity of text that is less than the threshold minimum value, the quantity of text that is greater than the threshold maximum value, or the
separation.
 6.  The method of claim 5, wherein the audio data includes digitized speech and at least a portion of the text data includes a transcription of at least a portion of the digitized speech, and wherein the modifying of the audio data further
includes one or more of: deleting a portion of the audio data corresponding to the repeated text string, modifying the audio data corresponding to the semantic error, modifying the audio data to change the quantity of text that corresponds to the
particular visible feature, modifying the audio data to change the total quantity of text, or modifying the audio data to change a length associated with the separation.
 7.  The method of claim 5, further comprising: determining, based on the first user data and the second user data, that the first user interactions for the visible element exceed the second user interactions for the audible element;  wherein
modifying the audio data includes one or more of removing at least a portion of the audible element or moving the audible element from a first location in the content to a second location subsequent to the first location.
 8.  The method of claim 5, further comprising: determining, based on the first user data and the second user data, that the first user interactions for the visible element are less than the second user interactions for the audible element;  and
modifying the first version of the content to include the audible element.
 9.  The method of claim 5, further comprising: determining third user data indicative of third user interactions with a modified version of the content subsequent to the modifying of the audio data;  determining, based on the second user data
and the third user data, that the second user interactions for the second version of the content exceed the third user interactions for the modified version of the content;  and removing one or more modifications to the audio data.
 10.  The method of claim 5, further comprising: determining a score value for the second version of the content based on the one or more differences and scoring data, wherein the scoring data associates score values with differences between the
visual data and the audio data;  determining correspondence between the score value and a threshold value;  and wherein the modifying of the audio data is performed based at least in part on the correspondence between the score value and the threshold
value.
 11.  The method of claim 5, wherein the audio data includes digitized speech, the method further comprising: receiving user input indicating a portion of the digitized speech;  determining a portion of the audio data that corresponds to the
portion of the digitized speech;  determining one or more differences between the portion of the audio data and a corresponding portion of the first version of the content;  and modifying at least a subset the audio data associated with the portion of
the digitized speech.
 12.  The method of claim 5, further comprising: determining a section of the first version of the content that includes an interactive feature;  determining a portion of the audio data that corresponds to the section;  determining one or more
of: a quantity of audio data associated with the interactive feature that is less than a threshold quantity;  absence of audio data associated with the interactive feature;  or a separation between the interactive feature and the quantity of audio data
associated with the interactive feature;  and further modifying the audio data based at least in part on the one or more of the quantity of audio data, the absence of audio data, or the separation.
 13.  A system comprising: one or more memories storing computer-executable instructions;  and one or more hardware processors, to execute the computer-executable instructions to: access visual data associated with a visible element of a first
version of content;  access audio data associated with an audible element of a second version of the content, wherein the audible element corresponds to the visible element;  generate text data based on the audio data;  determine, based on the text data,
one or more of: a repeated string in the text data;  a semantic error in the text data;  a total quantity of text, in the text data, that exceeds a threshold total quantity;  a quantity of text, in the text data, that corresponds to a particular visible
feature of the first version, that is less than a threshold minimum value;  a quantity of text, in the text data, that corresponds to the particular visible feature, that is greater than a threshold maximum value;  or a separation between at least a
portion of the text data and the particular visible feature that exceeds a threshold separation;  determine first user data indicative of first user interactions associated with the visible element;  determine second user data indicative of second user
interactions associated with the audible element;  determine, based on the first user data and the second user data, that the first user interactions exceeded the second user interactions, and further based on the text data, modify one or more of the
visual data or the audio data by one or more of: removing a portion of the visible element;  removing a portion of the audible element;  moving the portion of the visible element from a first location to a second location;  moving the portion of the
audible element from a third location to a fourth location;  or modifying a portion of the audio data that corresponds to the one or more of the repeated text string, the semantic error, the total quantity of text that exceeds the threshold quantity, the
quantity of text that is less than the threshold minimum value, the quantity of text that is greater than the threshold maximum value, or the separation.
 14.  The system of claim 13, further comprising computer-executable instructions to: determine, based on the audio data and the visual data, one or more differences between the first version of the content and the second version of the content; 
and based on the one or more differences, further modify the audio data.
 15.  The system of claim 13, wherein the audio data includes digitized speech, and wherein the text data includes a transcription of at least a portion of the digitized speech.
 16.  The system of claim 13, further comprising computer-executable instructions to: determine, based on the second user data, a first set of user characteristics and a second set of user characteristics;  wherein modifying the one or more of
the visual data or the audio data includes generating a first audio version of the content and a second audio version of the content, the first audio version including one or more features that correspond to the first set of user characteristics and the
second audio version including one or more features that correspond to the second set of user characteristics.
 17.  The system of claim 13, further comprising computer-executable instructions to: determine, based on the first user data and the second user data, that user interactions associated with a second element of the first version of the content
are less than user interactions associated with a portion of the second version of the content that corresponds to the second element;  and modify the second element of the first version of the content.
 18.  The system of claim 13, further comprising computer-executable instructions to: provide a modified version of the content based on a modified version of the one or more of the visual data or the audio data to one or more user devices; 
determine third user data indicative of third user interactions with the modified version of the content;  determine, based on the second user data and the third user data, that the third user interactions associated with the second version of the
content exceed the second user interactions associated with the modified version of the content;  and remove one or more modifications to the one or more of the visual data or the audio data.
 19.  The system of claim 13, further comprising computer-executable instructions to: determine a score value for the second version of the content based on the first user data, the second user data, and scoring data, wherein the scoring data
associates score values with differences between sets of user data;  and determine correspondence between the score value and modification data, wherein the modification data associates modifications with the score values;  wherein modifying of the audio
data is performed based at least in part on the correspondence between the score value and the modification data.
 20.  The system of claim 13, wherein the audio data includes digitized speech, the system further comprising computer-executable instructions to: generate text data based on the audio data;  receive user input indicating a portion of the
digitized speech;  determine a portion of the text data that corresponds to the portion of the digitized speech;  determine a portion of the audio data that corresponds to the portion of the text data;  determine one or more differences between the
portion of the text data and a corresponding portion of the first version of the content;  and modify the portion of the audio data.  "
"10,142,909","
     November 27, 2018
","Artificial intelligence-augmented, ripple-diamond-chain shaped rateless
     routing in wireless mesh networks with multi-beam directional antennas
"," Disclosed herein are systems and methods directed to routing in the
     wireless mesh network (WMN) with multi-beam directional antennas (MBDAs).
     The disclosed systems and methods describe Ripple-Diamond-Chain (RDC)
     shaped routing, systematic link quality modeling and artificial
     intelligence (AI) augmented path link selection. In simulations,
     real-time video as well as other types of traffic types are used to
     validate the high-throughput, quality of service (QoS)-differentiated,
     multi-beam routing efficiency of the disclosed systems and methods, as
     well as the intelligent path determination in dynamic WMN environments.
",G06N 7/005 (20130101); H04W 40/38 (20130101); G06N 3/006 (20130101); H04W 40/06 (20130101); H04L 45/24 (20130101); G06N 20/00 (20190101); H04L 45/123 (20130101); H04W 40/02 (20130101); H04W 84/18 (20130101),H04L 12/58 (20060101); H04L 12/707 (20130101); H04L 12/721 (20130101); G06N 99/00 (20100101); H04W 40/02 (20090101); H04W 40/38 (20090101); G06N 7/00 (20060101); H04W 40/06 (20090101); H04W 84/18 (20090101),"[['\n2003/0235175', '\nDecember 2003'], ['\n2006/0120370', '\nJune 2006'], ['\n2007/0206521', '\nSeptember 2007'], ['\n2007/0211636', '\nSeptember 2007'], ['\n2009/0073921', '\nMarch 2009'], ['\n2011/0182253', '\nJuly 2011']]",[1]," What is claimed is:  1.  A method of routing data between a source and a destination in a wireless mesh network (WMN), the method comprising: defining a main path between the source and
destination in the WMN, wherein the main path comprises a plurality of nodes in the WMN, each node configured to receive and transmit data, wherein at least one node is associated with a multi-beam directional antenna (MBDA);  defining one or more side
paths of the main path, wherein the one or more side paths comprise a plurality of nodes in the WMN, each node configured to receive and transmit data;  routing data between the source and the destination through the main path and the one or more side
paths, wherein at least a portion of the data being routed from the source to the destination diverges from the main path to at least one of the one or more side paths and converges from the at least one of the one or more side paths to the main path
while being routed from the source to the destination;  and using a cumulative distribution function (CDF) metric to characterize a statistical distribution of feedback delays during the routing of the data between the source and the destination through
the main path and the one or more side paths, a capture effect metric that is used to characterize a probability of an MBDA being associated with at least one node being captured, and a diamond transmission probability metric that is used to measure a
probability of a node becoming part of the main path.
 2.  The method of claim 1, wherein the at least a portion of the data being routed from the source to the destination diverging from the main path to the one or more side paths and converging from the side path to the main path while being
routed from the source to the destination comprises the at least a portion of data being routed from at least one node having a first ripple identifier to a plurality of nodes having a second ripple identifier, and then from the plurality of nodes having
the second ripple identifier to at least one node having a third ripple identifier.
 3.  The method of claim 1, wherein the method further comprises a multi-beam-oriented transmission control scheme that is used to synchronize beam communication associated with the MBDA during the routing of the data between the source and the
destination through the main path and the one or more side paths.
 4.  The method of claim 1, wherein the method further comprises the CDF metric, the capture effect metric, and the diamond transmission probability metric being adjusted based on different quality-of-service (QoS) requirements associated with
the data.
 5.  The method of claim 1, wherein fuzzy logic (FL) is used to obtain a fused metric that comprises the CDF metric, the capture effect metric, and the diamond transmission probability metric, wherein the fused metric measures a dynamic node
quality of at least one node, wherein the fused metric is used for a reward calculation in a reinforcement learning (RL)-based path search.
 6.  The method of claim 5, wherein the FL further comprises a Simple Additive Weighting Method (SAW) that includes a decision matrix, and the decision matrix comprises weights that are adjusted based on different QoS requirements for the data.
 7.  The method of claim 1, wherein online learning is used while routing data between the source and the destination through the main path and the one or more side paths, wherein the CDF metric is obtained through the online learning, and
wherein the online learning algorithm comprises Gaussian approximation or Maximum-Likelihood (ML) estimation.
 8.  The method of claim 1, wherein a neighbor table is associated with a node and the neighbor table contains information about one or more accessible one-hop and one or more two-hop neighbor nodes from the said node and one or more beam IDs
through which the said node can reach one or more of the one-hop neighbor nodes and the two-hop neighbor nodes.
 9.  The method of claim 1, wherein a Q-learning process is implemented for the routing of the data between the source and the destination in the WMN, said Q-learning process comprises a Markov decision process (MDP) process.
 10.  The method of claim 1, wherein the routing of the data between the source and the destination in the WMN is determined by maximizing expected rewards of all the nodes and a Bellman equation is used to perform an online policy search based
on one or more cumulative rewards of all the nodes.
 11.  A system for routing data between a source and a destination in a WMN, the system comprising: a plurality of nodes, wherein a portion of the nodes are associated with an MBDA;  a processor configured to execute computer-implemented code; 
wherein the processor executes the computer-implemented code to: define a main path between the source and destination in the WMN, wherein the main path comprises a portion of the plurality of nodes in the WMN, each node configured to receive and
transmit data;  define one or more side paths of the main path, wherein the one or more side paths comprise a portion of the plurality of nodes in the WMN, each node configured to receive and transmit data;  route data between the source and the
destination through the main path and the one or more side paths, wherein at least a portion of the data being routed from the source to the destination diverges from the main path to at least one of the one or more side paths and converges from the at
least one of the one or more side paths to the main path while being routed from the source to the destination;  and determine a cumulative distribution function (CDF) metric to characterize a statistical distribution of feedback delays during the
routing of the data between the source and the destination through the main path and the one or more side paths, to determine a capture effect metric that is used to characterize a probability of an MBDA being associated with at least one node being
captured, and to determine a diamond transmission probability metric that is used to measure a probability of a node becoming part of the main path.
 12.  The system of claim 11, wherein the at least a portion of the data being routed from the source to the destination diverging from the main path to the one or more side paths and converging from the side path to the main path while being
routed from the source to the destination comprises the at least a portion of data being routed from at least one node having a first ripple identifier to a plurality of nodes having a second ripple identifier, and then from the plurality of nodes having
the second ripple identifier to at least one node having a third ripple identifier.
 13.  The system of claim 11, wherein the system further comprises the processor executing the computer-implemented code to implement a multi-beam-oriented transmission control scheme that is used to synchronize beam communication associated with
the MBDA during the routing of the data between the source and the destination through the main path and the one or more side paths.
 14.  The system of claim 11, wherein the system further comprises the processor executing the computer-implemented code to adjust the CDF metric, the capture effect metric, and the diamond transmission probability metric based on different
quality-of-service (QoS) requirements associated with the data.
 15.  The system of claim 11, wherein the system further comprises the processor executing the computer-implemented code to implement fuzzy logic (FL) to obtain a fused metric that comprises the CDF metric, the capture effect metric, and the
diamond transmission probability metric, wherein the fused metric measures a dynamic node quality of at least one node, wherein the fused metric is used for a reward calculation in a reinforcement learning (RL)-based path search.
 16.  The system of claim 11, wherein the system further comprises the processor executing the computer-implemented code to implement online learning while routing data between the source and the destination through the main path and the one or
more side paths, wherein the CDF metric is obtained through the online learning, and wherein the online learning algorithm comprises Gaussian approximation or Maximum-Likelihood (ML) estimation.
 17.  The system of claim 11, wherein a neighbor table is associated with a node and the neighbor table contains information about one or more accessible one-hop and one or more two-hop neighbor nodes from the said node and one or more beam IDs
through which the said node can reach one or more of the one-hop neighbor nodes and the two-hop neighbor nodes.
 18.  The system of claim 15, wherein the FL further comprises a Simple Additive Weighting Method (SAW), wherein the SAW comprises a decision matrix, and wherein the decision matrix comprises weights that are adjusted based on different QoS
requirements for the data.
 19.  The system of claim 11, wherein the system further comprises the processor executing the computer-implemented code to implement a Q-learning process for the routing of the data between the source and the destination in the WMN and wherein
the Q-learning comprises a Markov decision process (MDP) process.
 20.  The system of claim 11, wherein the routing of the data between the source and the destination in the WMN is determined by maximizing expected rewards of all the nodes, wherein a Bellman equation to perform an online policy search based on
one or more cumulative rewards of all the nodes.  "
"10,147,216","
     December 4, 2018
","Intelligent camera
"," Presented here is technology to efficiently process camera images to
     generate artistic images and videos using an artificial intelligence
     module receiving inputs from multiple sensors. Multiple sensors can
     include a depth sensor, a conventional camera, and a motion tracker
     providing inputs to the artificial intelligence module. Based on the
     inputs, the artificial intelligence module can segment the received image
     and/or video into a foreground image and a background image to produce
     portrait imagery by blurring the background image and/or video. The
     artificial intelligence module can select the most aesthetically pleasing
     image from a video. In addition, the artificial intelligence module can
     adjust lighting in an image or video to create artistic lighting effects.
     All the processing can be done in real time due to efficient combination
     of artificial intelligence modules, traditional image processing
     techniques, and use of specialized hardware.
",G06N 3/0454 (20130101); G06V 30/194 (20220101); H04N 7/185 (20130101); G06N 3/04 (20130101); G06T 5/50 (20130101); G06K 9/6267 (20130101); G06N 3/08 (20130101); G06K 9/6274 (20130101); G06T 7/194 (20170101); G06T 11/60 (20130101); G06T 5/002 (20130101); G06T 7/11 (20170101); G06T 2207/10024 (20130101); H04N 7/18 (20130101); G06T 2207/10028 (20130101); G06T 2207/20081 (20130101); G06T 2207/30196 (20130101); G06T 2207/20084 (20130101),G06N 3/08 (20060101); G06T 7/194 (20170101); H04N 7/18 (20060101); G06T 11/60 (20060101); G06T 5/00 (20060101); G06K 9/66 (20060101); G06N 3/04 (20060101); G06K 9/62 (20060101),"[['\n7003136', '\nFebruary 2006'], ['\n9223781', '\nDecember 2015'], ['\n9704231', '\nJuly 2017'], ['\n9734567', '\nAugust 2017'], ['\n2008/0064377', '\nMarch 2008'], ['\n2015/0101064', '\nApril 2015'], ['\n2017/0155887', '\nJune 2017']]","[4, '11,258,962', '10,979,625', '10,937,216', '10,339,689']"," The invention claimed is:  1.  A method to create a portrait image in real time with a cell phone camera, the method comprising: receiving an image from a camera and a depth measurement from a
depth sensor, the camera and the depth sensor substantially collocated with each other;  separating the image into an image of a foreground object and an image of a background object using the depth measurement and a neural network trained to receive the
image and the depth measurement and to identify the foreground object and the background object, the neural network running on a processor optimized to execute operations associated with the neural network wherein the neural network comprises a plurality
of layers arranged sequentially, each layer in the plurality of layers comprising a plurality of nodes performing a plurality of computations in parallel;  measuring an amount of time associated with separating the image into the image of the foreground
object and the image of the background object;  when the amount of time exceeds a predetermined amount of time, distributing a first plurality of nodes associated with a first layer in the plurality of layers across multiple processors associated with a
cell phone until the amount of time is below the predetermined amount of time;  blurring the image of the background object;  and overlaying the image of the foreground object and the blurred image of the background object to obtain the portrait image.
 2.  A method comprising: receiving a visual representation from a light sensor and a depth representation from a depth sensor, the light sensor and the depth sensor substantially collocated with each other;  separating the visual representation
into a visual representation of a foreground object and a visual representation of a background object using the depth representation and an artificial intelligence module trained to receive the visual representation and to identify the foreground object
and the background object, said separating the visual representation comprising: receiving the visual representation of the foreground object and the visual representation of the background object from the artificial intelligence module;  verifying a
classification of an object in the visual representation into the visual representation of the foreground object and the visual representation of the background object by checking whether a first depth associated with the foreground object in the depth
representation and a second depth associated with the background object in the depth representation is below a predetermined threshold;  when the classification of the object should be changed, reclassifying the object into one of the visual
representation associated with the foreground object or the visual representation associated with the background object;  modifying the visual representation of the background object;  and combining the visual representation of the foreground object and
the modified visual representation of the background object to obtain a novel visual representation.
 3.  The method of claim 2, wherein the artificial intelligence module comprises a plurality of layers arranged sequentially, each layer in the plurality of layers comprising a plurality of nodes performing a plurality of computations in
parallel, the method comprising: measuring an amount of time associated with separating the visual representation into the visual representation of the foreground object and the visual representation of the background object;  and when the amount of time
exceeds a predetermined amount of time, distributing a first plurality of nodes associated with a first layer in the plurality of layers across multiple processors associated with a cell phone until the amount of time is below the predetermined amount of
time.
 4.  The method of claim 2, said separating the visual representation comprising: receiving the visual representation of the foreground object and the visual representation of the background object from the artificial intelligence module; 
verifying the classification of the object in the visual representation into the visual representation of the foreground object and the visual representation of the background object using a second artificial intelligence module trained to receive the
visual representation of the foreground object, the visual representation of the background object and the depth representation and to identify whether the classification of the object should be changed;  and when the classification of the object should
be changed, reclassifying the object into one of the visual representation associated with the foreground object or the visual representation associated with the background object.
 5.  The method of claim 2, said checking comprising: determining an average distance between the foreground object and the background object;  and when the average distance between the foreground object and the background object is below the
predetermined threshold, reclassifying the background object into the visual representation of the foreground object.
 6.  The method of claim 2, said checking comprising: determining a distance between a farthest point on the foreground object and a farthest point on the background object;  and when the distance is below the predetermined threshold,
reclassifying the background object into the visual representation of the foreground object.
 7.  The method of claim 2, the predetermined threshold comprising a multiple of a difference between a closest point associated with the foreground object and a farthest point associated with the foreground object.
 8.  The method of claim 2, comprising: comparing a magnitude of motion associated with a plurality of objects in the visual representation;  and associating a first object in the plurality of objects having a higher magnitude of motion with the
visual representation of the foreground object, and a second object in the plurality of objects having a lower magnitude of motion with the visual representation of the background object.
 9.  The method of claim 2, comprising: determining a location associated with the light sensor and the depth sensor;  retrieving a three-dimensional map from memory representing a first plurality of objects surrounding the light sensor and the
depth sensor;  establishing a correspondence between the first plurality of objects associated with the three-dimensional map and a second plurality of objects associated with the visual representation;  and separating the visual representation into the
visual representation of the foreground object and the visual representation of the background object based on the correspondence.
 10.  A system comprising: a light sensor to record a visual representation of an environment surrounding the light sensor;  a depth sensor to record a depth representation of the environment surrounding the depth sensor, the light sensor and the
depth sensor substantially collocated with each other;  an artificial intelligence module trained to receive the visual representation and the depth representation, to identify a foreground object and a background object and to separate the visual
representation into a visual representation of the foreground object and a visual representation of the background object;  and a processor to: receive the visual representation of the foreground object and the visual representation of the background
object from the artificial intelligence module;  verify a classification of an object in the visual representation into the visual representation of the foreground object and the visual representation of the background object by checking whether a first
depth associated with the foreground object in the depth representation and a second depth associated with the background object in the depth representation is below a predetermined threshold;  when the classification of the object should be changed,
reclassify the object into one of the visual representation associated with the foreground object or the visual representation associated with the background object;  and modify the visual representation of the background object and to combine the visual
representation of the foreground object and the modified visual presentation of the background object to obtain a novel visual representation.
 11.  The system of claim 10, the system comprising: the artificial intelligence module comprising a plurality of layers arranged sequentially, each layer in the plurality of layers comprising a plurality of nodes performing a plurality of
computations in parallel;  the processor to measure an amount of time associated with said separating the visual representation into the visual representation of the foreground object and the visual representation of the background object;  and when the
amount of time exceeds a predetermined amount of time, the processor to distribute a first plurality of nodes associated with a first layer in the plurality of layers across multiple processors associated with a cell phone until the amount of time is
below the predetermined amount of time.
 12.  The system of claim 10, the processor to check comprising: the processor to determine an average distance between the foreground object and the background object;  and when the average distance between the foreground object and the
background object is below the predetermined threshold, the processor to reclassify the background object into the visual representation of the foreground object.
 13.  The system of claim 10, the processor to check comprising: the processor to determine a distance between a farthest point on the foreground object and a farthest point on the background object;  and when the distance is below the
predetermined threshold, the processor to reclassify the background object into the visual representation of the foreground object.
 14.  The system of claim 10, the predetermined threshold comprising a fraction of a difference between a closest point associated with the foreground object and a farthest point associated with the foreground object.
 15.  The system of claim 10, comprising: the processor to compare a magnitude of motion associated with a plurality of objects in the visual representation;  and the processor to associate a first object in the plurality of objects having a
higher magnitude of motion with the visual representation of the foreground object, and a second object in the plurality of objects having a lower magnitude of motion with the visual representation of the background object.
 16.  The system of claim 10, comprising: a locator to determine a location associated with the light sensor and the depth sensor;  the processor to retrieve a three-dimensional map from a memory representing a first plurality of objects
surrounding the light sensor and the depth sensor;  the processor to establish a correspondence between the first plurality of objects associated with the three-dimensional map and a second plurality of objects associated with the visual representation; 
and the processor to separate the visual representation into the visual representation of the foreground object and the visual representation of the background object based on the correspondence.  "
"10,147,244","
     December 4, 2018
","Fault source parameter identification
"," A first air data value is generated based on a first set of parameters. A
     second set of parameters that does not include any of the first set of
     parameters is processed through an artificial intelligence network to
     generate a second air data value. The second set of parameters is
     processed through a plurality of diagnostic artificial intelligence
     networks to generate a plurality of diagnostic air data values. Each of
     the plurality of diagnostic artificial intelligence networks excludes a
     different one of the second set of parameters. One of the second set of
     parameters is identified, based on the first air data value and the
     plurality of diagnostic air data values, as a fault source parameter that
     is associated with a fault condition.
",G01P 5/16 (20130101); G01P 21/025 (20130101); B64C 13/505 (20180101); G01P 13/025 (20130101); G07C 5/0808 (20130101); B64D 43/00 (20130101),G07C 5/08 (20060101); G01P 5/16 (20060101); G01P 21/02 (20060101); G01P 13/02 (20060101); B64C 13/50 (20060101),"[['\n5919267', '\nJuly 1999'], ['\n6502042', '\nDecember 2002'], ['\n7257470', '\nAugust 2007'], ['\n7660774', '\nFebruary 2010'], ['\n7734400', '\nJune 2010'], ['\n8209083', '\nJune 2012'], ['\n8352216', '\nJanuary 2013'], ['\n8521341', '\nAugust 2013'], ['\n8572009', '\nOctober 2013'], ['\n8942882', '\nJanuary 2015'], ['\n2006/0212181', '\nSeptember 2006'], ['\n2007/0130096', '\nJune 2007'], ['\n2010/0100260', '\nApril 2010'], ['\n2015/0100184', '\nApril 2015'], ['\n2016/0004255', '\nJanuary 2016'], ['\n2016/0075444', '\nMarch 2016'], ['\n2017/0158347', '\nJune 2017']]","[2, '11,008,088', '10,864,978']"," The invention claimed is:  1.  A method comprising: sensing a first set of parameters;  receiving a second set of parameters that does not include any of the first set of parameters;  generating
a first air data value based on the first set of parameters;  processing the second set of parameters through an artificial intelligence network to generate a second air data value;  processing the second set of parameters through a plurality of
diagnostic artificial intelligence networks to generate a plurality of diagnostic air data values, each of the plurality of diagnostic artificial intelligence networks excluding a different one of the second set of parameters;  and identifying, based on
the first air data value and the plurality of diagnostic air data values, one of the second set of parameters as a fault source parameter that is associated with a fault condition.
 2.  The method of claim 1, further comprising: determining that the first air data value deviates from the second air data value by a threshold amount;  and initiating the processing of the second set of parameters through the plurality of
diagnostic artificial intelligence networks in response to determining that the first air data value deviates from the second air data value by the threshold amount.
 3.  The method of claim 1, wherein identifying the fault source parameter comprises: comparing each of the plurality of diagnostic air data values to the first air data value;  identifying one of the plurality of diagnostic air data values that
is in agreement with the first air data value;  identifying, as the fault source parameter, the one of the second set of parameters that was excluded during generation of the identified one of the plurality of diagnostic air data values that is in
agreement with the first air data value.
 4.  The method of claim 3, wherein identifying the one of the plurality of diagnostic air data values that is in agreement with the first air data value comprises identifying the one of the plurality of diagnostic air data values that is within
a threshold deviation from the first air data value.
 5.  The method of claim 1, further comprising: outputting an indication of the fault source parameter.
 6.  The method of claim 1, wherein receiving the second set of parameters comprises receiving one or more of an aircraft engine thrust parameter, an aircraft engine throttle setting, a flight control surface position, a flight control surface
loading, an aircraft fuel usage rate, an aircraft weight, a landing gear position, an aircraft mass balance, an aircraft acceleration, an aircraft angular rate, an aircraft calibrated airspeed, an aircraft true airspeed, an aircraft Mach number, an
aircraft pressure altitude, an aircraft angle of attack, an aircraft vertical speed, and an aircraft angle of sideslip.
 7.  The method of claim 1, wherein sensing the first set of parameters comprises sensing one or more of a static air pressure and a total air pressure.
 8.  The method of claim 1, wherein each of the first air data value and the second air data value comprise one of an aircraft calibrated airspeed, an aircraft true airspeed, an aircraft Mach number, an aircraft pressure altitude, an aircraft
angle of attack, an aircraft vertical speed, and an aircraft angle of sideslip.
 9.  The method of claim 1, wherein the artificial intelligence network and each of the plurality of diagnostic artificial intelligence networks comprise separate artificial neural networks, each having at least one internal layer of neurons that
apply one or more weights, biases, or transfer functions.
 10.  The method of claim 9, wherein the artificial intelligence network and each of the plurality of diagnostic artificial intelligence networks are pre-trained to determine the one or more weights, biases, or transfer functions.
 11.  A system comprising: one or more processors;  and non-transitory computer-readable memory encoded with instructions that, when executed by the one or more processors, cause the system to: generate a first air data value based on a first set
of parameters;  process a second set of parameters that does not include any of the first set of parameters through an artificial intelligence network to generate a second air data value;  process the second set of parameters through a plurality of
diagnostic artificial intelligence networks to generate a plurality of diagnostic air data values, each of the plurality of diagnostic artificial intelligence networks excluding a different one of the second set of parameters;  and identify, based on the
first air data value and the plurality of diagnostic air data values, one of the second set of parameters as a fault source parameter that is associated with a fault condition.
 12.  The system of claim 11, wherein the computer-readable memory is further encoded with instructions that, when executed by the one or more processors, cause the system to: determine that the first air data value deviates from the second air
data value by a threshold amount;  and initiate the processing of the second set of parameters through the plurality of diagnostic artificial intelligence networks in response to determining that the first air data value deviates from the second air data
value by the threshold amount.
 13.  The system of claim 11, wherein the computer-readable memory is further encoded with instructions that, when executed by the one or more processors, cause the system to identify the fault source parameter by at least causing the system to:
compare each of the plurality of diagnostic air data values to the first air data value;  identify one of the plurality of diagnostic air data values that is in agreement with the first air data value;  and identify, as the fault source parameter, the
one of the second set of parameters that was excluded during generation of the identified one of the plurality of diagnostic air data values that is in agreement with the first air data value.
 14.  The system of claim 13, wherein the computer-readable memory is further encoded with instructions that, when executed by the one or more processors, cause the system to identify the one of the plurality of diagnostic air data values that is
in agreement with the first air data value by at least causing the system to: identify the one of the plurality of diagnostic values that is within a threshold deviation from the first air data value.
 15.  The system of claim 11, wherein the computer-readable memory is further encoded with instructions that, when executed by the one or more processors, cause the system to output an indication of the fault source parameter.
 16.  The system of claim 11, wherein the second set of parameters comprises one or more of an aircraft engine thrust parameter, an aircraft engine throttle setting, a flight control surface position, a flight control surface loading, an aircraft
fuel usage rate, an aircraft weight, a landing gear position, an aircraft mass balance, an aircraft acceleration, an aircraft angular rate, aircraft calibrated airspeed, an aircraft true airspeed, an aircraft Mach number, an aircraft pressure altitude,
an aircraft angle of attack, an aircraft vertical speed, and an aircraft angle of sideslip.
 17.  The system of claim 11, further comprising: one or more sensors configured to sense the first set of parameters;  and an air data computer operatively coupled to the one or more sensors to receive the first set of parameters and generate
the first air data value based on the first set of parameters.
 18.  The system of claim 17, wherein the first set of parameters comprises one of more of a static air pressure and a total air pressure.
 19.  The system of claim 11, wherein each of the first air data value and the second air data value comprise one of an aircraft calibrated airspeed, an aircraft true airspeed, an aircraft Mach number, an aircraft pressure altitude, an aircraft
angle of attack, an aircraft vertical speed, and an aircraft angle of sideslip.
 20.  The system of claim 11, wherein the artificial intelligence network and each of the plurality of diagnostic artificial intelligence networks comprise separate artificial neural networks, each having at least one internal layer of neurons
that apply one or more pre-determined weights, biases, or transfer functions.  "
"10,148,692","
     December 4, 2018
","Aggregation of asynchronous trust outcomes in a mobile device
"," Systems and techniques are provided for aggregation of asynchronous trust
     outcomes in a mobile device. Trust levels may be determined from the
     signals. Each trust level may be determined independently of any other
     trust level. Each trust level may be determined based on applying to the
     signals heuristics, mathematical optimization, decisions trees, machine
     learning systems, or artificial intelligence systems. An aggregated trust
     outcome may be determined by aggregating the trust levels. Aggregating
     the trust levels may include applying heuristics, mathematical
     optimization, decisions trees, machine learning systems, or artificial
     intelligence systems to the trust levels, and wherein the aggregated
     trust outcome; and sending the aggregated trust outcome to be implemented
     by the enabling, disabling, or relaxing of at least one security measure
     based on the aggregated trust outcome.
",H04W 12/30 (20210101); H04W 12/06 (20130101); H04L 63/20 (20130101); H04W 12/65 (20210101); H04W 12/63 (20210101); H04W 88/02 (20130101),H04W 12/06 (20090101); H04L 29/06 (20060101); H04W 88/02 (20090101),"[['\n8387141', '\nFebruary 2013'], ['\n8412158', '\nApril 2013'], ['\n8713704', '\nApril 2014'], ['\n9633184', '\nApril 2017'], ['\n2006/0074986', '\nApril 2006'], ['\n2007/0150745', '\nJune 2007'], ['\n2008/0101658', '\nMay 2008'], ['\n2011/0016534', '\nJanuary 2011'], ['\n2012/0007713', '\nJanuary 2012'], ['\n2013/0055348', '\nFebruary 2013'], ['\n2013/0061305', '\nMarch 2013'], ['\n2013/0067566', '\nMarch 2013'], ['\n2013/0227678', '\nAugust 2013'], ['\n2014/0010417', '\nJanuary 2014'], ['\n2014/0033326', '\nJanuary 2014'], ['\n2014/0096231', '\nApril 2014'], ['\n2014/0289820', '\nSeptember 2014'], ['\n2014/0289833', '\nSeptember 2014'], ['\n2014/0380424', '\nDecember 2014']]",[1]," The invention claimed is:  1.  A computer-implemented method performed by a data processing apparatus, the method comprising: receiving one or more signals from one or more sensors, the one or
more sensors comprising at least one hardware sensor;  determining at least two trust levels from the one or more signals, wherein a first trust level of the at least two trust levels is determined independent of a determination of a second trust level
and without using the second trust level, and wherein the second trust level of the at least two trust levels is determined independent of the determination of the first trust level and without using the first trust level;  determining a first aggregated
trust outcome by aggregating the at least two trust levels, wherein the first aggregated trust outcome is a granular aggregated trust outcome associated with a first security measure of a computing device;  determining a second aggregated trust outcome
by aggregating the at least two trust levels, wherein the second aggregated trust outcome is a granular aggregated trust outcome associated with a second security measure different from the first security measure of the computing device;  wherein the
first aggregated trust outcome is determined independent of the second aggregated trust outcome and without using the second aggregated trust outcome;  wherein the second aggregated trust outcome is determined independent of the first aggregated trust
outcome and without using the first aggregated trust outcome;  enabling or disabling the first security measure based on the first granular aggregated trust outcome, wherein the first security measure is not associated with the second granular aggregated
trust outcome and the second granular aggregated trust outcome is not used to enable or disable the first security measure;  and enabling or disabling the second security measure based on the second granular aggregated trust outcome, wherein the second
security measure is not associated with the first granular aggregated trust outcome and the first granular aggregated trust outcome is not used to enable or disable the second security measure.
 2.  The computer-implemented method of claim 1, wherein determining one of the at least two trust levels further comprises applying data from at least one state wherein the data comprises one or more of a historical trust level and a historical
value for one of the one or more signals used to determine the one of the at least two trust levels.
 3.  The computer-implemented method of claim 1, wherein determining the first aggregated trust outcome further comprises applying at least one configuration setting.
 4.  The computer-implemented method of claim 1, wherein determining the first aggregated trust outcome further comprises applying data from a state, wherein the data comprises one or more of a historical trust level and a historical aggregated
trust outcome.
 5.  The computer-implemented method of claim 1, wherein the first aggregated trust outcome indicates a confidence level based on aggregating trust levels that a user of a mobile computing device is an authorized user of the mobile computing
device.
 6.  The computer-implemented method of claim 1, wherein one of the at least two trust levels indicates a confidence level based on analyzing received signals that a user of a mobile computing device is an authorized user of the mobile computing
device.
 7.  The computer-implemented method of claim 1, wherein the security measure is a request for credentials to unlock a mobile computing device.
 8.  A computer-implemented method performed by a data processing apparatus, the method comprising: receiving a plurality of signals from a plurality of sensors, wherein the plurality of sensors comprises hardware and software sensors of a
computing device;  determining a plurality of trust levels from the plurality of signals, wherein each of the plurality of trust levels is determined based on one or more of the plurality of signals independently from the determination of any other trust
level of the plurality of trust levels and without using any other trust level of the plurality of trust levels, and wherein the determination of at least one of the plurality of trust levels based on one or more of the plurality of signals uses a signal
from a hardware sensor;  aggregating the plurality of trust levels to determine a first aggregated trust outcome, wherein the first aggregated trust outcome is a granular aggregated trust outcome associated with a first security measure of a computing
device;  aggregating the plurality of trust levels to determine a second aggregated trust outcome, wherein the second aggregated trust outcome a granular aggregated trust outcome associated with a second security measure of the computing device;  wherein
the first aggregated trust outcome is determined independent of the second aggregated trust outcome and without using the second aggregated trust outcome;  wherein the second aggregated trust outcome is determined independent of the first aggregated
trust outcome and without using the first aggregated trust outcome;  enabling or disabling the first security measure based on the first granular aggregated trust outcome, wherein the first security measure is not associated with the second granular
aggregated trust outcome and the second granular aggregated trust outcome is not used to enable or disable the first security measure;  and enabling or disabling the second security measure based on the second granular aggregated trust outcome, wherein
the second security measure is not associated with the first granular aggregated trust outcome and the first granular aggregated trust outcome is not used to enable or disable the second security measure.
 9.  The computer-implemented method of claim 8, wherein each of the plurality of trust levels is determined by a trustlet.
 10.  The computer-implemented method of claim 8, wherein the first aggregated trust outcome is determined by a trust aggregator.
 11.  The computer-implemented method of claim 8, wherein each of the plurality of trust levels is a confidence level that a mobile computing device is either being used by an authorized user or is in a secure environment.
 12.  The computer-implemented method of claim 8, wherein the first aggregated trust outcome is a confidence level that a mobile computing device is either being used by an authorized user or is in a secure environment.
 13.  The computer-implemented method of claim 9, wherein at least one trustlet determines one of the plurality of trust levels based in part on a state associated with the trustlet.
 14.  The computer-implemented method of claim 10, wherein the trust aggregator determines the aggregated trust outcome based in part on a state of the trust aggregator.
 15.  The computer-implemented method of claim 10, wherein the trust aggregator determines the aggregated trust outcome based in part on configuration settings for the trust aggregator.
 16.  The computer-implemented method of claim 8, further comprising enabling at least one security measure of a mobile computing device when the first aggregated trust outcome is below a threshold.
 17.  The computer-implemented method of claim 8, further comprising disabling at least on security measure of a mobile computing device when the first aggregated trust outcome is above a threshold.
 18.  A computer-implemented system for aggregation of asynchronous trust outcomes comprising: a storage comprising configuration settings;  one or more sensors, wherein at least one of the one or more sensors is a hardware sensor, each sensor
adapted to generate at least one signal;  at least two trustlets, each of the at least two trustlets adapted to receive at least one signal from the one or more sensors and determine a trust level, a first of the at least two trustlets adapted to
determine a first trust level from at least one signal from the one or more sensors including at least one signal from a hardware sensor independently of a determination of a trust level by a second of the at least two trustlets and without using the
trust level determined by the second of the at least two trustlets, and the second of the at least two trustlets adapted to determine a trust level from at least one signal from the one or more sensors including at least one signal from a hardware sensor
independently of the determination of a trust level by the first of the at least two trustlets and without using the trust level determined by the first of the at least two trustlets;  and a first trust aggregator adapted to aggregate the trust levels,
determine a first aggregated trust outcome, wherein the first aggregated trust outcome is a granular aggregated trust outcome associated with a first security measure of a mobile computing device, and send the first aggregated trust outcome to be
implemented by enabling or disabling the first security measure based on the first granular aggregated trust outcome, wherein the first security measure is not associated with the second granular aggregated trust outcome and the second granular
aggregated trust outcome is not used to enable or disable the first security measure;  and a second trust aggregator adapted to aggregate the trust levels, determine a second aggregated trust outcome, wherein the second aggregated trust outcome is a
granular aggregated trust outcome associated with a second security measure of a mobile computing device, wherein the first aggregated trust outcome is determined independent of the second aggregated trust outcome and without using the second aggregated
trust outcome, wherein the second aggregated trust outcome is determined independent of the first aggregated trust outcome and without using the first aggregated trust outcome, and send the second aggregated trust outcome to be implemented by enabling or
disabling the second security measure based on the second granular aggregated trust outcome, wherein the second security measure is not associated with the first granular aggregated trust outcome and the first granular aggregated trust outcome is not
used to enable or disable the second security measure.
 19.  The computer-implemented system of claim 18, further comprising a trust consumer adapted to receive the first aggregated trust outcome and disable, enable, or relax at least one security measure based on the first aggregated trust outcome.
 20.  The computer-implemented system of claim 18, wherein at least one of the trustlets is further adapted to determine a trust level using one or more of heuristics, mathematical optimization, decisions trees, machine learning systems, and
artificial intelligence systems.
 21.  The computer-implemented system of claim 18, wherein the first trust aggregator is further adapted to aggregate the trust levels and determine the first aggregated trust outcome using one or more of heuristics, mathematical optimization,
decisions trees, machine learning systems, and artificial intelligence systems.
 22.  The computer-implemented system of claim 18, wherein the first aggregated trust outcome indicates a level of confidence that a mobile computing device is being used by an authorized user.
 23.  A system comprising: one or more computers and one or more storage devices storing instructions which are operable, when executed by the one or more computers, to cause the one or more computers to perform operations comprising: receiving
one or more signals from one or more sensors, the one or more sensors comprising at least one hardware sensor;  determining at least two trust levels from the one or more signals, wherein a first trust level of the at least two trust levels is determined
independent of a determination of a second trust level and without using the second trust level, and wherein the second trust level of the at least two trust levels, and wherein each trust level is determined independent of the determination of the first
trust level and without using the first trust level;  determining a first aggregated trust outcome by aggregating the at least two trust levels, wherein the first aggregated trust outcome is a granular aggregated trust outcome associated with a first
security measure of a computing device;  determining a second aggregated trust outcome by aggregating the at least two trust levels, wherein the second aggregated trust outcome is a granular aggregated trust outcome associated with a second security
measure different from the first security measure of the computing device;  wherein the first aggregated trust outcome is determined independent of the second aggregated trust outcome and without using the second aggregated trust outcome;  wherein the
second aggregated trust outcome is determined independent of the first aggregated trust outcome and without using the first aggregated trust outcome;  enabling or disabling the first security measure based on the first granular aggregated trust outcome,
wherein the first security measure is not associated with the second granular aggregated trust outcome and the second granular aggregated trust outcome is not used to enable or disable the first security measure;  and enabling or disabling the second
security measure based on the second granular aggregated trust outcome, wherein the second security measure is not associated with the first granular aggregated trust outcome and the first granular aggregated trust outcome is not used to enable or
disable the second security measure.  "
"10,152,349","
     December 11, 2018
","Kernel scheduling based on precedence constraints and/or artificial
     intelligence techniques
"," A device may receive information that identifies a set of tasks to be
     executed and precedence constraints associated with the set of tasks. The
     device may store the set of tasks in a data structure including a
     directed acyclic graph, and may determine a set of paths based on the
     information that identifies the set of tasks and the precedence
     constraints associated with the set of tasks. Each path, of the set of
     paths, may include particular tasks of the set of tasks. The device may
     determine a set of path execution times, for the set of paths, based on
     an artificial intelligence technique. The device may determine a critical
     path, of the set of paths, based on the set of path execution times. The
     device may determine an execution priority of the set of tasks based on
     the critical path. The device may provide the set of tasks for execution
     based on the execution priority.
",G06N 5/003 (20130101); G06N 20/00 (20190101); G06N 5/02 (20130101),G06F 9/46 (20060101); G06F 9/48 (20060101); G06N 5/02 (20060101),"[['\n6490566', '\nDecember 2002'], ['\n7930700', '\nApril 2011'], ['\n8015564', '\nSeptember 2011'], ['\n8959525', '\nFebruary 2015'], ['\n2005/0256818', '\nNovember 2005'], ['\n2005/0257221', '\nNovember 2005'], ['\n2006/0288346', '\nDecember 2006'], ['\n2008/0163213', '\nJuly 2008'], ['\n2008/0263323', '\nOctober 2008'], ['\n2009/0178047', '\nJuly 2009'], ['\n2010/0004916', '\nJanuary 2010'], ['\n2010/0198776', '\nAugust 2010'], ['\n2011/0231849', '\nSeptember 2011'], ['\n2012/0147387', '\nJune 2012'], ['\n2013/0047162', '\nFebruary 2013'], ['\n2013/0145299', '\nJune 2013'], ['\n2014/0282572', '\nSeptember 2014'], ['\n2014/0317629', '\nOctober 2014'], ['\n2014/0325521', '\nOctober 2014'], ['\n2015/0206092', '\nJuly 2015'], ['\n2016/0085584', '\nMarch 2016'], ['\n2016/0085591', '\nMarch 2016'], ['\n2017/0373955', '\nDecember 2017']]","[5, '11,221,885', '11,221,877', '11,138,522', '10,999,212', '10,748,067']"," What is claimed is:  1.  A device, comprising: a memory;  and one or more processors to: receive information that identifies a set of tasks to be executed and precedence constraints associated
with the set of tasks;  store the set of tasks in a data structure including a directed acyclic graph based on the information that identifies the set of tasks to be executed and the precedence constraints associated with the set of tasks;  determine a
set of paths based on the information that identifies the set of tasks and the precedence constraints associated with the set of tasks, each path, of the set of paths, including particular tasks of the set of tasks, and at least one path, of the set of
paths, including ordered levels that correspond to the set of tasks of the at least one path;  determine a set of path execution times, for the set of paths, based on an artificial intelligence technique;  determine a critical path, of the set of paths,
based on the set of path execution times;  identify a particular task, of the set of tasks, associated with a level of the ordered levels, the particular task being associated with the critical path;  determine an execution priority of the set of tasks
based on the critical path, the particular task being prioritized over other tasks, of the set of tasks, associated with the level, and the other tasks being prioritized based on respective path execution times of their respective paths;  and provide the
set of tasks for execution based on the execution priority.
 2.  The device of claim 1, where the one or more processors are further to: identify another path of the set of paths;  determine a difference between a first path execution time of the other path and a second path execution time of the critical
path;  and perform a power scaling technique based on the difference, the first path execution time to be increased based on the power scaling technique.
 3.  The device of claim 1, where the one or more processors are further to: receive information that identifies a set of instruction metrics associated with the set of tasks;  and where the one or more processors, when determining the set of
path execution times, are to: determine the set of path execution times based on the information that identifies the set of instruction metrics.
 4.  The device of claim 1, where the one or more processors are further to: determine an execution time of a first task based on providing the set of tasks for execution;  implement the artificial intelligence technique based on the execution
time of the first task;  implement a machine learning technique based on implementing the artificial intelligence technique;  and modify another execution time of the first task and another execution priority based on implementing the machine learning
technique.
 5.  The device of claim 1, where the one or more processors are further to: cause a first voltage to be adjusted in association with a first processing unit of the device, the first processing unit to execute a first subset of tasks of the set
of tasks;  and cause a second voltage to be adjusted in association with a second processing unit of the device, the second processing unit to execute a second subset of tasks of the set of tasks, the first subset of tasks and the second subset of tasks
not being associated with the critical path.
 6.  The device of claim 1, where the one or more processors are further to: identify a first execution result of a first task based on providing the set of tasks for execution, the first task being executed in association with a first path of
the set of paths;  identify a second path, of the set of paths, that includes the first task;  and apply the first execution result of the first task in association with the second path, the first execution result of the first task being reused to
determine a second execution result of the second path.
 7.  The device of claim 1, where the one or more processors are further to: determine that the particular task has remained in the data structure for an amount of time;  and provide the particular task for execution based on the amount of time
satisfying a threshold amount of time.
 8.  A non-transitory computer-readable medium storing instructions, the instructions comprising: one or more instructions that, when executed by one or more processors, cause the one or more processors to: receive information that identifies a
set of tasks to be executed and precedence constraints associated with the set of tasks;  store the information that identifies the set of tasks and the precedence constraints in a form of a directed acyclic graph;  determine, based on the directed
acyclic graph, a set of paths;  at least one path, of the set of paths, including ordered levels that correspond to the set of tasks of the at least one path;  determine a set of path execution times for the set of paths;  determine a critical path of
the set of paths based on the set of path execution times;  identify a particular task, of the set of tasks, associated with a level of the ordered levels, the particular task being associated with the critical path;  determine an execution priority of
the set of tasks based on the critical path and the particular task, the particular task being prioritized over other tasks, of the set of tasks, associated with the level, and the other tasks being prioritized based on respective path execution times of
their respective paths;  and provide the set of tasks for execution based on the execution priority.
 9.  The non-transitory computer-readable medium of claim 8, where the one or more instructions, when executed by the one or more processors, further cause the one or more processors to: perform a power scaling technique in association with
another path of the set of paths, a first set of tasks, associated with the other path, to be provided to a processing unit for execution, and the power scaling technique being associated with the processing unit.
 10.  The non-transitory computer-readable medium of claim 8, where the one or more instructions, when executed by the one or more processors, further cause the one or more processors to: determine a second set of path execution times based on
providing the set of tasks for execution;  implement an artificial intelligence technique based on the second set of path execution times;  receive information that identifies another set of tasks to be provided for execution;  and determine, for the
other set of tasks, a third set of path execution times based on the artificial intelligence technique;  and determine another execution priority of the other set of tasks based on the third set of path execution times.
 11.  The non-transitory computer-readable medium of claim 8, where the one or more instructions, when executed by the one or more processors, further cause the one or more processors to: determine, based on the information that identifies the
set of tasks, a number of lines of code associated with a first task of the set of tasks;  determine, based on the number of lines of code, a task execution time of the first task;  and where the one or more instructions, that cause the one or more
processors to determine the set of path execution times, cause the one or more processors to: determine the set of path execution times based on the task execution time of the first task.
 12.  The non-transitory computer-readable medium of claim 8, where the one or more instructions, when executed by the one or more processors, further cause the one or more processors to: determine a first path execution time of a first path of
the set of paths;  determine a second path execution time of a second path of the set of paths;  determine that the first path execution time is greater than the second path execution time;  and where the one or more instructions, that cause the one or
more processors to determine the critical path, cause the one or more processors to: determine the critical path based on the first path execution time being greater than the second path execution time, the first path being the critical path.
 13.  The non-transitory computer-readable medium of claim 8, where the one or more instructions, when executed by the one or more processors, further cause the one or more processors to: provide a first task, of the critical path, to a first
processing unit;  and provide a second task, of another path, to a second processing unit, the first processing unit being associated with a first processing metric that satisfies a second processing metric of another processing unit.
 14.  The non-transitory computer-readable medium of claim 8, where the one or more instructions, when executed by the one or more processors, further cause the one or more processors to: determine that the particular task has been awaiting
scheduling or execution for an amount of time;  and provide the particular task for execution based on the amount of time satisfying a threshold amount of time.
 15.  A method, comprising: receiving, by a device, information that identifies a set of tasks to be executed and precedence constraints associated with the set of tasks;  storing, by the device and based on the precedence constraints, the
information that identifies the set of tasks in association with a directed acyclic graph;  determining, by the device, a set of paths based on the directed acyclic graph, at least one path, of the set of paths, including ordered levels that correspond
to the set of tasks of the at least one path;  determining, by the device, a set of path execution times, for the set of paths, based on an artificial intelligence technique;  determining, by the device, a critical path, of the set of paths, based on the
set of path execution times;  identifying, by the device, a particular task, of the set of tasks, associated with a level of the ordered levels, the particular task being associated with the critical path;  determining, by the device, an execution
priority of the set of tasks based on the critical path and the particular task, the particular task being prioritized over other tasks, of the set of tasks, associated with the level, and the other tasks being prioritized based on respective path
execution times of their respective paths;  and providing, by the device and to one or more processing units of the device, the set of tasks for execution based on the execution priority.
 16.  The method of claim 15, further comprising: identifying another path of the set of paths;  determining a difference between a first path execution time of the critical path and a second path execution time of the other path;  and performing
a power scaling technique that causes the second path execution time to be adjusted based on the difference.
 17.  The method of claim 15, further comprising: determining a first execution result of a first task of the set of tasks, the first task being associated with a first path of the set of paths;  identifying a second path, of the set of paths,
that includes the first task;  and applying the first execution result of the first task in association with the second path, the first execution result of the first task being reused to determine a second execution result of the second path.
 18.  The method of claim 15, further comprising: receiving information that identifies task execution times of the set of tasks based on providing the set of tasks to the one or more processing units;  performing the artificial intelligence
technique based on the information that identifies the task execution times;  and determining another critical path based on performing the artificial intelligence technique.
 19.  The method of claim 15, further comprising: performing a dynamic voltage scaling technique to adjust a voltage of a processing unit, of the one or more processing units, based on the set of path execution times and a path execution time of
the critical path.
 20.  The method of claim 15, further comprising: determining that the particular task has been awaiting scheduling and/or execution for an amount of time;  and providing the particular task for execution based on the amount of time satisfying a
threshold amount of time.  "
"10,157,619","
     December 18, 2018
","Method and device for searching according to speech based on artificial
     intelligence
"," A method and a device for searching according to a speech based on
     artificial intelligence are provided. The method includes: identifying an
     input speech of a user to determine whether the input speech is a child
     speech; filtrating a searched result obtained according to the input
     speech to obtain a filtrated searched result, if the input speech is the
     child speech; and feeding the filtrated searched result back to the user.
",G06F 16/433 (20190101); G10L 25/27 (20130101); G10L 17/26 (20130101); G06F 16/436 (20190101); G10L 15/26 (20130101); G10L 15/02 (20130101); G10L 25/93 (20130101); G10L 25/30 (20130101); G10L 2015/223 (20130101); G10L 2025/786 (20130101),G06F 17/30 (20060101); G10L 17/26 (20130101); G10L 25/78 (20130101); G10L 15/02 (20060101); G10L 15/26 (20060101); G10L 25/27 (20130101); G10L 25/93 (20130101); G10L 25/30 (20130101); G10L 15/22 (20060101),"[['\n4672669', '\nJune 1987'], ['\n6286001', '\nSeptember 2001'], ['\n6336117', '\nJanuary 2002'], ['\n6529875', '\nMarch 2003'], ['\n8160877', '\nApril 2012'], ['\n8417530', '\nApril 2013'], ['\n2003/0001016', '\nJanuary 2003'], ['\n2003/0105680', '\nJune 2003'], ['\n2003/0140056', '\nJuly 2003'], ['\n2004/0128282', '\nJuly 2004'], ['\n2004/0139106', '\nJuly 2004'], ['\n2007/0005570', '\nJanuary 2007'], ['\n2008/0183694', '\nJuly 2008'], ['\n2009/0070293', '\nMarch 2009'], ['\n2011/0302633', '\nDecember 2011'], ['\n2012/0072424', '\nMarch 2012'], ['\n2014/0316769', '\nOctober 2014'], ['\n2015/0032758', '\nJanuary 2015'], ['\n2015/0199969', '\nJuly 2015'], ['\n2015/0287410', '\nOctober 2015'], ['\n2016/0171109', '\nJune 2016'], ['\n2017/0061014', '\nMarch 2017'], ['\n2017/0337610', '\nNovember 2017']]",[0]," What is claimed is:  1.  A method for searching according to a speech based on artificial intelligence, comprising: acquiring, by at least one computing device, sample speeches for training a
preset classifier;  removing, by the at least one computing device, a silent speech from the sample speeches by performing a speech activity detection on the sample speeches, to obtain training speeches;  extracting, by the at least one computing device,
acoustic features of each training speech;  and training, by the at least one computing device, the preset classifier by inputting the acoustic features of the each training speech into the preset classifier, to obtain a target classifier;  identifying,
by at least one computing device, an input speech of a user to determine whether the input speech is a child speech;  filtrating, by the at least one computing device, a searched result obtained according to the input speech to obtain a filtrated
searched result, if the input speech is the child speech;  and feeding, by the at least one computing device, the filtrated searched result to the user, wherein removing, by the at least one computing device, the silent speech from the sample speeches by
performing a speech activity detection on the sample speeches, to obtain training speeches comprises: dividing, by the at least one computing device, each sample speech into frames by a preset first step size, and removing, by the at least one computing
device, the silent speech from each frame of the each sample speech by performing the speech activity detection on the each frame of the each sample speech, to obtain the each training speech;  wherein extracting, by the at least one computing device,
the acoustic features of each training speech comprises: dividing, by the at least one computing device, the each training speech by a preset second step size;  and extracting, by the at least one computing device, by a preset third step size, the
acoustic features of the each training speech after dividing by the preset second step size.
 2.  The method according to claim 1, wherein filtrating, by the at least one computing device, a searched result obtained according to the input speech comprises: converting, by the at least one computing device, the input speech into a text
content;  obtaining, by the at least one computing device, the searched result by searching according to the text content;  and filtrating, by the at least one computing device, the searched result to remove a sensitive content unsuitable for a child.
 3.  The method according to claim 2, wherein obtaining, by the at least one computing device, the searched result by searching according to the text content comprises: searching, by the at least one computing device, according to the text
content in a first database pre-established for children;  and searching, by the at least one computing device, according to the text content in a second database to obtain the searched result, if no content related to the input speech is searched in the
first database.
 4.  The method according to claim 1, wherein identifying, by the at least one computing device, an input speech of a user to determine whether the input speech is a child speech comprises: removing, by the at least one computing device, a silent
speech from the input speech by performing the speech activity detection on the input speech, to obtain a tested speech;  extracting, by the at least one computing device, acoustic features of the tested speech;  and identifying, by the at least one
computing device, the acoustic features of the tested speech by inputting the acoustic features of the tested speech into the target classifier, to determine whether the input speech is the child speech.
 5.  The method according to claim 4, wherein removing, by the at least one computing device, the silent speech from the input speech by performing the speech activity detection on the input speech, to obtain a tested speech comprises: dividing,
by the at least one computing device, the input speech into frames by a preset first step size, and removing, by the at least one computing device, the silent speech from each frame of the input speech by performing the speech activity detection on the
each frame of the input speech, to obtain the tested speech;  extracting, by the at least one computing device, acoustic features of the tested speech comprises: dividing, by the at least one computing device, the tested speech by a preset second step
size;  and extracting, by the at least one computing device, by a preset third step size, the acoustic features of the tested speech after dividing by the preset second step size;  and, identifying, by the at least one computing device, the acoustic
features of the tested speech by inputting the acoustic features of the tested speech into the target classifier, to determine whether the input speech is the child speech comprises: grading, by the at least one computing device, the acoustic features of
the tested speech by inputting the acoustic features of the tested speech into the target classifier;  acquiring, by the at least one computing device, an average value of the tested speech;  and determining, by the at least one computing device, that
the input speech is the child speech if the average value is greater than a preset threshold.
 6.  A device for searching according to a speech based on artificial intelligence, comprising: a processor;  and a memory, configured to store instructions executable by the processor, wherein the processor is configured to: acquire sample
speeches for training a preset classifier;  remove a silent speech from the sample speeches by performing a speech activity detection on the sample speeches, to obtain training speeches;  extract acoustic features of each training speech;  and train the
preset classifier by inputting the acoustic features of the each training speech into the preset classifier, to obtain a target classifier;  identify an input speech of a user to determine whether the input speech is a child speech;  filtrate a searched
result obtained according to the input speech to obtain a filtrated searched result, if the input speech is the child speech;  and feed the filtrated searched result to the user, wherein the processor is configured to remove a silent speech from the
sample speeches by performing a speech activity detection on the sample speeches, to obtain training speeches by acts of: dividing each sample speech into frames by a preset first step size, and removing the silent speech from each frame of the each
sample speech by performing the speech activity detection on the each frame of the each sample speech, to obtain the each training speech;  and the processor is configured to extract the acoustic features of each training speech by acts of: dividing the
each training speech by a preset second step size;  and extracting by a preset third step size, the acoustic features of the each training speech after dividing by the preset second step size.
 7.  The device according to claim 6, wherein the processor is configured to filtrate the searched result obtained according to the input speech by acts of: converting the input speech into a text content;  obtaining the searched result by
searching according to the text content;  and filtrating the searched result to remove a sensitive content unsuitable for a child.
 8.  The device according to claim 7, wherein the processor is configured to obtain the searched result by searching according to the text content by acts of: searching according to the text content in a first database pre-established for
children;  and searching according to the text content in a second database to obtain the searched result, if no content related to the input speech is searched in the first database.
 9.  The device according to claim 6, wherein the processor is configured to identify an input speech of a user to determine whether the input speech is a child speech by acts of: removing a silent speech from the input speech by performing the
speech activity detection on the input speech, to obtain a tested speech;  extracting acoustic features of the tested speech;  and identifying the acoustic features of the tested speech by inputting the acoustic features of the tested speech into the
target classifier, to determine whether the input speech is the child speech.
 10.  The device according to claim 9, wherein the processor is configured to remove the silent speech from the input speech by performing the speech activity detection on the input speech, to obtain a tested speech by acts of: dividing the input
speech into frames by a preset first step size, and removing the silent speech from each frame of the input speech by performing the speech activity detection on the each frame of the input speech, to obtain the tested speech;  the processor is
configured to extract acoustic features of the tested speech by acts of: dividing the tested speech by a preset second step size;  and extracting by a preset third step size, the acoustic features of the tested speech after dividing by the preset second
step size, and the processor is configured to identify the acoustic features of the tested speech by inputting the acoustic features of the tested speech into the target classifier, to determine whether the input speech is the child speech by acts of:
grading the acoustic features of the tested speech by inputting the acoustic features of the tested speech into the target classifier;  acquiring an average value of the tested speech;  and determining that the input speech is the child speech if the
average value is greater than a preset threshold.
 11.  A non-transitory computer readable storage medium comprising instructions, wherein the instructions are executed by a processor of a device to perform: acquiring sample speeches for training a preset classifier;  removing a silent speech
from the sample speeches by performing a speech activity detection on the sample speeches, to obtain training speeches;  extracting acoustic features of each training speech;  and training the preset classifier by inputting the acoustic features of the
each training speech into the preset classifier, to obtain a target classifier;  identifying an input speech of a user to determine whether the input speech is a child speech;  filtrating a searched result obtained according to the input speech to obtain
a filtrated searched result, if the input speech is the child speech;  and feeding the filtrated searched result to the user, wherein removing a silent speech from the sample speeches by performing a speech activity detection on the sample speeches, to
obtain training speeches comprises: dividing each sample speech into frames by a preset first step size, and removing the silent speech from each frame of the each sample speech by performing the speech activity detection on the each frame of the each
sample speech, to obtain the each training speech;  wherein extracting the acoustic features of each training speech comprises: dividing the each training speech by a preset second step size;  and extracting by a preset third step size, the acoustic
features of the each training speech after dividing by the preset second step size.
 12.  The non-transitory computer readable storage medium according to claim 11, wherein filtrating a searched result obtained according to the input speech comprises: converting the input speech into a text content;  obtaining the searched
result by searching according to the text content;  and filtrating the searched result to remove a sensitive content unsuitable for a child.
 13.  The non-transitory computer readable storage medium according to claim 12, wherein obtaining the searched result by searching according to the text content comprises: searching according to the text content in a first database
pre-established for children;  and searching according to the text content in a second database to obtain the searched result, if no content related to the input speech is searched in the first database.
 14.  The non-transitory computer readable storage medium according to claim 11, wherein identifying an input speech of a user to determine whether the input speech is a child speech comprises: removing a silent speech from the input speech by
performing the speech activity detection on the input speech, to obtain a tested speech;  extracting acoustic features of the tested speech;  and identifying the acoustic features of the tested speech by inputting the acoustic features of the tested
speech into the target classifier, to determine whether the input speech is the child speech.
 15.  The non-transitory computer readable storage medium according to claim 14, wherein removing the silent speech from the input speech by performing the speech activity detection on the input speech, to obtain a tested speech comprises:
dividing the input speech into frames by a preset first step size, and removing the silent speech from each frame of the input speech by performing the speech activity detection on the each frame of the input speech, to obtain the tested speech; 
extracting acoustic features of the tested speech comprises: dividing, by the at least one computing device, the tested speech by a preset second step size;  and extracting by a preset third step size, the acoustic features of the tested speech after
dividing by the preset second step size;  and wherein, identifying the acoustic features of the tested speech by inputting the acoustic features of the tested speech into the target classifier, to determine whether the input speech is the child speech
comprises: grading the acoustic features of the tested speech by inputting the acoustic features of the tested speech into the target classifier;  acquiring an average value of the tested speech;  and determining that the input speech is the child speech
if the average value is greater than a preset threshold.  "
"10,158,653","
     December 18, 2018
","Artificial intelligence with cyber security
"," A cyber security system that uses artificial intelligence, such neural
     networks, to monitor the security of a computer network and take
     automated remedial action based on the monitoring. The security system
     autonomically learns behavior profiles, attack profiles and circumvention
     techniques used to target the network. The remedial action taken by the
     system includes isolating any misuse that has been identified,
     surveilling the misuse in the isolated environment, analyzing its
     behavior profile and reconfiguring the network to enhance security.
",G06N 3/08 (20130101); H04L 63/1408 (20130101); H04L 63/1441 (20130101),H04L 29/06 (20060101); G06N 3/063 (20060101); G06N 3/08 (20060101),"[['\n6212895', '\nApril 2001'], ['\n6990395', '\nJanuary 2006'], ['\n7058710', '\nJune 2006'], ['\n7278273', '\nOctober 2007'], ['\n7525207', '\nApril 2009'], ['\n7738251', '\nJune 2010'], ['\n8359191', '\nJanuary 2013'], ['\n8853872', '\nOctober 2014'], ['\n2002/0010709', '\nJanuary 2002'], ['\n2006/0259201', '\nNovember 2006'], ['\n2007/0213000', '\nSeptember 2007'], ['\n2007/0281639', '\nDecember 2007'], ['\n2008/0209234', '\nAugust 2008'], ['\n2009/0037268', '\nFebruary 2009'], ['\n2009/0083126', '\nMarch 2009'], ['\n2009/0084297', '\nApril 2009'], ['\n2009/0126910', '\nMay 2009'], ['\n2009/0207567', '\nAugust 2009'], ['\n2009/0295167', '\nDecember 2009'], ['\n2009/0299824', '\nDecember 2009'], ['\n2010/0030552', '\nFebruary 2010'], ['\n2010/0298997', '\nNovember 2010'], ['\n2011/0060470', '\nMarch 2011'], ['\n2011/0072293', '\nMarch 2011'], ['\n2011/0207391', '\nAugust 2011'], ['\n2012/0042263', '\nFebruary 2012'], ['\n2012/0136998', '\nMay 2012'], ['\n2012/0166433', '\nJune 2012'], ['\n2012/0166616', '\nJune 2012'], ['\n2013/0238795', '\nSeptember 2013'], ['\n2014/0165060', '\nJune 2014'], ['\n2014/0259618', '\nSeptember 2014'], ['\n2016/0044054', '\nFebruary 2016']]","[3, '11,310,259', '11,019,078', '10,812,504']"," The invention claimed is:  1.  A computer implemented method for detecting security threats over a network, and for performing cyber-security defense by taking remedial action on detected
threats, comprising: collecting security information data over the network, from a plurality of appliances and application layers;  based on the collected security information data, assessing a risk component of the collected security information and
identifying based on pre-determined criteria, one or more security risks from the collected data by performing cognitive cyber-security analytics in an artificial neural network implemented method;  and based on the assessed risk component and the
identified one or more security risks, triggering a remedial action;  wherein the assessing is based on a pre-configured library, a periodic surveying, a periodic change managing, and a periodic reconfiguration;  autonomically learning a behavior profile
of the plurality of appliances or application layers;  and based on the learned behavior profile, autonomically learning of attack profiles and circumvention techniques used to target the network, the appliances and the application layers.
 2.  The computer implemented method of claim 1 further comprising in identifying based on pre-determined criteria, one or more security risks from the collected data, at least one of evaluating, simulating and recognizing a usage pattern.
 3.  The computer implemented method of claim 1 wherein, in identifying based on pre-determined criteria, one or more security risks from the collected data the cognitive cyber-security analytics in the artificial neural network implemented
method further comprises autonomic machine learning for recognition of threat patterns, vulnerabilities, anomalous behavior, malicious attack or misuse of network or application assets.
 4.  The computer implemented method of claim 1 further comprising: data collection via a data collection layer;  artificial intelligence machine learning based on the collected data and assessment, via an artificial intelligence machine learning
layer;  and wherein the assessment further comprises natural language processing, a periodic reconnaissance, and a periodic risk assessment.
 5.  The computer implemented method of claim 1 further comprising: analyzing and identifying a risk profile of an appliance or application based on the assessed risk level and the one or more identified security risks;  automatically isolating
any misuse that has been identified with the identified security risk profiles and automatically implementing surveillance of the misuse in the isolated environment;  and analyzing the security and behavior profile data collected from the surveillance of
the isolated misuse.
 6.  A computer automated system comprising a non-transitory machine readable medium having encoded instructions and coupled to a hardware processor, wherein the encoded instructions when executed by the hardware processor, cause the computer
automated system to: collect by the hardware processor, security information data over a network, from a plurality of appliances and application layers and store the collected security information data in the non-transitory machine readable medium; 
based on the collected security information data, assess a risk component of the collected security information and identify based on pre-determined criteria, one or more security risks from the collected data via a cognitive cyber-security analytics in
an artificial neural network implementation;  and based on the assessed risk component and the identified one or more security risks, trigger a remedial action;  wherein the assessing is based on a pre-configured library, a periodic surveying, a periodic
change managing, and a periodic reconfiguration;  autonomically learn a behavior profile of the plurality of appliances or application layers;  and based on the learned behavior profile, autonomically learn of attack profiles and circumvention techniques
used to target the network, the appliances and the application layers.
 7.  The system of claim 6 wherein the system is further caused to: in identifying based on pre-determined criteria, one or more security risks from the collected data, at least one of evaluate, simulate and recognize a usage pattern.
 8.  The system of claim 6 wherein the cognitive cyber-security analytics in the artificial neural network implementation further comprises autonomic machine learning for recognition of threat patterns, vulnerabilities, anomalous behavior,
malicious attack or misuse of network or application assets.
 9.  The system of claim 6 wherein the system is further caused to: collect the data via a data collection layer;  learn, based on the collected data and assessment, via an artificial intelligence machine learning layer;  and wherein the
assessment further comprises natural language processing in a natural language processing layer, a periodic reconnaissance, and a periodic risk assessment.
 10.  The computer automated system of claim 6, wherein the computer automated system is further caused to: analyze and identify a risk profile of an appliance or application based on the assessed risk level and the one or more identified
security risks;  automatically isolate any misuse that has been identified with the identified security risk profiles and automatically implement surveillance of the misuse in the isolated environment;  and analyze the security and behavior profile data
collected from the surveillance of the isolated misuse.  "
"10,166,675","
     January 1, 2019
","Trainable modular robotic apparatus
"," Apparatus and methods for a modular robotic device with artificial
     intelligence that is receptive to training controls. In one
     implementation, modular robotic device architecture may be used to
     provide all or most high cost components in an autonomy module that is
     separate from the robotic body. The autonomy module may comprise
     controller, power, actuators that may be connected to controllable
     elements of the robotic body. The controller may position limbs of the
     toy in a target position. A user may utilize haptic training approach in
     order to enable the robotic toy to perform target action(s). Modular
     configuration of the disclosure enables users to replace one toy body
     (e.g., the bear) with another (e.g., a giraffe) while using hardware
     provided by the autonomy module. Modular architecture may enable users to
     purchase a single AM for use with multiple robotic bodies, thereby
     reducing the overall cost of ownership.
",A63H 3/20 (20130101); G06N 20/00 (20190101); B25J 9/163 (20130101); G06N 3/008 (20130101); B25J 13/08 (20130101); B25J 9/1694 (20130101); Y10S 901/04 (20130101); G06N 3/049 (20130101); Y10S 901/09 (20130101); Y10S 901/02 (20130101); Y10S 901/50 (20130101),B25J 9/16 (20060101); G06N 99/00 (20100101); G06N 3/00 (20060101); B25J 13/08 (20060101); A63H 3/20 (20060101); G06N 3/04 (20060101),"[['\n4600355', '\nJuly 1986'], ['\n4687457', '\nAugust 1987'], ['\n4762455', '\nAugust 1988'], ['\n4820233', '\nApril 1989'], ['\n4853771', '\nAugust 1989'], ['\n4889027', '\nDecember 1989'], ['\n5042807', '\nAugust 1991'], ['\n5063603', '\nNovember 1991'], ['\n5083803', '\nJanuary 1992'], ['\n5355435', '\nOctober 1994'], ['\n5369497', '\nNovember 1994'], ['\n5378188', '\nJanuary 1995'], ['\n5638359', '\nJune 1997'], ['\n5652594', '\nJuly 1997'], ['\n5673367', '\nSeptember 1997'], ['\n5673387', '\nSeptember 1997'], ['\n5875108', '\nFebruary 1999'], ['\n6009418', '\nDecember 1999'], ['\n6014653', '\nJanuary 2000'], ['\n6061088', '\nMay 2000'], ['\n6084373', '\nJuly 2000'], ['\n6124541', '\nSeptember 2000'], ['\n6253058', '\nJune 2001'], ['\n6259988', '\nJuly 2001'], ['\n6338013', '\nJanuary 2002'], ['\n6411055', '\nJune 2002'], ['\n6429291', '\nAugust 2002'], ['\n6435936', '\nAugust 2002'], ['\n6458157', '\nOctober 2002'], ['\n6504610', '\nJanuary 2003'], ['\n6545705', '\nApril 2003'], ['\n6545708', '\nApril 2003'], ['\n6546291', '\nApril 2003'], ['\n6547631', '\nApril 2003'], ['\n6560511', '\nMay 2003'], ['\n6565407', '\nMay 2003'], ['\n6570608', '\nMay 2003'], ['\n6581046', '\nJune 2003'], ['\n6615108', '\nSeptember 2003'], ['\n6633232', '\nOctober 2003'], ['\n6682392', '\nJanuary 2004'], ['\n6697711', '\nFebruary 2004'], ['\n6760645', '\nJuly 2004'], ['\n6774908', '\nAugust 2004'], ['\n6780042', '\nAugust 2004'], ['\n7023833', '\nApril 2006'], ['\n7054850', '\nMay 2006'], ['\n7235013', '\nJune 2007'], ['\n7418320', '\nAugust 2008'], ['\n7565203', '\nJuly 2009'], ['\n7742625', '\nJune 2010'], ['\n7765029', '\nJuly 2010'], ['\n7849030', '\nDecember 2010'], ['\n8015130', '\nSeptember 2011'], ['\n8015785', '\nSeptember 2011'], ['\n8145355', '\nMarch 2012'], ['\n8145492', '\nMarch 2012'], ['\n8154436', '\nApril 2012'], ['\n8157612', '\nApril 2012'], ['\n8281997', '\nOctober 2012'], ['\n8295955', '\nOctober 2012'], ['\n8315305', '\nNovember 2012'], ['\n8346692', '\nJanuary 2013'], ['\n8401242', '\nMarch 2013'], ['\n8467623', '\nJune 2013'], ['\n8467823', '\nJune 2013'], ['\n8515160', '\nAugust 2013'], ['\n8527094', '\nSeptember 2013'], ['\n8542872', '\nSeptember 2013'], ['\n8571261', '\nOctober 2013'], ['\n8578810', '\nNovember 2013'], ['\n8583286', '\nNovember 2013'], ['\n8712939', '\nApril 2014'], ['\n8712941', '\nApril 2014'], ['\n8719199', '\nMay 2014'], ['\n8725658', '\nMay 2014'], ['\n8725662', '\nMay 2014'], ['\n8731295', '\nMay 2014'], ['\n8756183', '\nJune 2014'], ['\n8775341', '\nJuly 2014'], ['\n8793205', '\nJuly 2014'], ['\n8880222', '\nNovember 2014'], ['\n8943008', '\nJanuary 2015'], ['\n8954193', '\nFebruary 2015'], ['\n8972315', '\nMarch 2015'], ['\n8977582', '\nMarch 2015'], ['\n8983216', '\nMarch 2015'], ['\n8990133', '\nMarch 2015'], ['\n8996177', '\nMarch 2015'], ['\n9002511', '\nApril 2015'], ['\n9043952', '\nJune 2015'], ['\n9508235', '\nNovember 2016'], ['\n2001/0020944', '\nSeptember 2001'], ['\n2001/0045809', '\nNovember 2001'], ['\n2002/0038294', '\nMarch 2002'], ['\n2002/0072293', '\nJune 2002'], ['\n2002/0081937', '\nJune 2002'], ['\n2002/0156556', '\nOctober 2002'], ['\n2002/0158599', '\nOctober 2002'], ['\n2002/0183895', '\nDecember 2002'], ['\n2002/0198854', '\nDecember 2002'], ['\n2003/0050903', '\nMarch 2003'], ['\n2003/0222987', '\nDecember 2003'], ['\n2003/0232568', '\nDecember 2003'], ['\n2004/0016638', '\nJanuary 2004'], ['\n2004/0100563', '\nMay 2004'], ['\n2004/0153211', '\nAugust 2004'], ['\n2004/0158358', '\nAugust 2004'], ['\n2004/0162638', '\nAugust 2004'], ['\n2004/0193670', '\nSeptember 2004'], ['\n2004/0204792', '\nOctober 2004'], ['\n2004/0212148', '\nOctober 2004'], ['\n2004/0220082', '\nNovember 2004'], ['\n2004/0244138', '\nDecember 2004'], ['\n2005/0010331', '\nJanuary 2005'], ['\n2005/0012830', '\nJanuary 2005'], ['\n2005/0015351', '\nJanuary 2005'], ['\n2005/0022751', '\nFebruary 2005'], ['\n2005/0036649', '\nFebruary 2005'], ['\n2005/0049749', '\nMarch 2005'], ['\n2005/0065651', '\nMarch 2005'], ['\n2005/0209749', '\nSeptember 2005'], ['\n2005/0240412', '\nOctober 2005'], ['\n2005/0283450', '\nDecember 2005'], ['\n2006/0069448', '\nMarch 2006'], ['\n2006/0161218', '\nJuly 2006'], ['\n2007/0008405', '\nJanuary 2007'], ['\n2007/0037475', '\nFebruary 2007'], ['\n2007/0176643', '\nAugust 2007'], ['\n2007/0208678', '\nSeptember 2007'], ['\n2007/0239315', '\nOctober 2007'], ['\n2007/0244610', '\nOctober 2007'], ['\n2007/0258329', '\nNovember 2007'], ['\n2008/0039974', '\nFebruary 2008'], ['\n2008/0170130', '\nJuly 2008'], ['\n2008/0201282', '\nAugust 2008'], ['\n2008/0294074', '\nNovember 2008'], ['\n2009/0014402', '\nJanuary 2009'], ['\n2009/0043722', '\nFebruary 2009'], ['\n2009/0066790', '\nMarch 2009'], ['\n2009/0118890', '\nMay 2009'], ['\n2009/0141939', '\nJune 2009'], ['\n2009/0153499', '\nJune 2009'], ['\n2009/0161981', '\nJune 2009'], ['\n2009/0287624', '\nNovember 2009'], ['\n2009/0310862', '\nDecember 2009'], ['\n2010/0036780', '\nFebruary 2010'], ['\n2010/0086171', '\nApril 2010'], ['\n2010/0091286', '\nApril 2010'], ['\n2010/0152894', '\nJune 2010'], ['\n2010/0166320', '\nJuly 2010'], ['\n2010/0228418', '\nSeptember 2010'], ['\n2010/0250022', '\nSeptember 2010'], ['\n2010/0283853', '\nNovember 2010'], ['\n2010/0286824', '\nNovember 2010'], ['\n2010/0290710', '\nNovember 2010'], ['\n2010/0292835', '\nNovember 2010'], ['\n2010/0316257', '\nDecember 2010'], ['\n2011/0016071', '\nJanuary 2011'], ['\n2011/0078717', '\nMarch 2011'], ['\n2011/0119214', '\nMay 2011'], ['\n2011/0119215', '\nMay 2011'], ['\n2011/0134245', '\nJune 2011'], ['\n2011/0178658', '\nJuly 2011'], ['\n2011/0184556', '\nJuly 2011'], ['\n2011/0222832', '\nSeptember 2011'], ['\n2011/0228742', '\nSeptember 2011'], ['\n2011/0235698', '\nSeptember 2011'], ['\n2011/0245974', '\nOctober 2011'], ['\n2012/0011090', '\nJanuary 2012'], ['\n2012/0063736', '\nMarch 2012'], ['\n2012/0081552', '\nApril 2012'], ['\n2012/0083982', '\nApril 2012'], ['\n2012/0098933', '\nApril 2012'], ['\n2012/0109866', '\nMay 2012'], ['\n2012/0109886', '\nMay 2012'], ['\n2012/0117012', '\nMay 2012'], ['\n2012/0143495', '\nJune 2012'], ['\n2012/0173021', '\nJuly 2012'], ['\n2012/0185092', '\nJuly 2012'], ['\n2012/0196679', '\nAugust 2012'], ['\n2012/0209428', '\nAugust 2012'], ['\n2012/0209432', '\nAugust 2012'], ['\n2012/0211923', '\nAugust 2012'], ['\n2012/0215348', '\nAugust 2012'], ['\n2012/0303091', '\nNovember 2012'], ['\n2012/0308076', '\nDecember 2012'], ['\n2012/0308136', '\nDecember 2012'], ['\n2012/0330872', '\nDecember 2012'], ['\n2013/0046716', '\nFebruary 2013'], ['\n2013/0073491', '\nMarch 2013'], ['\n2013/0073496', '\nMarch 2013'], ['\n2013/0073500', '\nMarch 2013'], ['\n2013/0077597', '\nMarch 2013'], ['\n2013/0103626', '\nApril 2013'], ['\n2013/0116827', '\nMay 2013'], ['\n2013/0117212', '\nMay 2013'], ['\n2013/0151450', '\nJune 2013'], ['\n2013/0176423', '\nJuly 2013'], ['\n2013/0204814', '\nAugust 2013'], ['\n2013/0204820', '\nAugust 2013'], ['\n2013/0216144', '\nAugust 2013'], ['\n2013/0218821', '\nAugust 2013'], ['\n2013/0226342', '\nAugust 2013'], ['\n2013/0245937', '\nSeptember 2013'], ['\n2013/0251278', '\nSeptember 2013'], ['\n2013/0314502', '\nNovember 2013'], ['\n2013/0325768', '\nDecember 2013'], ['\n2013/0325773', '\nDecember 2013'], ['\n2013/0325774', '\nDecember 2013'], ['\n2013/0325775', '\nDecember 2013'], ['\n2014/0008496', '\nJanuary 2014'], ['\n2014/0016858', '\nJanuary 2014'], ['\n2014/0032021', '\nJanuary 2014'], ['\n2014/0078343', '\nMarch 2014'], ['\n2014/0085545', '\nMarch 2014'], ['\n2014/0089232', '\nMarch 2014'], ['\n2014/0175267', '\nJune 2014'], ['\n2014/0198838', '\nJuly 2014'], ['\n2014/0240492', '\nAugust 2014'], ['\n2014/0247325', '\nSeptember 2014'], ['\n2014/0276951', '\nSeptember 2014'], ['\n2014/0277718', '\nSeptember 2014'], ['\n2014/0313032', '\nOctober 2014'], ['\n2014/0320668', '\nOctober 2014'], ['\n2014/0350722', '\nNovember 2014'], ['\n2015/0042485', '\nFebruary 2015'], ['\n2015/0157182', '\nJune 2015'], ['\n2015/0168954', '\nJune 2015'], ['\n2015/0234385', '\nAugust 2015'], ['\n2015/0362919', '\nDecember 2015'], ['\n2016/0104044', '\nApril 2016'], ['\n2016/0179096', '\nJune 2016']]",[0]," What is claimed:  1.  A robotic apparatus operable to conduct one or more assigned physical tasks, the robotic apparatus comprising: at least one processor;  and a non-transitory
computer-readable storage medium having computer-readable instructions stored thereon, the computer-readable instructions being configured to cause, when executed by the at least one processor, a first controller of the robotic apparatus to: produce an
output signal based on a sensory input comprising an environmental characteristic and a learning process of the first controller, the first controller being configured to be coupled to a first robotic body of a plurality of robotic bodies via a first
type of interface, the first robotic body including a first set of one or more degrees of freedom;  initialize a trajectory of the first robotic body of the plurality of robotic bodies based on the output signal;  receive a teaching signal in response to
the initialized trajectory that teaches a target trajectory;  update the learning process of the first controller based on the received teaching signal;  and implement an adjusted trajectory for the first robotic body of the plurality of robotic bodies
based on the updated learning process of the first controller;  store the adjusted trajectory in one or more trained robotic configurations;  and transfer the stored one or more trained robotic configurations to a second controller on a second robotic
body of the plurality of robotic bodies via a second type of interface, the second robotic body including a second set of one or more degrees of freedom distinct from the first set of one or more degrees of freedom, the second type of interface being
different from the first type of interface, the one or more trained robotic configurations comprising the updated learning process of the first controller;  wherein the transfer of the stored one or more trained robotic configurations to the second
controller is configured to: enable an implementation of the adjusted trajectory on the second robotic body of the plurality of robotic bodies via the transfer, the implementation of the adjusted trajectory on the second robotic body of the plurality of
robotic bodies being based on the transferred one or more trained robotic configurations;  and enable the second controller of the second robotic body of the plurality of robotic bodies to update the updated learning process for use by one or more other
controllers other than the second controller.
 2.  The robotic apparatus of claim 1, wherein the first controller comprises a learning apparatus capable of being trained to conduct the one or more assigned physical tasks via at least feedback, the feedback comprising the teaching signal; 
wherein the robotic apparatus is configured to train the learning apparatus to conduct the one or more assigned physical tasks via the at least feedback.
 3.  The robotic apparatus of claim 1, wherein the first controller is in data communication with one or more motive sources, and is configured to produce one or more inputs, the one or more inputs comprising one or more mechanical force inputs.
 4.  The robotic apparatus of claim 1, wherein: the at least one processor is further configured to operate an adaptive learning process to conduct the one or more assigned physical tasks, the adaptive learning process being characterized by a
plurality of trials;  the robotic apparatus is configured to provide at least one actuation output to the first robotic body, the at least one actuation output comprising first and second portions configured to effectuate movement of a first controllable
element and a second controllable element of the first robotic body, respectively;  the adaptive learning process is configured to determine, during a trial of the plurality of trials, at least one actuation output, the at least one actuation output
having a first trajectory associated therewith;  the first controllable element is configured to effectuate movement of the first robotic body in a first degree of freedom (DOF);  the second controllable element is configured to effectuate movement of
the first robotic body in a second DOF independent from the first DOF;  and the first and the second portions of the at least one actuation output are configured based on one or more instructions from the at least one processor.
 5.  The robotic apparatus of claim 4, wherein: the movement in the first DOF of the first controllable element and the movement in the second DOF of the second controllable element cooperate to effectuate conduction of the one or more assigned
tasks by the first robotic body;  the adaptive learning process comprises a haptic learning process characterized by at least a teaching input provided by a trainer;  and the teaching input is configured based on an adjustment of the first trajectory via
a physical contact of the trainer with the first robotic body.
 6.  The robotic apparatus of claim 5, wherein: the adjustment of the first trajectory is configured based at least on an observation of a discrepancy between the first trajectory and the target trajectory during the trial;  and the adjustment of
the first trajectory is configured to cause a modification of the learning process so as to generate the updated learning process and determine a second control input configured to transition the first trajectory towards the target trajectory during
another trial subsequent to the trial.
 7.  The robotic apparatus of claim 6, wherein: the modification of the learning process is characterized by a determination of one or more values by the at least one processor;  the robotic apparatus is further configured to provide another
actuation output to the second robotic body, the other actuation output being configured to effectuate movement of the second robotic body in a first DOF;  and the other actuation output is configured based on the one or more values.
 8.  The robotic apparatus of claim 4, wherein: a detachable enclosure is configured to house a camera adapted to provide sensory input to the at least one processor, the sensory input being used for determination of the at least one actuation
output in accordance with a target task, and the one or more instructions are configured based on at least the sensory input.
 9.  The robotic apparatus of claim 4, wherein: the at least one processor is configured to receive an audio input, the audio input being used for determining the at least one actuation output in accordance with a target task, and the one or more
instructions are configured based on at least the audio input.
 10.  The robotic apparatus of claim 9, wherein: a detachable enclosure is configured to house a sound receiving module configured to effectuate provision of the audio input to the processor;  and the audio input is configured based at least on a
command of a trainer.
 11.  The robotic apparatus of claim 4, wherein: a detachable enclosure is configured to house one or more inertial sensors configured to provide information related to a movement characteristic of the first robotic body to the at least one
processor;  and the one or more instructions are configured based at least on the information.
 12.  The robotic apparatus of claim 4, wherein the at least one processor is configured to determine a displacement of a first joint and a second joint associated, respectively, with the movement of the first controllable element in the first
degree of freedom (DOF) and the movement of the second controllable element in the second DOF.
 13.  The robotic apparatus of claim 12, wherein the determination of the displacement is configured based at least on feedback information provided from first and second actuators to the at least one processor, and the feedback information
comprises one or more of actuator displacement, actuator torque, and/or actuator current draw.
 14.  The robotic apparatus of claim 4, wherein: the first robotic body comprises an identifier configured to convey information related to a configuration of the first robotic body;  and the adaptive learning process is configured to adapt a
parameter based on receipt of the information, the adaptation of the parameter being configured to enable the adaptive learning process to adapt the at least one actuation output consistent with the configuration of the first robotic body.
 15.  The robotic apparatus of claim 1, wherein the first type of interface comprises: a first interface portion comprising a shape;  and a second interface portion particularly adapted to interface only with other interface portions comprising
the shape;  wherein a coupling of the first and second interface portions is configured to animate the first robotic body via at least a mechanical force transferred over the coupled first and second interface portions.
 16.  The robotic apparatus of claim 15, wherein the coupled first and second interface portions comprise at least one mechanical interface configured to transfer a force, and at least one electrical interface configured to transfer electrical
signals or power across the coupled first and second interface portions;  and the shape comprises a substantially male feature.
 17.  The robotic apparatus of claim 1, wherein the transfer of the one or more trained robotic configurations by the first controller of the robotic apparatus via the second type of interface comprises (i) storage of one or more neural network
weights determined by the learning process on a server entity and (ii) transfer of the one or more neural network weights from the server entity to the second robotic body or a third robotic body.
 18.  The robotic apparatus of claim 17, wherein: the storage of the one or more neural network weights comprises: addition of the one or more trained configurations to a repository of trained configurations;  and addition of performance metrics
corresponding to the one or more trained configurations;  and the one or more trained configurations are configured to be selectable among the repository of trained configurations by a user of the second robotic body or a user of the third robotic body.
 19.  The robotic apparatus of claim 1, wherein the received teaching signal is determined based on a first discrepancy measure between the trajectory of the first robotic body and the target trajectory;  wherein the instructions are further
configured to cause, when executed by the at least one processor, the first controller of the robotic apparatus to initialize the adjusted trajectory of the robotic body based on the updated learning process of the first controller, the adjusted
trajectory comprising a second discrepancy measure that is smaller than the first discrepancy measure, the second discrepancy measure being between the adjusted trajectory of the first robotic body and the target trajectory.
 20.  The robotic apparatus of claim 1, wherein the instructions are further configured to cause, when executed by the at least one processor, the first controller of the robotic apparatus to receive a concurrent teaching indication that
accompanies the teaching signal.  "
"10,168,677","
     January 1, 2019
","Systems and methods for smart spaces
"," A smart space may be provided by a hub and an artificial intelligence
     server in communication with the hub. The hub may receive data from at
     least one smart object in the smart space. The artificial intelligence
     server may generate clusters of the data received from each of the at
     least one smart objects. The server may perform processing comprising
     using a cluster to detect an anomaly in the smart object, identify the
     smart object, classify the smart object, determine a user behavior,
     determine a user mood, determine an energy consumption pattern, or create
     an automated action, or a combination thereof.
",H04L 12/2816 (20130101); H04L 41/16 (20130101); H04L 12/2823 (20130101); G05B 15/02 (20130101); G06N 5/047 (20130101); H04L 12/2834 (20130101); G05B 2219/2642 (20130101); G05B 2219/25011 (20130101); G05B 2219/25168 (20130101); G06N 20/00 (20190101),G05B 15/02 (20060101); G06N 5/04 (20060101); H04L 12/28 (20060101); H04L 12/24 (20060101); G06N 99/00 (20100101),"[['\n8255090', '\nAugust 2012'], ['\n2003/0005107', '\nJanuary 2003'], ['\n2009/0195349', '\nAugust 2009'], ['\n2010/0145542', '\nJune 2010'], ['\n2010/0250590', '\nSeptember 2010'], ['\n2010/0289643', '\nNovember 2010'], ['\n2011/0046805', '\nFebruary 2011'], ['\n2011/0257795', '\nOctober 2011'], ['\n2011/0264605', '\nOctober 2011'], ['\n2012/0130924', '\nMay 2012'], ['\n2012/0296488', '\nNovember 2012'], ['\n2013/0128118', '\nMay 2013'], ['\n2014/0084165', '\nMarch 2014'], ['\n2014/0098247', '\nApril 2014'], ['\n2014/0201381', '\nJuly 2014'], ['\n2014/0266669', '\nSeptember 2014'], ['\n2015/0347910', '\nDecember 2015'], ['\n2018/0005125', '\nJanuary 2018']]","[6, '11,137,161', '10,970,128', '10,924,606', '10,921,762', '10,897,374', '10,326,607']"," What is claimed is:  1.  A system for providing a smart space, comprising: a hub configured to receive data from at least one smart object in the smart space;  and an artificial intelligence
server in communication with the hub configured to: generate clusters of the data received from each of the at least one smart objects, each cluster comprising data received from one of the at least one smart objects during one of a plurality of time
periods, wherein a plurality of clusters are generated for each of the at least one smart objects, each of the plurality of clusters for each of the at least one smart objects comprising data received during a different time period;  perform processing
comprising using the clusters to detect an anomaly in the at least one smart object, the processing comprising evaluating a plurality of the clusters to identify a consistency between the clusters and/or evaluating at least one of the clusters to
identify a difference between the at least one cluster and at least one other cluster, wherein detecting the anomaly comprises identifying at least one event in the data that is not within a threshold distance from any of the clusters;  and based on the
processing, generate a command and send the command to the at least one smart object, thereby causing a change in an operation of the at least one smart object.
 2.  The system of claim 1, wherein the at least one smart object comprises a door/window sensor, a smart plug, a sensor unit, a smart socket, a skipper, a presence tag, a smart wall unit, a thermostat, a plug, a dimmer, a television, a home
theater component, an appliance, a lock, a machine, or a device, or a combination thereof.
 3.  The system of claim 1, wherein the processing further comprises using the clusters to identify the at least one smart object, classify the at least one smart object, determine a function of the at least one smart object, determine a behavior
of the at least one smart object, determine a user behavior, determine a user mood, determine an energy consumption pattern, or create an automated action, or a combination thereof.
 4.  The system of claim 3, wherein creating the automated action comprises generating an alert.
 5.  The system of claim 3, wherein the command comprises the automated action.
 6.  The system of claim 1, wherein generating clusters comprises: obtaining the data over a period of time;  identifying temporal relationships between events in the data;  and forming the clusters at times indicative of the temporal
relationships.
 7.  The system of claim 1, wherein classifying the smart object comprises analyzing the clusters to identify a pattern indicative of an object type.
 8.  The system of claim 1, wherein classifying the smart object comprises: analyzing the clusters to identify pattern indicative of a plurality of object types, thereby identifying a plurality of smart objects;  and associating the plurality of
smart objects with one another into a composite object.
 9.  The system of claim 1, wherein determining the user behavior comprises analyzing the clusters to identify a pattern indicative of a repeated user action.
 10.  The system of claim 1, wherein determining the energy consumption pattern comprises identifying events in the data corresponding to energy use and compiling energy use data for a period of time.
 11.  The system of claim 10, wherein determining the energy consumption pattern further comprises: obtaining weather information;  and correlating the weather information with the energy use.
 12.  The system of claim 1, wherein the artificial intelligence server is further configured to perform processing comprising associating the at least one smart object into a smart space network.
 13.  The system of claim 1, wherein the processing to identify the smart object comprises performing a discovery process via one or more communications protocols.
 14.  The system of claim 1, wherein an outcome of the processing is reported to a mobile device, a personal computer, a television, or a combination thereof.
 15.  The system of claim 1, wherein the hub is further configured to control output displayed on a television.
 16.  The system of claim 1, wherein the artificial intelligence server is further configured to install software on the hub.
 17.  The system of claim 1, wherein: the hub is in communication with a display and a controller, and the hub is further configured to provide a user interface for control of the smart space via the display and receive a user command via the
controller.
 18.  The system of claim 17, wherein the display is a television.
 19.  The system of claim 1, wherein generating the command is further based on at least one event planned by at least one intelligent agent.
 20.  The system of claim 19, wherein: the at least one intelligent agent comprises a drag and drop user interface configured to receive user input, and generating the command is further based on the user input.
 21.  The system of claim 19, wherein: the at least one intelligent agent comprises an artificial intelligence and/or machine learning component, and generating the command is further based on code automatically generated by the artificial
intelligence and/or machine learning component.
 22.  A method for providing a smart space, comprising: receiving, at a hub, data from at least one smart object in the smart space;  generating, with an artificial intelligence server in communication with the hub, clusters of the data received
from each of the at least one smart objects, each cluster comprising data received from one of the at least one smart objects during one of a plurality of time periods, wherein a plurality of clusters are generated for each of the at least one smart
objects, each of the plurality of clusters for each of the at least one smart objects comprising data received during a different time period;  performing processing, with the server, comprising using a cluster to detect an anomaly in the at least one
smart object by performing processing including identifying at least one event in the data that is not within a threshold distance from any of the clusters;  and based on the processing, generating a command and sending the command to the at least one
smart object, thereby causing a change in an operation of the at least one smart object.
 23.  The method of claim 22, wherein the at least one smart object comprises a door/window sensor, a smart plug, a sensor unit, a smart socket, a skipper, a presence tag, a smart wall unit, a thermostat, a plug, a dimmer, a television, a home
theater component, an appliance, a lock, a machine, or a device, or a combination thereof.
 24.  The method of claim 22, wherein the processing further comprises using the clusters to identify the at least one smart object, classify the at least one smart object, determine a function of the at least one smart object, determine a
behavior of the at least one smart object, determine a user behavior, determine a user mood, determine an energy consumption pattern, or create an automated action, or a combination thereof.
 25.  The method of claim 24, wherein the command comprises the automated action.
 26.  The method of claim 24, wherein creating the automated action comprises generating an alert.
 27.  The method of claim 22, wherein generating clusters comprises: obtaining the data over a period of time;  identifying temporal relationships between events in the data;  and forming the clusters at times indicative of the temporal
relationships.
 28.  The method of claim 22, wherein classifying the smart object comprises analyzing the clusters to identify a pattern indicative of an object type.
 29.  The method of claim 22, wherein classifying the smart object comprises: analyzing the clusters to identify pattern indicative of a plurality of object types, thereby identifying a plurality of smart objects;  and associating the plurality
of smart objects with one another into a composite object.
 30.  The method of claim 22, wherein determining the user behavior comprises analyzing the clusters to identify a pattern indicative of a repeated user action.
 31.  The method of claim 22, wherein determining the energy consumption pattern comprises identifying events in the data corresponding to energy use and compiling energy use data for a period of time.
 32.  The method of claim 31, wherein determining the energy consumption pattern further comprises: obtaining weather information;  and correlating the weather information with the energy use.
 33.  The method of claim 22, further comprising performing processing, with the artificial intelligence server, comprising associating the at least one smart object into a smart space network.
 34.  The method of claim 22, wherein the processing to identify the smart object comprises performing a discovery process via one or more communications protocols.
 35.  The method of claim 22, wherein an outcome of the processing is reported to a mobile device, a personal computer, a television, or a combination thereof.
 36.  The method of claim 22, further comprising controlling, with the hub, output displayed on a television.
 37.  The method of claim 22, further comprising installing, with the artificial intelligence server, software on the hub.
 38.  The method of claim 22, wherein the hub is in communication with a display and a controller, the method further comprising: providing, with the hub, a user interface for control of the smart space via the display;  and receiving, with the
hub, a user command via the controller.
 39.  The method of claim 38, wherein the display is a television.
 40.  The method of claim 22, wherein generating the command is further based on at least one event planned by at least one intelligent agent.
 41.  The method of claim 40, wherein: the at least one intelligent agent comprises a drag and drop user interface configured to receive user input, and generating the command is further based on the user input.
 42.  The method of claim 40, wherein: the at least one intelligent agent comprises an artificial intelligence and/or machine learning component, and generating the command is further based on code automatically generated by the artificial
intelligence and/or machine learning component.  "
"10,171,310","
     January 1, 2019
","Ensuring regulatory compliance during application migration to cloud-based
     containers
"," A method, system, and/or computer program product ensures regulatory
     compliance during application migration to cloud-based containers. In
     response to receiving a message directing an application to be migrated
     to a container cloud, the application is matched to multiple containers
     described in a container registry, where each container is matched to a
     service that provides a server. A regulatory rule by which at least one
     of the server, the application, and requisite containers are governed is
     identified. In response to detecting a change to at least one of the
     server, the application, and the requisite containers so that there is
     noncompliance with a regulatory rule, a chain of compliance services is
     automatically recomposed using an artificial intelligence planning
     technology.
",H04L 41/16 (20130101); H04L 41/0816 (20130101); H04L 67/10 (20130101); H04L 41/142 (20130101); H04L 41/22 (20130101),H04L 12/24 (20060101); H04L 29/08 (20060101),"[['\n8631458', '\nJanuary 2014'], ['\n2003/0004754', '\nJanuary 2003'], ['\n2011/0145393', '\nJune 2011'], ['\n2011/0270968', '\nNovember 2011'], ['\n2011/0276951', '\nNovember 2011'], ['\n2012/0011077', '\nJanuary 2012'], ['\n2012/0060171', '\nMarch 2012'], ['\n2012/0179746', '\nJuly 2012'], ['\n2012/0203908', '\nAugust 2012'], ['\n2012/0303776', '\nNovember 2012'], ['\n2012/0304179', '\nNovember 2012'], ['\n2013/0204849', '\nAugust 2013'], ['\n2014/0137244', '\nMay 2014'], ['\n2014/0149494', '\nMay 2014'], ['\n2014/0237550', '\nAugust 2014']]","[3, '11,243,926', '11,061,718', '10,949,406']"," What is claimed is:  1.  A method of ensuring regulatory compliance during application migration to cloud-based containers, the method comprising: receiving, by one or more processors, a message
directing an application to be migrated to a container cloud, wherein the container cloud is a cloud of servers and networks that support containers, wherein a container is a class of objects used to store objects following specific access rules;  in
response to receiving the message directing the application to be migrated to the container cloud, matching, by one or more processors, the application to multiple containers described in a container registry, wherein each container is matched to a
service that provides a server, and wherein the server hosts cloud-based containers whose logical behavior is defined by a set of values and a set of operations;  grouping, by one or more processors, lines of code from the application according to
different regulations, wherein a first group of lines of code from the application is regulated by a first regulatory rule, and wherein a second group of lines of code from the application is regulated by a second regulatory rule;  identifying, by one or
more processors, at least one regulatory rule as an identified at least one regulatory rule by which the server, the application, and requisite containers are governed;  detecting, by one or more processors, a change to at least one of the server, the
application, and the requisite containers so that there is noncompliance with the identified at least one regulatory rule;  and in response to detecting the change, automatically recomposing a chain of compliance services using an artificial intelligence
planning technology, wherein the chain of compliance services monitors compliance with the identified at least one regulatory rule by the server and the requisite containers.
 2.  The method of claim 1, wherein the container cloud is a hybrid cloud that contains an application container service and a database container service, wherein the application container service contains the application, and wherein the
database container service contains data required for executing the application by the requisite containers.
 3.  The method of claim 1, further comprising: analyzing, by one or more processors, terms found in an application log for the application, wherein the application log describes actions performed by the application;  matching, by one or more
processors, identified terms from the application log to key terms used to describe the identified at least one regulatory rule;  and further identifying, by one or more processors, the identified at least one regulatory rule according to said matching
identified terms from the application log to key terms used to describe the identified at least one regulatory rule.
 4.  The method of claim 1, further comprising: analyzing, by one or more processors, terms found in container logs for the multiple containers, wherein the container logs describe actions performed by the multiple containers;  matching, by one
or more processors, identified terms from the container logs to key terms used to describe the identified at least one regulatory rule;  and further identifying, by one or more processors, the identified at least one regulatory rule according to said
matching identified terms from the container logs to the key terms used to describe the identified at least one regulatory rule.
 5.  The method of claim 1, further comprising: further identifying, by a compliance monitor engine, the identified at least one regulatory rule by matching types, relationships, and properties of multiple container stacks to the identified at
least one regulatory rule.
 6.  The method of claim 1, further comprising: transmitting, by one or more processors, an advisory message to a developer of the application, wherein the advisory message includes a description of requirements of the identified at least one
regulatory rule.
 7.  The method of claim 1, further comprising: detecting, by one or more processors, non-compliance with the identified at least one regulatory rule by the server, the application, and the requisite containers;  and in response to detecting
non-compliance with the identified at least one regulatory rule by the server, the application, and the requisite containers, transforming, by an advisory composition engine, the chain of compliance services by decomposing and rebundling resources
provided by the chain of compliance services.
 8.  The method of claim 1, further comprising: visually modifying, by one or more processors, a display of the first group of lines of code to indicate an applicability of the first regulatory rule.
 9.  A computer program product for ensuring regulatory compliance during application migration to cloud-based containers, the computer program product comprising a non-transitory computer readable storage medium having program code embodied
therewith, the program code readable and executable by a processor to perform a method comprising: receiving a message directing an application to be migrated to a container cloud, wherein the container cloud is a cloud of servers and networks that
support containers, wherein a container is a class of objects used to store objects following specific access rules;  in response to receiving the message directing the application to be migrated to the container cloud, matching the application to
multiple containers described in a container registry, wherein each container is matched to a service that provides a server, and wherein the server hosts cloud-based containers whose logical behavior is defined by a set of values and a set of
operations;  grouping lines of code from the application according to different regulations, wherein a first group of lines of code from the application is regulated by a first regulatory rule, and wherein a second group of lines of code from the
application is regulated by a second regulatory rule;  identifying at least one regulatory rule as an identified at least one regulatory rule by which at least one of the server, the application, and requisite containers are governed;  detecting a change
to at least one of the server, the application, and the requisite containers so that there is noncompliance with the identified at least one regulatory rule;  and in response to detecting the change, automatically recomposing a chain of compliance
services using an artificial intelligence planning technology, wherein the chain of compliance services monitors compliance by the server and the requisite containers with the identified at least one regulatory rule.
 10.  The computer program product of claim 9, wherein the container cloud is a hybrid cloud that contains an application container service and a database container service, wherein the application container service contains the application, and
wherein the database container service contains data required for executing the application.
 11.  The computer program product of claim 9, wherein the method further comprises: analyzing terms found in container logs for the multiple containers, wherein the container logs describe actions performed by the multiple containers;  matching
analyzed terms from the container logs to key terms used to describe the identified at least one regulatory rule;  and further identifying the identified at least one regulatory rule according to said matching analyzed terms from the container logs to
the key terms used to describe the identified at least one regulatory rule.
 12.  The computer program product of claim 9, wherein the method further comprises: transmitting an advisory message to a developer of the application, wherein the advisory message includes a description of requirements of the identified at
least one regulatory rule.
 13.  The computer program product of claim 9, wherein the method further comprises: detecting non-compliance with the identified at least one regulatory rule by the server, the application, and the requisite containers;  and in response to
detecting non-compliance with the identified at least one regulatory rule by the server, the application, and the requisite containers, transforming, by an advisory composition engine, the chain of compliance services by decomposing and rebundling
resources provided by the chain of compliance services.
 14.  A computer system comprising: a processor, a computer readable memory, and a non-transitory computer readable storage medium;  first program instructions to receive a message directing an application to be migrated to a container cloud,
wherein the container cloud is a cloud of servers and networks that support containers, wherein a container is a class of objects used to store objects following specific access rules;  second program instructions to, in response to receiving the message
directing the application to be migrated to the container cloud, match the application to multiple containers described in a container registry, wherein said matching identifies requisite containers for executing the application across multiple container
stacks in a container cloud;  third program instructions to group lines of code from the application according to different regulations, wherein a first group of lines of code from the application is regulated by a first regulatory rule, and wherein a
second group of lines of code from the application is regulated by a second regulatory rule;  fourth program instructions to match the requisite containers to a service, wherein the service provides a server that hosts cloud-based containers;  fifth
program instructions to identify at least one regulatory rule as an identified at least one regulatory rule by which the server, the application, and the requisite containers are governed;  sixth program instructions to implement a chain of compliance
services to monitor compliance with the identified at least one regulatory rule by the server, the application, and the requisite containers;  seventh program instructions to detect a change to the server, the application, and the requisite containers; 
eighth program instructions to, in response to detecting non-compliance with the identified at least one regulatory rule by the server, the application, and the requisite containers after the change, reconfigure the server and the requisite containers; 
and ninth program instructions to, in response to reconfiguring the server and the requisite containers, execute artificial intelligence planning technology to automatically re-compose the chain of compliance services, wherein the chain of compliance
services monitors compliance by the server and the requisite containers with the identified at least one regulatory rule;  and wherein the first, second, third, fourth, fifth, sixth, seventh, eighth, and ninth program instructions are stored on the
non-transitory computer readable storage medium for execution by one or more processors via the computer readable memory.
 15.  The computer system of claim 14, wherein the container cloud is a hybrid cloud that contains an application container service and a database container service, wherein the application container service contains the application, and wherein
the database container service contains data required for executing the application.
 16.  The computer system of claim 14, further comprising: tenth program instructions to analyze terms found in an application log for the application, wherein the application log describes actions performed by the application;  eleventh program
instructions to match analyzed terms from the application log to key terms used to describe the identified at least one regulatory rule;  and twelfth program instructions to further identify the identified at least one regulatory rule according to said
matching analyzed terms from the application log to the key terms used to describe the identified at least one regulatory rule;  and wherein the tenth, eleventh, and twelfth program instructions are stored on the non-transitory computer readable storage
medium for execution by one or more processors via the computer readable memory.
 17.  The computer system of claim 14, further comprising: tenth program instructions to analyze terms found in container logs for the multiple containers, wherein the container logs describe actions performed by the multiple containers; 
eleventh instructions to match analyzed terms from the container logs to key terms used to describe the identified at least one regulatory rule;  and twelfth program instructions to further identify the identified at least one regulatory rule according
to said matching analyzed terms from the container logs to the key terms used to describe the identified at least one regulatory rule;  and wherein the tenth, eleventh, and twelfth instructions are stored on the non-transitory computer readable storage
medium for execution by one or more processors via the computer readable memory.
 18.  The computer system of claim 14, further comprising: tenth program instructions to further identify the identified at least one regulatory rule by matching types, relationships, and properties of the multiple container stacks to the
identified at least one regulatory rule;  and eleventh program instructions to transmit an advisory message to a developer of the application, wherein the advisory message includes a description of requirements of the identified at least one regulatory
rule;  and wherein the tenth and eleventh program instructions are stored on the non-transitory computer readable storage medium for execution by one or more processors via the computer readable memory.
 19.  The computer system of claim 14, further comprising: tenth program instructions to detect non-compliance with the identified at least one regulatory rule by the server, the application, and the requisite containers;  eleventh program
instructions to, in response to detecting non-compliance with the identified at least one regulatory rule by the server, the application, and the requisite containers, transform the chain of compliance services by decomposing and rebundling resources
provided by the chain of compliance services;  and wherein the tenth and eleventh program instructions are stored on the non-transitory computer readable storage medium for execution by one or more processors via the computer readable memory. 
"
"10,176,280","
     January 8, 2019
","Modeling of crop growth for desired moisture content of bovine feedstuff
     and determination of harvest windows for corn silage using field-level
     diagnosis and forecasting of weather conditions and field observations
"," A modeling framework for evaluating the impact of weather conditions on
     farming and harvest operations applies real-time, field-level weather
     data and forecasts of meteorological and climatological conditions
     together with user-provided and/or observed feedback of a present state
     of a harvest-related condition to agronomic models and to generate a
     plurality of harvest advisory outputs for precision agriculture. A
     harvest advisory model simulates and predicts the impacts of this weather
     information and user-provided and/or observed feedback in one or more
     physical, empirical, or artificial intelligence models of precision
     agriculture to analyze crops, plants, soils, and resulting agricultural
     commodities, and provides harvest advisory outputs to a diagnostic
     support tool for users to enhance farming and harvest decision-making,
     whether by providing pre-, post-, or in situ-harvest operations and crop
     analyses.
",G01W 1/10 (20130101); A01B 79/005 (20130101); G06F 30/20 (20200101); Y02A 40/10 (20180101); G06N 99/00 (20130101); Y02A 90/10 (20180101); G06N 20/00 (20190101); A01C 21/00 (20130101),G01W 1/10 (20060101); A01B 79/00 (20060101); G06N 99/00 (20100101); G06F 17/50 (20060101); A01C 21/00 (20060101),"[['\n5884225', '\nMarch 1999'], ['\n7167797', '\nJanuary 2007'], ['\n2012/0109614', '\nMay 2012'], ['\n2013/0173321', '\nJuly 2013'], ['\n2014/0012732', '\nJanuary 2014']]",[0]," The invention claimed is:  1.  A method comprising: ingesting, as input data, weather information and crop-specific information for a corn crop to be harvested, the weather information including
at least one of current field-level weather data and forecasted weather data, and the crop-specific information at least including planting data and bovine feedstuff data that includes a desired moisture content of between 55% and 70% whole plant
moisture for a bovine feedstuff comprised of corn silage to be stored in one or more post-harvest storage structures, the one or more post-harvest storage structures including an upright tower silo, a bunker silo, a bag silo, and an oxygen-limiting silo; modeling the input data in a plurality of data processing modules within a computing environment in which the plurality of data processing modules are executed in conjunction with, and performed on, at least one computer processor, the data processing
modules configured to profile crop growth for a harvest of the corn silage at the desired moisture content, by developing an agricultural model of one or more physical and empirical characteristics impacting harvest operations of the corn crop, to enable
a harvest operation to obtain the bovine feedstuff comprised of corn silage at the desired moisture content, including: 1) predicting expected weather conditions at a specific location impacting a crop moisture content of the corn crop during crop
growth, 2) aggregating the expected weather conditions with the crop-specific information into the agricultural model to simulate the moisture content of the corn crop at one or more crop growth stages, 3) identifying differences between the simulated
moisture content and additional data representing in-field observations of the timing of the crop growth as a growing season progresses and crop growth stage observations of the crop moisture content in a planted field taken at the specific location at
times corresponding to the one or more crop growth stages to determine a variance between the crop moisture content and the simulated moisture content expected from in-field dry-down of the crop, and 4) adapting the simulation of the moisture content to
the crop growth stage observations to predict a harvest condition to produce bovine feedstuff having the desired moisture content from the corn silage;  generating, as output data, a harvest condition output profile representing the predicted harvest
condition;  and initiating a harvest operation from the harvest condition output profile representing the predicted harvest condition to harvest the corn crop at the desired moisture content for the bovine feedstuff, wherein a user performs the harvest
operation in the particular field based on the predicted harvest condition for the corn crop, or an automated harvest operation is controlled based on the predicted harvest condition for the corn crop.
 2.  The method of claim 1, further comprising applying the harvest output condition profile to a decision support tool configured to provide one or more advisories of the predicted harvest condition, wherein the decision support tool generates
an advisory of one or more harvest opportunity windows to achieve the desired moisture content for the corn silage to a user.
 3.  The method of claim 2, wherein the decision support tool generates a risk advisory of a risk from harvesting corn silage at a moisture content that exceeds the desired moisture content, the risk advisory including one or more of damage to a
storage structure, loss of feedstuff nutrient levels due to runoff and seepage in the storage structure, and water supply contamination.
 4.  The method of claim 2, wherein the decision support tool generates a risk advisory of a risk from harvesting corn silage at a lesser moisture content than the desired moisture content, the risk advisory including one or more of a packing
factor, a digestibility, and a sugar content of the bovine feedstuff.
 5.  The method of claim 1, wherein the modeling further comprises applying the weather information to one or more predictive numerical weather models to generate the prediction of expected weather conditions.
 6.  The method of claim 1, further comprising ingesting the additional data representing crop growth stage observations at the specific location at corresponding times, the additional data including physical, empirical or observed agricultural
information, wherein the physical, empirical or observed agricultural information and field and soil information includes at least one of crop moisture content, soil temperature, soil moisture content, and current and/or forecasted soil conditions at a
particular crop growth stage.
 7.  The method of claim 1, wherein the agricultural model of one or more physical and empirical characteristics impacting harvest operations of the agricultural commodity includes at least one of a crop-specific growth model, a soil model, and a
land surface model.
 8.  The method of claim 1, further comprising applying the expected weather conditions and the additional data representing crop growth stage observations of the crop moisture content in a planted field to develop an artificial intelligence
model configured to analyze a specific harvest condition model by building a comprehensive harvest condition dataset for the agricultural model of one or more physical and empirical characteristics impacting harvest operations of the corn crop, and
applying the artificial intelligence model to predict the one or more harvest opportunity windows to produce bovine feedstuff having the desired moisture content from the corn silage.
 9.  The method of claim 1, wherein the desired moisture content is between 65% and 70% whole plant moisture for storage of the corn silage in a bunker silo.
 10.  The method of claim 1, wherein the desired moisture content is between 63% and 70% whole plant moisture for storage of the corn silage in an upright tower silo.
 11.  The method of claim 1, wherein the desired moisture content is between 63% and 67% whole plant moisture for storage of the corn silage in a bag silo.
 12.  The method of claim 1, wherein the desired moisture content is between 55% and 60% whole plant moisture for storage of the corn silage in an oxygen-limiting silo.
 13.  A system comprising: a computing environment including at least one computer-readable storage medium having program instructions stored therein and a computer processor operable to execute the program instructions to profile crop growth for
a harvest of corn silage at desired moisture content in a harvest advisory model within a plurality of data processing modules, the plurality of data processing modules including: a weather modeling module configured to diagnose and predict expected
weather conditions at a specific location impacting a moisture content of a corn crop during crop growth by applying weather information including current field-level weather data and forecasted weather data to one or more predictive numerical weather
models;  one or more modules configured to develop an agricultural model of one or more physical and empirical characteristics impacting harvest operations of the corn crop, to enable a harvest operation to obtain a bovine feedstuff comprised of corn
silage at the desired moisture content, by 1) aggregating the expected weather conditions with crop-specific information comprised at least of planting data and bovine feedstuff data that includes a desired moisture content of between 55% and 70% whole
plant moisture for the bovine feedstuff comprised of the corn silage to be stored in one or more post-harvest storage structures, into the agricultural model, 2) simulating the moisture content of the corn crop at one or more crop growth stages, and 3)
comparing additional data representing in-field observations of the timing of the crop growth as a growing season progresses and moisture content from one or more crop growth stage observations of crop moisture content in a planted field taken at the
specific location at times corresponding to the one or more crop growth stage to determine a variance between the crop moisture content and the simulated moisture content expected from in-field dry-down of the crop to adapt the simulation to the one or
more crop growth stage observations;  a harvest condition prediction module configured to predict one or more harvest opportunity windows to produce bovine feedstuff having the desired moisture content from the corn silage in a harvest output condition
profile for planning a timing of harvest operations and storage of the corn silage;  and an output module configured to initiate a harvest operation from the harvest condition output profile representing the predicted harvest condition to harvest the
corn crop at the desired moisture content for the bovine feedstuff, wherein a user performs the harvest operation in the particular field based on the predicted harvest condition for the corn crop, or an automated harvest operation is controlled based on
the predicted harvest condition for the corn crop.
 14.  The system of claim 13, wherein the harvest output condition profile is applied to a diagnostic support tool configured to provide one or more advisories to a user.
 15.  The system of claim 14, wherein the decision support tool generates a risk advisory of a risk from harvesting corn silage at a moisture content that exceeds the desired moisture content, the risk advisory including one or more of damage to
a storage structure, loss of feedstuff nutrient levels due to runoff and seepage in the storage structure, and water supply contamination.
 16.  The system of claim 14, wherein the decision support tool generates a risk advisory of a risk from harvesting corn silage at a lesser moisture content than the desired moisture content, the risk advisory including one or more of a packing
factor, a digestability, and a sugar content of the bovine feedstuff.
 17.  The system of claim 13, wherein the one or more modules are further configured to apply the expected weather conditions and the additional data representing one or more crop growth stage observations of the crop moisture content at the
specific location at corresponding times to develop an artificial intelligence model configured to analyze a specific harvest condition model by building a comprehensive harvest condition dataset for the agricultural model of one or more physical and
empirical characteristics impacting harvest operations of the corn crop, and applying the artificial intelligence model to predict the one or more harvest opportunity windows to produce bovine feedstuff having the desired moisture content from the corn
silage.
 18.  The system of claim 13, wherein the one or more post-harvest storage structures include an upright tower silo, a bunker silo, a bag silo, and an oxygen-limiting silo.
 19.  The system of claim 13, wherein the desired moisture content is between 65% and 70% whole plant moisture for storage of the corn silage in a bunker silo.
 20.  The system of claim 13, wherein the desired moisture content is between 63% and 70% whole plant moisture for storage of the corn silage in an upright tower silo.
 21.  The system of claim 13, wherein the desired moisture content is between 63% and 67% whole plant moisture for storage of the corn silage in a bag silo.
 22.  The system of claim 13, wherein the desired moisture content is between 55% and 60% whole plant moisture for storage of the corn silage in an oxygen-limiting silo.
 23.  A method of evaluating agricultural conditions to support harvest operations and storage of a bovine feedstuff, comprising: within a computing environment comprised of a computer processor and at least one computer-readable storage medium
operably coupled to the computer processor and having program instructions stored therein, the computer processor being operable to execute the program instructions to profile crop growth for a harvest of corn silage at a desired moisture content in a
harvest advisory model configured to perform the steps of: developing an agricultural model of one or more physical and empirical characteristics impacting harvest operations of the corn crop, to enable a harvest operation to obtain bovine feedstuff
comprised of corn silage at the desired moisture content, by forecasting weather conditions impacting moisture content during crop growth of a corn crop to be harvested in a specific location, by applying weather information comprised of current
field-level weather data and forecasted weather data to one or more predictive numerical weather models;  simulating the moisture content of the corn crop at one or more crop growth stages by applying the forecasted weather conditions and crop-specific
information comprised at least of planting data and bovine feedstuff data that includes a desired moisture content of between 55% and 70% whole plant moisture for a bovine feedstuff comprised of corn silage to be stored in one or more post-harvest
storage structures, to the agricultural model, the agricultural model comprising at least one of a crop-specific growth model, a soil model, and a land surface model;  and associating the simulated moisture content with plant moisture from one or more
crop growth stage observations using at least one of 1) differences between the simulated moisture content and additional data representing in-field observations of the timing of the crop growth as a growing season progresses and the plant moisture from
the one or more crop growth stage observations of crop moisture content in a planted field taken at the specific location at times corresponding to the one or more crop growth stages, determined from a variance between the crop moisture content and the
simulated moisture content expected from in-field dry-down of the crop, and 2) one or more artificial intelligence models developed to analyze the moisture content of the corn crop at one or more crop growth stages from a comprehensive harvest condition
dataset for the agricultural model of the one or more physical and empirical characteristics impacting harvest operations of the corn crop, to generate a prediction of one or more harvest opportunity windows to achieve the desired moisture content of the
corn silage for storage in one or more of an upright tower silo, a bunker silo, a bag silo, and an oxygen-limiting silo;  and initiating a harvest operation from the harvest condition output profile representing the predicted harvest condition to harvest
the corn crop at the desired moisture content for the bovine feedstuff, wherein a user performs the harvest operation in the particular field based on the predicted harvest condition for the corn crop, or an automated harvest operation is controlled
based on the predicted harvest condition for the corn crop.
 24.  The method of claim 21, further comprising generating a harvest output condition profile representing the prediction, and applying the harvest output condition profile to a diagnostic support tool configured to provide one or more
advisories based on the prediction to a user performing harvest operations.
 25.  The method of claim 22, wherein the decision support tool generates a risk advisory of a risk from harvesting corn silage at a moisture content that is higher than the desired moisture content, the risk advisory including one or more of
damage to a storage structure, loss of feedstuff nutrient levels due to runoff and seepage in the storage structure, and water supply contamination.
 26.  The method of claim 22, wherein the decision support tool generates a risk advisory of a risk from harvesting corn silage at a moisture content that is lower than the desired moisture content, the risk advisory including one or more of a
packing factor, a digestability, and a sugar content of the bovine feedstuff.
 27.  The method of claim 23, wherein the desired moisture content is between 65% and 70% whole plant moisture for storage of the corn silage in a bunker silo.
 28.  The method of claim 23, wherein the desired moisture content is between 63% and 70% whole plant moisture for storage of the corn silage in an upright tower silo.
 29.  The method of claim 23, wherein the desired moisture content is between 63% and 67% whole plant moisture for storage of the corn silage in a bag silo.
 30.  The method of claim 23, wherein the desired moisture content is between 55% and 60% whole plant moisture for storage of the corn silage in an oxygen-limiting silo.  "
"10,176,405","
     January 8, 2019
","Vehicle re-identification techniques using neural networks for image
     analysis, viewpoint-aware pattern recognition, and generation of multi-
     view vehicle representations
"," This disclosure relates to improved vehicle re-identification techniques.
     The techniques described herein utilize artificial intelligence (AI) and
     machine learning functions to re-identify vehicles across multiple
     cameras. Vehicle re-identification can be performed using an image of the
     vehicle that is captured from any single viewpoint. Attention maps may be
     generated that identify regions of the vehicle that include visual
     patterns that overlap between the viewpoint of the captured image and one
     or more additional viewpoints. The attention maps are used to generate a
     multi-view representation of the vehicle that provides a global view of
     the vehicle across multiple viewpoints. The multi-view representation of
     the vehicle can then be compared to previously captured image data to
     perform vehicle re-identification.
",G06V 20/41 (20220101); H04N 7/181 (20130101); G06V 20/54 (20220101); G06K 9/6218 (20130101); G06V 20/52 (20220101); G06K 9/6256 (20130101); G06K 9/6271 (20130101); G06T 7/246 (20170101); G06V 10/82 (20220101); G06V 20/63 (20220101); G06T 7/73 (20170101); G06K 9/6269 (20130101); G06K 9/6201 (20130101); G06K 9/6215 (20130101); G06T 2207/20081 (20130101); G06T 2207/10016 (20130101); G06V 2201/08 (20220101),G06K 9/00 (20060101); G06T 7/246 (20170101); H04N 7/18 (20060101); G06T 7/73 (20170101); G06K 9/62 (20060101),"[['\n8983133', '\nMarch 2015'], ['\n9239955', '\nJanuary 2016'], ['\n9396403', '\nJuly 2016'], ['\n9911056', '\nMarch 2018'], ['\n9953245', '\nApril 2018'], ['\n2017/0097944', '\nApril 2017'], ['\n2017/0140247', '\nMay 2017'], ['\n2018/0005079', '\nJanuary 2018']]","[12, '11,348,223', '11,288,887', '11,270,446', '11,252,398', '11,232,541', '11,093,789', '11,062,486', '10,997,537', '10,937,540', '10,592,779', '10,540,578', '10,325,201']"," What is claimed is:  1.  A system for re-identifying a vehicle comprising: a camera system comprising a plurality of cameras;  one or more computing devices comprising one or more processors and
one or more non-transitory storage devices for storing instructions, wherein execution of the instructions by the one or more processors causes the one or more computing device to: receive an image of a vehicle from a camera included in the camera
system;  identify, with a trained neural network, a viewpoint of the image;  generate attention maps from the image that identify regions of the vehicle which include overlapping visual patterns between the identified viewpoint and one or more additional
viewpoints;  generate a multi-view representation of the vehicle utilizing the attention maps;  and perform vehicle re-identification by comparing the multi-view representation to vehicles identified in previously captured images.
 2.  The system of claim 1, wherein the multi-view representation includes inferred vehicle information associated with the one or more additional viewpoints that provides a global view of the vehicle.
 3.  The system of claim 2, wherein a conditional generative adversarial network is utilized to infer the vehicle information that is used to generate the multi-view representation of the vehicle.
 4.  The system of claim 3, wherein the conditional generative adversarial network comprises a generative neural network and a discriminative neural network that compete against one another to generate the multi-view representation of the
vehicle.
 5.  The system of claim 4, wherein the conditional generative adversarial network is trained using a second generative neural network that utilizes real image data of vehicles.
 6.  The system of claim 1, wherein comparing the multi-view representation to vehicles identified in previously captured images includes utilizing a pairwise distance metric learning function to compute distance metrics indicating a similarity
between the vehicle in the image and the vehicles identified in the previously captured images.
 7.  The system of claim 1, wherein the attention maps are generated, at least in part, using an attention model that enables identification of context vectors indicating the overlapping visual patterns of the vehicle between the identified
viewpoint and the one or more additional viewpoints.
 8.  The system of claim 1, wherein execution of the instructions by the one or more processors causes the computing device to: use a trained neural network to extract vehicle features from the image, including features that identify a color of
the vehicle, a model of the vehicle, and a type of the vehicle.
 9.  The system of claim 8, wherein: the trained neural network comprises a convolutional neural network that is trained using vehicle attribute labels;  and the convolutional neural network is utilized to train an attention model that is used to
generate the attention maps.
 10.  The system of claim 1, wherein the system for re-identifying the vehicle is utilized in connection with a surveillance system or a transportation system.
 11.  A method for re-identifying a vehicle comprising: receiving an image of a vehicle from a camera included in a camera system;  identifying, with a trained neural network, a viewpoint of the image;  generating attention maps from the image
that identify regions of the vehicle which include overlapping visual patterns between the identified viewpoint and one or more additional viewpoints;  generating a multi-view representation of the vehicle utilizing the attention maps;  and performing
vehicle re-identification by comparing the multi-view representation to vehicles identified in previously captured images.
 12.  The method of claim 11, wherein the multi-view representation includes inferred vehicle information associated with the one or more additional viewpoints that provides a global view of the vehicle.
 13.  The method of claim 12, wherein a conditional generative adversarial network is utilized to infer the vehicle information that is used to generate the multi-view representation of the vehicle.
 14.  The method of claim 13, wherein the conditional generative adversarial network comprises a generative neural network and a discriminative neural network that compete against one another to generate the multi-view representation of the
vehicle.
 15.  The method of claim 14, wherein the conditional generative adversarial network is trained using a second generative neural network that utilizes real image data of vehicles.
 16.  The method of claim 11, wherein comparing the multi-view representation to vehicles identified in previously captured images includes utilizing a pairwise distance metric learning function to compute distance metrics indicating a similarity
between the vehicle in the image and the vehicles identified in the previously captured images.
 17.  The method of claim 11, wherein the attention maps are generated, at least in part, using an attention model that enables identification of context vectors indicating the overlapping visual patterns of the vehicle between the identified
viewpoint and the one or more additional viewpoints.
 18.  The method of claim 11, wherein a trained neural network is used to extract vehicle features from the image, including features that identify a color of the vehicle, a model of the vehicle, and a type of the vehicle.
 19.  The method of claim 18, wherein: the trained neural network comprises a convolutional neural network that is trained using vehicle attribute labels;  and the convolutional neural network is utilized to train an attention model that is used
to generate the attention maps.
 20.  The method of claim 11, wherein the method for re-identifying the vehicle is utilized in connection with a surveillance system or a transportation system.  "
"10,180,998","
     January 15, 2019
","Modeling of crop growth for desired moisture content of bovine feedstuff
     and determination of harvest windows for corn earlage using field-level
     diagnosis and forecasting of weather conditions and field observations
"," A modeling framework for evaluating the impact of weather conditions on
     farming and harvest operations applies real-time, field-level weather
     data and forecasts of meteorological and climatological conditions
     together with user-provided and/or observed feedback of a present state
     of a harvest-related condition to agronomic models and to generate a
     plurality of harvest advisory outputs for precision agriculture. A
     harvest advisory model simulates and predicts the impacts of this weather
     information and user-provided and/or observed feedback in one or more
     physical, empirical, or artificial intelligence models of precision
     agriculture to analyze crops, plants, soils, and resulting agricultural
     commodities, and provides harvest advisory outputs to a diagnostic
     support tool for users to enhance farming and harvest decision-making,
     whether by providing pre-, post-, or in situ-harvest operations and crop
     analyses.
",G06F 30/20 (20200101); Y02A 40/10 (20180101); G06N 20/00 (20190101),G06F 17/50 (20060101); G06N 99/00 (20100101),"[['\n5884225', '\nMarch 1999'], ['\n7167797', '\nJanuary 2007'], ['\n2012/0109614', '\nMay 2012'], ['\n2013/0173321', '\nJuly 2013'], ['\n2014/0012732', '\nJanuary 2014']]",[0]," The invention claimed is:  1.  A method comprising: ingesting, as input data, weather information and crop-specific information for a corn crop to be harvested, the weather information including
at least one of current field-level weather data and forecasted weather data, and the crop-specific information at least including planting data and bovine feedstuff data that includes a desired moisture content of between 26% and 40% whole plant
moisture for a bovine feedstuff comprised of corn earlage to be stored in one or more post-harvest storage structures, the one or more post-harvest storage structures including an upright tower silo, a bunker silo, a bag silo, and an oxygen-limiting
silo;  modeling the input data in a plurality of data processing modules within a computing environment in which the plurality of data processing modules are executed in conjunction with, and performed on, at least one computer processor, the data
processing modules configured to profile crop growth for a harvest of the corn earlage at the desired moisture content, by developing an agricultural model of one or more physical and empirical characteristics impacting harvest operations of the corn
crop, to enable a harvest operation to obtain the bovine feedstuff comprised of corn earlage at the desired moisture content, including: 1) predicting expected weather conditions at a specific location impacting a crop moisture content of the corn crop
during crop growth, 2) aggregating the expected weather conditions with the crop-specific information into the agricultural model to simulate the moisture content of the corn crop at one or more crop growth stages, 3) identifying differences between the
simulated moisture content and additional data representing in-field observations of the timing of the crop growth as a growing season progresses and crop growth stage observations of the crop moisture content in a planted field taken at the specific
location at times corresponding to the one or more crop growth stages to determine a variance between the crop moisture content and the simulated moisture content expected from in-field dry-down of the crop, and 4) adapting the simulation of the moisture
content to the crop growth stage observations to predict a harvest condition to produce bovine feedstuff having the desired moisture content from the corn earlage;  generating, as output data, a harvest condition output profile representing the predicted
harvest condition;  and initiating a harvest operation from the harvest condition output profile representing the predicted harvest condition to harvest the corn crop at the desired moisture content for the bovine feedstuff, wherein a user performs the
harvest operation in the particular field based on the predicted harvest condition for the corn crop, or an automated harvest operation is controlled based on the predicted harvest condition for the corn crop.
 2.  The method of claim 1, further comprising applying the harvest output condition profile to a decision support tool configured to provide one or more advisories of the predicted harvest condition, wherein the decision support tool generates
an advisory of one or more harvest opportunity windows to achieve the desired moisture content for the corn earlage to a user.
 3.  The method of claim 2, wherein the decision support tool generates a risk advisory of a risk from harvesting corn earlage at a moisture content that exceeds the desired moisture content, the risk advisory including one or more of damage to a
storage structure, loss of feedstuff nutrient levels due to runoff and seepage in the storage structure, and water supply contamination.
 4.  The method of claim 2, wherein the decision support tool generates a risk advisory of a risk from harvesting corn earlage at a lesser moisture content than the desired moisture content, the risk advisory including one or more of a packing
factor, a digestability, and a sugar content of the bovine feedstuff.
 5.  The method of claim 1, wherein the modeling further comprises applying the weather information to one or more predictive numerical weather models to generate the prediction of expected weather conditions.
 6.  The method of claim 1, further comprising ingesting the additional data representing crop growth stage observations in a planted field at the specific location at corresponding times, the additional data including physical, empirical or
observed agricultural information, wherein the physical, empirical or observed agricultural information and field and soil information includes at least one of crop moisture content, soil temperature, soil moisture content, and current and/or forecasted
soil conditions at a particular crop growth stage.
 7.  The method of claim 1, wherein the agricultural model of one or more physical and empirical characteristics impacting harvest operations of the agricultural commodity includes at least one of a crop-specific growth model, a soil model, and a
land surface model.
 8.  The method of claim 1, further comprising applying the expected weather conditions and the additional data representing crop growth stage observations of the crop moisture content in a planted field to develop an artificial intelligence
model configured to analyze a specific harvest condition model by building a comprehensive harvest condition dataset for the agricultural model of one or more physical and empirical characteristics impacting harvest operations of the corn crop, and
applying the artificial intelligence model to predict the one or more harvest opportunity windows to produce bovine feedstuff having the desired moisture content from the corn earlage.
 9.  The method of claim 1, wherein the desired moisture content is between 26% and 40% whole plant moisture for storage of the corn earlage in a bunker silo, an upright tower silo, or a bag silo.
 10.  The method of claim 1, wherein the desired moisture content is between 26% and 36% whole plant moisture for storage of the corn earlage in an oxygen-limiting silo.
 11.  A system comprising: a computing environment including at least one computer-readable storage medium having program instructions stored therein and a computer processor operable to execute the program instructions to profile crop growth for
a harvest of corn earlage at a desired moisture content in a harvest advisory model within a plurality of data processing modules, the plurality of data processing modules including: a weather modeling module configured to diagnose and predict expected
weather conditions at a specific location impacting a moisture content of a corn crop during crop growth by applying weather information including current field-level weather data and forecasted weather data to one or more predictive numerical weather
models;  one or more modules configured to develop an agricultural model of one or more physical and empirical characteristics impacting harvest operations of the corn crop, to enable a harvest operation to obtain a bovine feedstuff comprised of corn
earlage at the desired moisture content, by 1) aggregating the expected weather conditions with crop-specific information comprised at least of planting data and bovine feedstuff data that includes a desired moisture content of between 26% and 40% whole
plant moisture for the bovine feedstuff comprised of the corn earlage to be stored in one or more post-harvest storage structures, into the agricultural model, 2) simulating the moisture content of the corn crop at one or more crop growth stages, and 3)
comparing additional data representing moisture content from one or more crop growth observations crop moisture content in a planted field taken at the specific location at times corresponding to the one or more crop growth stage to determine a variance
between the crop moisture content and the simulated moisture content expected from in-field dry-down of the crop to adapt the simulation to the one or more crop growth stage observations;  a harvest condition prediction module configured to predict one
or more harvest opportunity windows to produce bovine feedstuff having the desired moisture content from the corn earlage in a harvest output condition profile for planning a timing of harvest operations and storage of the corn earlage;  and an output
module configured to initiate a harvest operation from the harvest condition output profile representing the predicted harvest condition to harvest the corn crop at the desired moisture content for the bovine feedstuff, wherein a user performs the
harvest operation in the particular field based on the predicted harvest condition for the corn crop, or an automated harvest operation is controlled based on the predicted harvest condition for the corn crop.
 12.  The system of claim 11, wherein the harvest output condition profile is applied to a diagnostic support tool configured to provide one or more advisories to a user.
 13.  The system of claim 12, wherein the decision support tool generates a risk advisory of a risk from harvesting corn earlage at a moisture content that exceeds the desired moisture content, the risk advisory including one or more of damage to
a storage structure, loss of feedstuff nutrient levels due to runoff and seepage in the storage structure, and water supply contamination.
 14.  The system of claim 12, wherein the decision support tool generates a risk advisory of a risk from harvesting corn earlage at a lesser moisture content than the desired moisture content, the risk advisory including one or more of a packing
factor, a digestability, and a sugar content of the bovine feedstuff.
 15.  The system of claim 11, wherein the one or more modules are further configured to apply the expected weather conditions and the additional data representing one or more crop growth stage observations of the crop moisture content at the
specific location at corresponding times to develop an artificial intelligence model configured to analyze a specific harvest condition model by building a comprehensive harvest condition dataset for the agricultural model of one or more physical and
empirical characteristics impacting harvest operations of the corn crop, and applying the artificial intelligence model to predict the one or more harvest opportunity windows to produce bovine feedstuff having the desired moisture content from the corn
earlage.
 16.  The system of claim 11, wherein the one or more post-harvest storage structures include an upright tower silo, a bunker silo, a bag silo, and an oxygen-limiting silo.
 17.  The system of claim 11, wherein the desired moisture content is between 26% and 40% whole plant moisture for storage of the corn earlage in a bunker silo, an upright tower silo, or a bag silo.
 18.  The system of claim 11, wherein the desired moisture content is between 26% and 36% whole plant moisture for storage of the corn earlage in an oxygen-limiting silo.
 19.  A method of evaluating agricultural conditions to support harvest operations and storage of a bovine feedstuff, comprising: within a computing environment comprised of a computer processor and at least one computer-readable storage medium
operably coupled to the computer processor and having program instructions stored therein, the computer processor being operable to execute the program instructions to profile crop growth for a harvest of corn earlage at a desired moisture content in a
harvest advisory model configured to perform the steps of: developing an agricultural model of one or more physical and empirical characteristics impacting harvest operations of the corn crop, to enable a harvest operation to obtain bovine feedstuff
comprised of corn earlage at the desired moisture content, by forecasting weather conditions impacting moisture content during crop growth of a corn crop to be harvested in a specific location, by applying weather information comprised of current
field-level weather data and forecasted weather data to one or more predictive numerical weather models;  simulating the moisture content of the corn crop at one or more crop growth stages by applying the forecasted weather conditions and crop-specific
information comprised at least of planting data and bovine feedstuff data that includes a desired moisture content of between 26% and 40% of whole plant moisture for a bovine feedstuff comprised of corn earlage to be stored in one or more post-harvest
storage structures, to the agricultural model, the agricultural model comprising at least one of a crop-specific growth model, a soil model, and a land surface model;  and associating the simulated moisture content with plant moisture from one or more
crop growth stage observations using at least one of 1) differences between the simulated moisture content and additional data representing in-field observations of the timing of the crop growth as a growing season progresses and the plant moisture from
the one or more crop growth stage observations of crop moisture content in a planted field taken at the specific location at times corresponding to the one or more crop growth stages, determined from a variance between the crop moisture content and the
simulated moisture content expected from in-field dry-down of the crop, and 2) one or more artificial intelligence models developed to analyze the moisture content of the corn crop at one or more crop growth stages from a comprehensive harvest condition
dataset for the agricultural model of the one or more physical and empirical characteristics impacting harvest operations of the corn crop, to generate a prediction of one or more harvest opportunity windows to achieve the desired moisture content of the
corn earlage for storage in one or more of an upright tower silo, a bunker silo, a bag silo, and an oxygen-limiting silo;  and initiating a harvest operation from the harvest condition output profile representing the predicted harvest condition to
harvest the corn crop at the desired moisture content for the bovine feedstuff, wherein a user performs the harvest operation in the particular field based on the predicted harvest condition for the corn crop, or an automated harvest operation is
controlled based on the predicted harvest condition for the corn crop.
 20.  The method of claim 19, further comprising generating a harvest output condition profile representing the prediction, and applying the harvest output condition profile to a diagnostic support tool configured to provide one or more
advisories based on the prediction to a user performing harvest operations.
 21.  The method of claim 20, wherein the decision support tool generates a risk advisory of a risk from harvesting corn earlage at a moisture content that is higher than the desired moisture content, the risk advisory including one or more of
damage to a storage structure, loss of feedstuff nutrient levels due to runoff and seepage in the storage structure, and water supply contamination.
 22.  The method of claim 20, wherein the decision support tool generates a risk advisory of a risk from harvesting corn earlage at a moisture content that is lower than the desired moisture content, the risk advisory including one or more of a
packing factor, a digestability, and a sugar content of the bovine feedstuff.
 23.  The method of claim 19, wherein the desired moisture content is between 26% and 40% whole plant moisture for storage of the corn earlage in a bunker silo, an upright tower silo, or a bag silo.
 24.  The method of claim 23, wherein the desired moisture content is between 26% and 36% whole plant moisture for storage of the corn earlage in an oxygen-limiting silo.  "
"10,185,790","
     January 22, 2019
","Modeling of crop growth for desired moisture content of targeted livestock
     feedstuff for determination of harvest windows using field-level
     diagnosis and forecasting of weather conditions and observations and user
     input of harvest condition states
"," A modeling framework for evaluating the impact of weather conditions on
     farming and harvest operations applies real-time, field-level weather
     data and forecasts of meteorological and climatological conditions
     together with user-provided and/or observed feedback of a present state
     of a harvest-related condition to agronomic models and to generate a
     plurality of harvest advisory outputs for precision agriculture. A
     harvest advisory model simulates and predicts the impacts of this weather
     information and user-provided and/or observed feedback in one or more
     physical, empirical, or artificial intelligence models of precision
     agriculture to analyze crops, plants, soils, and resulting agricultural
     commodities, and provides harvest advisory outputs to a diagnostic
     support tool for users to enhance farming and harvest decision-making,
     whether by providing pre-, post-, or in situ-harvest operations and crop
     analyzes.
",G01W 1/10 (20130101); G06F 30/20 (20200101); A01B 79/005 (20130101); G06N 20/00 (20190101); Y02A 40/10 (20180101); Y02A 90/10 (20180101),G01W 1/10 (20060101); A01B 79/00 (20060101); G06F 17/50 (20060101); G06N 99/00 (20100101),"[['\n5884225', '\nMarch 1999'], ['\n7167797', '\nJanuary 2007'], ['\n2012/0109614', '\nMay 2012'], ['\n2013/0173321', '\nJuly 2013'], ['\n2014/0012732', '\nJanuary 2014']]",[0]," The invention claimed is:  1.  A method comprising: ingesting, as input data, weather information and crop-specific information for an agricultural commodity to be harvested, the weather
information including at least one of current field-level weather data and forecasted weather data, and the crop-specific information including crop type data, planting data, and targeted livestock feedstuff data that includes storage characteristics for
a targeted livestock feedstuff;  modeling the input data in a plurality of data processing modules within a computing environment in which the plurality of data processing modules are executed in conjunction with, and performed on, at least one computer
processor, the data processing modules configured to profile crop growth for the agricultural commodity, by developing an agricultural model of one or more physical and empirical characteristics impacting harvest operations of the agricultural commodity,
to enable a harvest operation to obtain the targeted livestock feedstuff under desired moisture characteristics, including: 1) predicting expected weather conditions at a specific location impacting crop moisture content of the agricultural commodity
during crop growth, 2) aggregating the expected weather conditions with the crop-specific information into the agricultural model to simulate the moisture content of the agricultural commodity at one or more crop growth stages, 3) identifying differences
between the simulated moisture content and additional data representing in-field observations of the timing of the crop growth as a growing season progresses and sampled observations of the crop moisture content taken at the specific location at times
corresponding to the one or more crop growth stages to determine a variance between the crop moisture content and the simulated moisture content expected from in-field dry-down of the crop, and 4) modifying the simulation of the moisture content based on
the differences between the simulated moisture content and the additional data to predict a harvest condition for producing the targeted livestock feedstuff from the agricultural commodity;  generating, as output data, a harvest condition output profile
representing the predicted harvest condition;  and initiating a harvest operation from the harvest condition output profile representing the predicted harvest condition to harvest the agricultural commodity for the targeted livestock feedstuff, wherein a
user performs the harvest operation in the particular field based on the predicted harvest condition for the agricultural commodity, or an automated harvest operation is controlled based on the predicted harvest condition for the agricultural commodity.
 2.  The method of claim 1, further comprising applying the harvest output condition profile to a decision support tool configured to provide one or more advisories of the predicted harvest condition to a user, wherein the predicted harvest
condition represents a number of days that the agricultural commodity will be at an appropriate crop growth stage for producing the targeted livestock feedstuff at a desired moisture content for storage of the targeted livestock feedstuff according to
the storage characteristics.
 3.  The method of claim 2, wherein the decision support tool generates an advisory of one or more temporal harvest opportunity windows for the targeted livestock feedstuff at the desired moisture content.
 4.  The method of claim 2, wherein the decision support tool generates a risk advisory of a risk from harvesting targeted livestock feedstuff at a moisture content that exceeds the desired moisture content, the risk advisory including one or
more of damage to a storage structure, loss of feedstuff nutrient levels due to runoff and seepage in the storage structure, and water supply contamination.
 5.  The method of claim 2, wherein the decision support tool generates a risk advisory of a risk from harvesting targeted livestock feedstuff at a lesser moisture content than the desired moisture content, the risk advisory including one or more
of a packing factor, a digestability, and a sugar content of the targeted livestock feedstuff.
 6.  The method of claim 1, wherein the defined storage characteristics include information for maintaining the targeted livestock feedstuff in different post-harvest storage structures, the different post-harvest storage structures including an
upright tower silo, a bunker silo, a bag silo, and an oxygen-limiting silo.
 7.  The method of claim 1, wherein the modeling further comprises applying the weather information to one or more predictive numerical weather models to generate the prediction of expected weather conditions.
 8.  The method of claim 1, further comprising ingesting the additional data representing sampled observations at the specific location at corresponding times, the additional data including physical, empirical or observed agricultural
information, wherein the physical, empirical or observed agricultural information and field and soil information includes at least one of sampled crop moisture content, sampled soil temperature, sampled soil moisture content, and current and/or
forecasted soil conditions.
 9.  The method of claim 1, wherein the agricultural model of one or more physical and empirical characteristics impacting harvest operations of the agricultural commodity includes at least one of a crop-specific growth model, a soil model, and a
land surface model.
 10.  The method of claim 1, further comprising applying the expected weather conditions and the additional data representing sampled observations at the specific location at corresponding times to develop an artificial intelligence model
configured to analyze a specific harvest condition model by building a comprehensive harvest condition dataset for the agricultural model of one or more physical and empirical characteristics impacting harvest operations of the agricultural commodity,
and applying the artificial intelligence model to predict the harvest condition to produce the targeted livestock feedstuff.
 11.  The method of claim 1, wherein the targeted livestock feedstuff is a bovine feedstuff includes one or more of corn silage, corn earlage, corn snaplage, and corn having a specific desired moisture content.
 12.  A system comprising: a computing environment including at least one computer-readable storage medium having program instructions stored therein and a computer processor operable to execute the program instructions to profile crop growth for
an agricultural commodity in a harvest advisory model within a plurality of data processing modules, the plurality of data processing modules including: a weather modeling module configured to diagnose and predict expected weather conditions at a
specific location impacting crop moisture content of the agricultural commodity during crop growth by applying weather information including current field-level weather data and forecasted weather data to one or more predictive numerical weather models; 
one or more modules configured to develop an agricultural model of one or more physical and empirical characteristics impacting harvest operations of the agricultural commodity, to enable a harvest operation to obtain a targeted livestock feedstuff under
desired moisture characteristics, by 1) aggregating the expected weather conditions with crop-specific information comprised of crop type data, planting data, and targeted livestock feedstuff data that includes storage characteristics for the targeted
livestock feedstuff into the agricultural model 2) simulating the moisture content of the agricultural commodity at one or more crop growth stages, and 3) comparing additional data representing in-field observations of the timing of the crop growth as a
growing season progresses and sampled one or more observations of the crop moisture content taken at the specific location at times corresponding to the one or more crop growth stage to determine a variance between the crop moisture content and the
simulated moisture content expected from in-field dry-down of the crop to modify the simulation based on identified differences between the otherwise simulated moisture content and the additional data representing in-field observations of the timing of
the one or more crop growth stages and one or more sampled observations of the crop moisture content;  a harvest condition prediction module configured to predict a harvest condition for producing the targeted livestock feedstuff from the agricultural
commodity in a harvest output condition profile for planning a timing of harvest operations and storage of the harvested livestock feedstuff;  and an output module configured to initiate a harvest operation from the harvest condition output profile to
harvest the agricultural commodity for the targeted livestock feedstuff, wherein a user performs the harvest operation in the particular field based on the predicted harvest condition for the agricultural commodity, or an automated harvest operation is
controlled based on the predicted harvest condition for the agricultural commodity.
 13.  The system of claim 12, wherein the predicted harvest condition includes a number of days that the agricultural commodity will be at an appropriate crop growth stage for producing the targeted livestock feedstuff at a desired moisture
content for storage of the targeted livestock feedstuff according to the storage characteristics.
 14.  The system of claim 13, wherein the harvest output condition profile is applied to a diagnostic support tool configured to provide one or more advisories to a user.
 15.  The system of claim 14, wherein the decision support tool generates an advisory of one or more temporal harvest opportunity windows for the targeted livestock feedstuff at the desired moisture content.
 16.  The system of claim 14, wherein the decision support tool generates a risk advisory of a risk from harvesting livestock bovine feedstuff at a moisture content that exceeds the desired moisture content, the risk advisory including one or
more of damage to a storage structure, loss of feedstuff nutrient levels due to runoff and seepage in the storage structure, and water supply contamination.
 17.  The system of claim 14, wherein the decision support tool generates a risk advisory of a risk from harvesting targeted livestock feedstuff at a lesser moisture content than the desired moisture content, the risk advisory including one or
more of a packing factor, a digestability, and a sugar content of the targeted livestock feedstuff.
 18.  The system of claim 12, wherein the defined storage characteristics include information for maintaining the targeted livestock feedstuff in different post-harvest storage structures, the different post-harvest storage structures including
an upright tower silo, a bunker silo, a bag silo, and an oxygen-limiting silo.
 19.  The system of claim 8, wherein the additional data representing sampled observations at the specific location at corresponding times includes physical, empirical or observed agricultural information and field and soil information, wherein
the physical, empirical or observed agricultural information and field and soil information includes at least one of sampled crop moisture content, sampled soil temperature, sampled soil moisture content, and current and/or forecasted soil conditions.
 20.  The method of claim 12, wherein the targeted livestock feedstuff is a bovine feedstuff that includes one or more of corn silage, corn earlage, corn snaplage, and corn having a specific desired moisture content.
 21.  A method of evaluating agricultural conditions to support harvest operations and storage of livestock feedstuff, comprising: within a computing environment comprised of a computer processor and at least one computer-readable storage medium
operably coupled to the computer processor and having program instructions stored therein, the computer processor being operable to execute the program instructions to profile crop growth of an agricultural commodity to be harvested in a harvest advisory
model configured to perform the steps of: developing an agricultural model of one or more physical and empirical characteristics impacting harvest operations of the agricultural commodity, to enable a harvest operation to obtain a targeted livestock
feedstuff under desired moisture characteristics, by forecasting weather conditions impacting crop moisture content of the agricultural commodity during crop growth in a specific location, by applying weather information comprised of current field-level
weather data and forecasted weather data to one or more predictive numerical weather models;  simulating the moisture content of the agricultural commodity at one or more crop growth stages by applying the forecasted weather conditions and crop-specific
information comprised of crop type data, planting data, and targeted livestock feedstuff data that includes storage characteristics for the targeted livestock feedstuff to the agricultural model, the agricultural model comprising at least one of a
crop-specific growth model, a soil model, and a land surface model;  and adjusting the simulation of the moisture content of the agricultural commodity at one or more crop growth stages using at least one of 1) differences between the simulated moisture
content and additional data representing in-field observations of the timing of the crop growth as a growing season progresses and sampled observations of the crop moisture content taken at the specific location at times corresponding to the one or more
crop growth stages, determined from a variance between the crop moisture content and the simulated moisture content expected from in-field dry-down of the crop, and 2) one or more artificial intelligence models developed to analyze the moisture content
of the agricultural commodity at one or more crop growth stages from a comprehensive harvest condition dataset for the agricultural model of the one or more physical and empirical characteristics impacting harvest operations of the agricultural
commodity, to generate a prediction of the number of days that the agricultural commodity will be at an appropriate crop growth stage for producing the targeted livestock feedstuff at a desired moisture content for storage of the targeted livestock
feedstuff according to the storage characteristics;  and initiating a harvest operation from the harvest condition output profile representing the predicted harvest condition to harvest the agricultural commodity for the targeted livestock feedstuff,
wherein a user performs the harvest operation in the particular field based on the predicted harvest condition for the agricultural commodity, or an automated harvest operation is controlled based on the predicted harvest condition for the agricultural
commodity.
 22.  The method of claim 21, further comprising generating a harvest output condition profile representing the prediction, and applying the harvest output condition profile to a diagnostic support tool configured to provide one or more
advisories based on the prediction to a user performing harvest operations.
 23.  The method of claim 22, wherein the decision support tool generates an advisory of one or more temporal harvest opportunity windows for the targeted livestock feedstuff at the desired moisture content.
 24.  The method of claim 22, wherein the decision support tool generates a risk advisory of a risk from harvesting targeted livestock feedstuff at a moisture content that exceeds the desired moisture content, the risk advisory including one or
more of damage to a storage structure, loss of feedstuff nutrient levels due to runoff and seepage in the storage structure, and water supply contamination.
 25.  The method of claim 22, wherein the decision support tool generates a risk advisory of a risk from harvesting targeted livestock feedstuff at a lesser moisture content than the desired moisture content, the risk advisory including one or
more of a packing factor, a digestibility, and a sugar content of the targeted livestock feedstuff.
 26.  The method of claim 21, wherein the defined storage characteristics include information for maintaining the targeted livestock feedstuff in different post-harvest storage structures, the different post-harvest storage structures including
an upright tower silo, a bunker silo, a bag silo, and an oxygen-limiting silo.
 27.  The method of claim 21, wherein the additional data representing sampled observations at the specific location at corresponding times is comprised of physical, empirical or observed agricultural information and field and soil information
that includes at least one of sampled crop moisture content, sampled soil temperature, sampled soil moisture content, and current and/or forecasted soil conditions.
 28.  The method of claim 21, wherein the targeted livestock feedstuff is a bovine feedstuff includes one or more of corn silage, corn earlage, corn snaplage, and corn having a specific desired moisture content. 
"
"10,185,917","
     January 22, 2019
","Computer-aided decision systems
"," Decision systems and processes implementing digital personas are
     presented. A digital persona is a digital representation of an entity in
     accordance with a specific set of rules, preferences, or priorities with
     respect to a defined situation or opportunity. A digital persona may
     interact with a universe, which can be a set of conditions and
     information that an artificial intelligence engine implementing the
     digital personas can perceive. The digital personas can learn, via the
     artificial intelligence engine, from actions of a user, events in the
     universe, other personas, or a multitude of other factors. In some
     examples discussed, the artificial intelligence engine may include a
     persona artificial intelligence engine and an evolutionary artificial
     intelligence engine.
",G06Q 30/0619 (20130101); G06N 5/045 (20130101); G06N 20/00 (20190101),G06Q 30/00 (20120101); G06Q 30/06 (20120101); G06N 99/00 (20100101),"[['\n5097407', '\nMarch 1992'], ['\n5412756', '\nMay 1995'], ['\n5537618', '\nJuly 1996'], ['\n5566291', '\nOctober 1996'], ['\n5680305', '\nOctober 1997'], ['\n5732398', '\nMarch 1998'], ['\n5948040', '\nSeptember 1999'], ['\n6023687', '\nFebruary 2000'], ['\n6049784', '\nApril 2000'], ['\n6088686', '\nJuly 2000'], ['\n6119101', '\nSeptember 2000'], ['\n6169981', '\nJanuary 2001'], ['\n6512525', '\nJanuary 2003'], ['\n6738753', '\nMay 2004'], ['\n7072847', '\nJuly 2006'], ['\n7096193', '\nAugust 2006'], ['\n7107239', '\nSeptember 2006'], ['\n7152037', '\nDecember 2006'], ['\n7162494', '\nJanuary 2007'], ['\n7203662', '\nApril 2007'], ['\n7254559', '\nAugust 2007'], ['\n7287008', '\nOctober 2007'], ['\n7302406', '\nNovember 2007'], ['\n7319976', '\nJanuary 2008'], ['\n7376613', '\nMay 2008'], ['\n7386620', '\nJune 2008'], ['\n7464109', '\nDecember 2008'], ['\n7533046', '\nMay 2009'], ['\n7580855', '\nAugust 2009'], ['\n7613692', '\nNovember 2009'], ['\n7739408', '\nJune 2010'], ['\n7769705', '\nAugust 2010'], ['\n7809601', '\nOctober 2010'], ['\n7865404', '\nFebruary 2011'], ['\n7904366', '\nMarch 2011'], ['\n7921068', '\nApril 2011'], ['\n7933399', '\nApril 2011'], ['\n7962578', '\nJune 2011'], ['\n7979314', '\nJuly 2011'], ['\n7984005', '\nJuly 2011'], ['\n8145536', '\nMarch 2012'], ['\n8190568', '\nMay 2012'], ['\n8204790', '\nJune 2012'], ['\n8234375', '\nJuly 2012'], ['\n8257173', '\nSeptember 2012'], ['\n8306874', '\nNovember 2012'], ['\n8326890', '\nDecember 2012'], ['\n8346624', '\nJanuary 2013'], ['\n8364520', '\nJanuary 2013'], ['\n8364559', '\nJanuary 2013'], ['\n8478660', '\nJuly 2013'], ['\n8494936', '\nJuly 2013'], ['\n8521677', '\nAugust 2013'], ['\n8560396', '\nOctober 2013'], ['\n8589529', '\nNovember 2013'], ['\n8606636', '\nDecember 2013'], ['\n8660670', '\nFebruary 2014'], ['\n8666844', '\nMarch 2014'], ['\n8693751', '\nApril 2014'], ['\n8700620', '\nApril 2014'], ['\n2002/0046147', '\nAugust 2002'], ['\n2002/0107764', '\nAugust 2002'], ['\n2003/0004859', '\nJanuary 2003'], ['\n2003/0093289', '\nMay 2003'], ['\n2003/0144945', '\nJuly 2003'], ['\n2003/0208362', '\nNovember 2003'], ['\n2004/0039679', '\nFebruary 2004'], ['\n2004/0107173', '\nJune 2004'], ['\n2004/0128230', '\nJuly 2004'], ['\n2005/0055298', '\nMarch 2005'], ['\n2005/0177448', '\nAugust 2005'], ['\n2005/0177488', '\nAugust 2005'], ['\n2005/0222861', '\nOctober 2005'], ['\n2006/0161482', '\nJuly 2006'], ['\n2006/0184440', '\nAugust 2006'], ['\n2006/0190279', '\nAugust 2006'], ['\n2006/0212386', '\nSeptember 2006'], ['\n2006/0277077', '\nDecember 2006'], ['\n2007/0016488', '\nJanuary 2007'], ['\n2007/0043654', '\nFebruary 2007'], ['\n2007/0106563', '\nMay 2007'], ['\n2007/0106656', '\nMay 2007'], ['\n2007/0130059', '\nJune 2007'], ['\n2007/0162301', '\nJuly 2007'], ['\n2007/0162332', '\nJuly 2007'], ['\n2008/0040141', '\nFebruary 2008'], ['\n2008/0065429', '\nMarch 2008'], ['\n2008/0120244', '\nMay 2008'], ['\n2008/0243637', '\nOctober 2008'], ['\n2008/0270163', '\nOctober 2008'], ['\n2008/0320040', '\nDecember 2008'], ['\n2009/0076926', '\nMarch 2009'], ['\n2009/0138342', '\nMay 2009'], ['\n2009/0254360', '\nAugust 2009'], ['\n2010/0010872', '\nJanuary 2010'], ['\n2010/0121808', '\nMay 2010'], ['\n2010/0241595', '\nSeptember 2010'], ['\n2010/0312464', '\nDecember 2010'], ['\n2011/0015954', '\nJanuary 2011'], ['\n2011/0112869', '\nMay 2011'], ['\n2011/0137776', '\nJune 2011'], ['\n2011/0212428', '\nSeptember 2011'], ['\n2011/0289076', '\nNovember 2011'], ['\n2011/0307478', '\nDecember 2011'], ['\n2011/0320395', '\nDecember 2011'], ['\n2012/0010922', '\nJanuary 2012'], ['\n2012/0084129', '\nApril 2012'], ['\n2012/0143712', '\nJune 2012'], ['\n2012/0158500', '\nJune 2012'], ['\n2012/0221393', '\nAugust 2012'], ['\n2012/0239523', '\nSeptember 2012'], ['\n2012/0239524', '\nSeptember 2012'], ['\n2012/0253517', '\nOctober 2012'], ['\n2012/0253907', '\nOctober 2012'], ['\n2012/0265646', '\nOctober 2012'], ['\n2012/0284138', '\nNovember 2012'], ['\n2012/0330774', '\nDecember 2012'], ['\n2013/0066697', '\nMarch 2013'], ['\n2013/0151368', '\nJune 2013'], ['\n2013/0151369', '\nJune 2013'], ['\n2013/0173335', '\nJuly 2013'], ['\n2013/0173336', '\nJuly 2013'], ['\n2013/0173418', '\nJuly 2013'], ['\n2013/0231824', '\nSeptember 2013'], ['\n2013/0287270', '\nOctober 2013'], ['\n2013/0316834', '\nNovember 2013'], ['\n2013/0325652', '\nDecember 2013'], ['\n2014/0032467', '\nJanuary 2014'], ['\n2014/0044364', '\nFebruary 2014'], ['\n2014/0046891', '\nFebruary 2014'], ['\n2014/0074617', '\nMarch 2014'], ['\n2014/0081684', '\nMarch 2014'], ['\n2014/0108189', '\nApril 2014'], ['\n2014/0114705', '\nApril 2014'], ['\n2014/0214486', '\nJuly 2014'], ['\n2014/0214615', '\nJuly 2014']]","[2, '11,055,761', '10,922,105']"," What is claimed is:  1.  A method comprising: normalizing search results corresponding to a query at a normalizer of a digital persona decision system by extracting data from the search results,
transforming the extracted data into a format suitable for a table, loading the data into the table, and processing the data in the table to produce a set of multiple possible solutions;  configuring an artificial intelligence engine using one or more
personas of a plurality of personas, the one or more personas corresponding to digital representations of different decision-making characteristics of an entity that initiated the query, the entity including at least one of a user communicating with the
digital persona system through a network via a computing device and a computing device operating under control of a computer program;  processing the set to weigh and score each of the multiple possible solutions using the artificial intelligence engine
configured according to the one or more personas of a plurality of personas;  determining a selected solution from the multiple possible solutions using the artificial intelligence engine configured according to the one or more personas and based on the
scores;  and sending the selected solution within a graphical interface to one of the computing device associated with the user and the computing device operating under the control of the computer program.
 2.  The method of claim 1, wherein processing the multiple possible solutions comprises: selecting a persona of the one or more personas;  configuring the artificial intelligence engine with the selected persona;  determining a score for each
parameter of a plurality of parameters for each of the multiple possible solutions based on the selected persona;  and repeating the selecting, the configuring, and the determining for each of the one or more personas.
 3.  The method of claim 1, wherein determining the selected solution comprises: determining a score for each parameter of each solution of the multiple possible solutions based on each of the one or more personas;  determining an average of all
of the scores for each parameter;  summing the scores for each solution of the multiple possible solutions;  and determining the selected solution based on the sum.
 4.  The method of claim 3, further comprising: determining a veto reference value associated with a selected digital persona of the one or more digital personas, the veto reference value corresponding to a parameter of the multiple possible
solutions;  and when a veto reference value is present for the parameter, selecting a solution of the multiple possible solutions for the selected digital persona based on the score for the parameter.
 5.  The method of claim 3, wherein determining the score comprises: determining a relative power differentiation of each of the one or more digital personas and for each parameter;  and selectively amending the score for each parameter for each
of the one or more digital personas based on the relative power differentiation.
 6.  A computing system comprising: an interface configured to communicate with one or more devices through a communications network;  a processor coupled to the interface;  and a memory accessible to the processor and configured to store
instructions that, when executed, cause the processor to: select one or more personas of a plurality of personas, the one or more selected personas representing at least one of a user of a first device or a program executing on a second device that sent
a query to the interface, each of the one or more personas corresponding to a digital representations of a plurality of decision-making characteristics that is different in at least one of the plurality of decision-making characteristics from others of
the selected one or more personas;  configure a persona artificial intelligence (AI) engine using the one or more selected personas of a plurality of personas, the persona AI engine configured by the one or more selected personas to receive multiple
possible solutions, weigh and score the multiple possible solutions based on each of the one or more selected personas to provide multiple scored solutions;  select a subset of the multiple scored solutions using at least one of the one or more selected
personas;  and provide a graphical interface including at least one result from the subset to one of the first device and the second device.
 7.  The computing system of claim 6, wherein the memory further includes instructions that, when executed, cause the processor to: process information pertaining to one or more personas to produce a persona adjustment;  and selectively modify at
least one persona based on the persona adjustment;  wherein the selectively modified at least one persona can subsequently be selected and applied to the AI engine to cause decision-making by the AI engine to evolve over time based on the selectively
modified at least one personal.
 8.  The computing system of claim 7, wherein the information comprises a plurality of snapshots associated with one or more personas.
 9.  The computing system of claim 7, wherein the evolutionary AI engine selectively modifies the at least one persona in response to a triggering event.
 10.  A system comprising: an interface configured to communicate with one or more data sources through a network;  a processor coupled to the interface;  a memory coupled to the processor and configured to store a plurality of digital personas
and to store instructions that, when executed, cause the processor to: select one or more digital personas from the plurality of digital personas in the memory to represent a user of a computing device that sent a query to the interface, each of the one
or more personas corresponding to a digital representation of a plurality of decision-making characteristics that is different in at least one of the plurality of decision-making characteristics from others of the selected one or more personas; 
configure an artificial intelligence (AI) engine with the selected one or more digital personas to interact with the one or more data sources to determine a list of potential solutions to a problem, the AI engine configured to weigh and score each of the
potential solutions based on each of the selected one or more personas to provide multiple scored solutions;  and execute a selector module to select one of the multiple scored solutions as an output based on the digital persona;  and provide a graphical
interface including the output to the computing device through the network.
 11.  The system of claim 10, further comprising a query normalizer module executable by the processor to: normalize a query into data formatted for a particular search;  cause the query to be performed;  receive results based on the query;  and
provide the results to the artificial intelligence engine.
 12.  The system of claim 10, wherein the artificial intelligence engine further comprises: a persona artificial intelligence engine configured to apply the persona to search results;  and an evolutionary artificial intelligence engine configured
to initiate changes in the persona.
 13.  The system of claim 12, wherein the evolutionary artificial intelligence engine is further configured to initiate changes in the persona based on user interactions with the search results.
 14.  The system of claim 12, wherein the evolutionary artificial intelligence engine is further configured to initiate changes in the persona based on information derived from an other persona.
 15.  The system of claim 12, wherein the evolutionary artificial intelligence engine is further configured to initiate changes in the persona based on information derived from a universe in which the persona interacts.
 16.  The system of claim 10, further comprising a persona manager module executable by the processor to select the persona from multiple available personas.
 17.  The system of claim 10, further comprising: an input/output (I/O) normalizer module executable by the processor to translates received data into a format suitable for processing by the artificial intelligence engine;  and an application
programming interface (API) configured to: coordinate interactions between the system and external components;  and receive data from a network communicatively coupled to the API;  and provide the data to the I/O normalizer.
 18.  The system of claim 17, further comprising: a persona manager module executable by the processor to: receive data from the I/O normalizer;  and selectively execute one of a persona artificial intelligence engine, which applies selected
personas to data to analyze the potential solutions, and an evolutionary artificial intelligence engine, which may initiate changes in selected personas.
 19.  The system of claim 10, wherein at least one of the selected one or more personas includes an expert persona representing decision-making characteristics of an individual previously determined to be an expert.
 20.  The system of claim 10, wherein at least one of the selected one or more personas includes a trusted persona representing decision-making characteristics of an individual previously determined by the user to be a trusted individual. 
"
"10,186,115","
     January 22, 2019
","System and method of revealing real world wager outcomes based on user
     interactions with interactive media
"," The invention relates to systems and methods of selecting and placing
     real-world wagers responsive to one or more wager triggers, obtaining
     outcomes of the real-world wagers, facilitating user interactions with
     various interactive media, and revealing the outcomes of the real-world
     wagers through the interactive media to give an appearance that the
     outcomes of the real-world wagers resulted from the user interactions
     even though the outcomes resulted from the real-world wagers and were
     determined before the user interactions. By integrating real-world wager
     outcomes with gameplay and other user interactions, the system may
     provide an engaging user experience with interactive media. For example,
     a game may reveal an outcome of a real-world wager (which has already
     occurred), in response to in-game actions such as user actions in the
     game, other players' actions in the game, game events (e.g., events
     caused by the game logic/artificial intelligence), and/or other events.
",G07F 17/3225 (20130101); G07F 17/3288 (20130101); G07F 17/3227 (20130101); G07F 17/3262 (20130101); A63F 13/00 (20130101); G07F 17/3244 (20130101); G07F 17/326 (20130101); G07F 17/3211 (20130101); G07F 17/3209 (20130101); G07F 17/3241 (20130101),A63F 13/50 (20140101); G07F 17/32 (20060101); A63F 13/00 (20140101),"[['\n4582324', '\nApril 1986'], ['\n5709603', '\nJanuary 1998'], ['\n5749785', '\nMay 1998'], ['\n6450887', '\nSeptember 2002'], ['\n6511375', '\nJanuary 2003'], ['\n6729961', '\nMay 2004'], ['\n6749500', '\nJune 2004'], ['\n6761632', '\nJuly 2004'], ['\n6786818', '\nSeptember 2004'], ['\n6939224', '\nSeptember 2005'], ['\n7530892', '\nMay 2009'], ['\n7658672', '\nFebruary 2010'], ['\n7666082', '\nFebruary 2010'], ['\n7699706', '\nApril 2010'], ['\n7785187', '\nAugust 2010'], ['\n7798896', '\nSeptember 2010'], ['\n7931531', '\nApril 2011'], ['\n7942735', '\nMay 2011'], ['\n7950993', '\nMay 2011'], ['\n8012014', '\nSeptember 2011'], ['\n8162735', '\nApril 2012'], ['\n8167309', '\nMay 2012'], ['\n8348753', '\nJanuary 2013'], ['\n8353757', '\nJanuary 2013'], ['\n8376833', '\nFebruary 2013'], ['\n8393949', '\nMarch 2013'], ['\n8430735', '\nApril 2013'], ['\n8449375', '\nMay 2013'], ['\n8613649', '\nDecember 2013'], ['\n8651960', '\nFebruary 2014'], ['\n8657680', '\nFebruary 2014'], ['\n8721433', '\nMay 2014'], ['\n8764567', '\nJuly 2014'], ['\n8777735', '\nJuly 2014'], ['\n8784198', '\nJuly 2014'], ['\n8819758', '\nAugust 2014'], ['\n8845409', '\nSeptember 2014'], ['\n8864564', '\nOctober 2014'], ['\n8864572', '\nOctober 2014'], ['\n8915781', '\nDecember 2014'], ['\n8961300', '\nFebruary 2015'], ['\n8961301', '\nFebruary 2015'], ['\n8968082', '\nMarch 2015'], ['\n8968104', '\nMarch 2015'], ['\n8974284', '\nMarch 2015'], ['\n8986096', '\nMarch 2015'], ['\n8992311', '\nMarch 2015'], ['\n8992312', '\nMarch 2015'], ['\n9070252', '\nJune 2015'], ['\n9070253', '\nJune 2015'], ['\n9076294', '\nJuly 2015'], ['\n9076295', '\nJuly 2015'], ['\n9092939', '\nJuly 2015'], ['\n9092940', '\nJuly 2015'], ['\n9214063', '\nDecember 2015'], ['\n9224262', '\nDecember 2015'], ['\n9299218', '\nMarch 2016'], ['\n9305428', '\nApril 2016'], ['\n9317997', '\nApril 2016'], ['\n9317998', '\nApril 2016'], ['\n9317999', '\nApril 2016'], ['\n9406195', '\nAugust 2016'], ['\n9443383', '\nSeptember 2016'], ['\n9489798', '\nNovember 2016'], ['\n9536385', '\nJanuary 2017'], ['\n9552694', '\nJanuary 2017'], ['\n9552696', '\nJanuary 2017'], ['\n9558623', '\nJanuary 2017'], ['\n2001/0007828', '\nJuly 2001'], ['\n2001/0008842', '\nJuly 2001'], ['\n2002/0010013', '\nJanuary 2002'], ['\n2002/0022516', '\nFebruary 2002'], ['\n2002/0049082', '\nApril 2002'], ['\n2002/0077165', '\nJune 2002'], ['\n2002/0151363', '\nOctober 2002'], ['\n2002/0169018', '\nNovember 2002'], ['\n2003/0003980', '\nJanuary 2003'], ['\n2003/0013514', '\nJanuary 2003'], ['\n2003/0054879', '\nMarch 2003'], ['\n2003/0060276', '\nMarch 2003'], ['\n2003/0078103', '\nApril 2003'], ['\n2003/0087683', '\nMay 2003'], ['\n2003/0199312', '\nOctober 2003'], ['\n2003/0220133', '\nNovember 2003'], ['\n2004/0072618', '\nApril 2004'], ['\n2004/0229671', '\nNovember 2004'], ['\n2004/0235542', '\nNovember 2004'], ['\n2004/0259631', '\nDecember 2004'], ['\n2005/0023758', '\nFebruary 2005'], ['\n2005/0054441', '\nMarch 2005'], ['\n2005/0143170', '\nJune 2005'], ['\n2005/0164779', '\nJuly 2005'], ['\n2006/0035697', '\nFebruary 2006'], ['\n2006/0052148', '\nMarch 2006'], ['\n2006/0082056', '\nApril 2006'], ['\n2006/0111172', '\nMay 2006'], ['\n2006/0121973', '\nJune 2006'], ['\n2006/0135249', '\nJune 2006'], ['\n2006/0154714', '\nJuly 2006'], ['\n2006/0154720', '\nJuly 2006'], ['\n2006/0154721', '\nJuly 2006'], ['\n2006/0154724', '\nJuly 2006'], ['\n2006/0258459', '\nNovember 2006'], ['\n2006/0294568', '\nDecember 2006'], ['\n2007/0060254', '\nMarch 2007'], ['\n2007/0060295', '\nMarch 2007'], ['\n2007/0167226', '\nJuly 2007'], ['\n2007/0173327', '\nJuly 2007'], ['\n2007/0178969', '\nAugust 2007'], ['\n2007/0265060', '\nNovember 2007'], ['\n2007/0265068', '\nNovember 2007'], ['\n2007/0293305', '\nDecember 2007'], ['\n2008/0026816', '\nJanuary 2008'], ['\n2008/0032762', '\nFebruary 2008'], ['\n2008/0045295', '\nFebruary 2008'], ['\n2008/0064478', '\nMarch 2008'], ['\n2008/0070669', '\nMarch 2008'], ['\n2008/0085769', '\nApril 2008'], ['\n2008/0108406', '\nMay 2008'], ['\n2008/0108425', '\nMay 2008'], ['\n2008/0176637', '\nJuly 2008'], ['\n2008/0200225', '\nAugust 2008'], ['\n2008/0270288', '\nOctober 2008'], ['\n2008/0300055', '\nDecember 2008'], ['\n2009/0054148', '\nFebruary 2009'], ['\n2009/0093299', '\nApril 2009'], ['\n2009/0227367', '\nSeptember 2009'], ['\n2009/0227377', '\nSeptember 2009'], ['\n2009/0247272', '\nOctober 2009'], ['\n2009/0291736', '\nNovember 2009'], ['\n2009/0312094', '\nDecember 2009'], ['\n2010/0016056', '\nJanuary 2010'], ['\n2010/0035679', '\nFebruary 2010'], ['\n2010/0062840', '\nMarch 2010'], ['\n2010/0120491', '\nMay 2010'], ['\n2010/0144428', '\nJune 2010'], ['\n2010/0304843', '\nDecember 2010'], ['\n2010/0311496', '\nDecember 2010'], ['\n2011/0086702', '\nApril 2011'], ['\n2011/0212766', '\nSeptember 2011'], ['\n2011/0218028', '\nSeptember 2011'], ['\n2011/0224001', '\nSeptember 2011'], ['\n2011/0224002', '\nSeptember 2011'], ['\n2011/0250974', '\nOctober 2011'], ['\n2011/0287841', '\nNovember 2011'], ['\n2012/0071223', '\nMarch 2012'], ['\n2012/0100918', '\nApril 2012'], ['\n2012/0172112', '\nJuly 2012'], ['\n2012/0178514', '\nJuly 2012'], ['\n2012/0190435', '\nJuly 2012'], ['\n2012/0201365', '\nAugust 2012'], ['\n2012/0202571', '\nAugust 2012'], ['\n2012/0214576', '\nAugust 2012'], ['\n2012/0244923', '\nSeptember 2012'], ['\n2012/0302312', '\nNovember 2012'], ['\n2012/0309499', '\nDecember 2012'], ['\n2012/0309511', '\nDecember 2012'], ['\n2012/0315979', '\nDecember 2012'], ['\n2012/0315981', '\nDecember 2012'], ['\n2012/0322545', '\nDecember 2012'], ['\n2013/0053118', '\nFebruary 2013'], ['\n2013/0072288', '\nMarch 2013'], ['\n2013/0084951', '\nApril 2013'], ['\n2013/0102372', '\nApril 2013'], ['\n2013/0122999', '\nMay 2013'], ['\n2013/0165209', '\nJune 2013'], ['\n2013/0165210', '\nJune 2013'], ['\n2013/0172059', '\nJuly 2013'], ['\n2013/0190074', '\nJuly 2013'], ['\n2013/0217471', '\nAugust 2013'], ['\n2013/0225272', '\nAugust 2013'], ['\n2013/0225297', '\nAugust 2013'], ['\n2013/0225298', '\nAugust 2013'], ['\n2013/0237326', '\nSeptember 2013'], ['\n2013/0244764', '\nSeptember 2013'], ['\n2013/0244765', '\nSeptember 2013'], ['\n2013/0252687', '\nSeptember 2013'], ['\n2013/0252693', '\nSeptember 2013'], ['\n2013/0252718', '\nSeptember 2013'], ['\n2013/0324220', '\nDecember 2013'], ['\n2013/0324228', '\nDecember 2013'], ['\n2013/0337887', '\nDecember 2013'], ['\n2014/0024437', '\nJanuary 2014'], ['\n2014/0024442', '\nJanuary 2014'], ['\n2014/0024454', '\nJanuary 2014'], ['\n2014/0040765', '\nFebruary 2014'], ['\n2014/0080585', '\nMarch 2014'], ['\n2014/0082645', '\nMarch 2014'], ['\n2014/0087801', '\nMarch 2014'], ['\n2014/0087851', '\nMarch 2014'], ['\n2014/0094274', '\nApril 2014'], ['\n2014/0135084', '\nMay 2014'], ['\n2014/0274263', '\nSeptember 2014'], ['\n2014/0274264', '\nSeptember 2014'], ['\n2014/0274265', '\nSeptember 2014'], ['\n2014/0274266', '\nSeptember 2014'], ['\n2014/0274267', '\nSeptember 2014'], ['\n2014/0274268', '\nSeptember 2014'], ['\n2014/0274269', '\nSeptember 2014'], ['\n2014/0274270', '\nSeptember 2014'], ['\n2014/0274271', '\nSeptember 2014'], ['\n2014/0274272', '\nSeptember 2014'], ['\n2014/0274325', '\nSeptember 2014'], ['\n2014/0274326', '\nSeptember 2014'], ['\n2014/0274327', '\nSeptember 2014'], ['\n2014/0274328', '\nSeptember 2014'], ['\n2014/0274329', '\nSeptember 2014'], ['\n2014/0274330', '\nSeptember 2014'], ['\n2014/0274331', '\nSeptember 2014'], ['\n2014/0274333', '\nSeptember 2014'], ['\n2014/0274334', '\nSeptember 2014'], ['\n2014/0274335', '\nSeptember 2014'], ['\n2014/0274336', '\nSeptember 2014'], ['\n2014/0274337', '\nSeptember 2014'], ['\n2014/0274338', '\nSeptember 2014'], ['\n2014/0274339', '\nSeptember 2014'], ['\n2014/0274340', '\nSeptember 2014'], ['\n2014/0274352', '\nSeptember 2014'], ['\n2014/0274365', '\nSeptember 2014'], ['\n2014/0302905', '\nOctober 2014'], ['\n2015/0170470', '\nJune 2015'], ['\n2015/0170471', '\nJune 2015'], ['\n2015/0199875', '\nJuly 2015'], ['\n2015/0206389', '\nJuly 2015'], ['\n2015/0332554', '\nNovember 2015'], ['\n2016/0071368', '\nMarch 2016'], ['\n2016/0071372', '\nMarch 2016'], ['\n2016/0098898', '\nApril 2016'], ['\n2016/0232746', '\nAugust 2016'], ['\n2016/0247351', '\nAugust 2016'], ['\n2016/0275753', '\nSeptember 2016'], ['\n2017/0127135', '\nMay 2017']]",[0]," What is claimed is:  1.  A device that that reveals outcomes of wagers placed on behalf of a user, the device comprising: one or more physical processors programmed with computer program
instructions that, when executed by the one or more physical processors, program the device to: determine a first location of the device;  determine that the first location corresponds to one or more locations at which wagers are legally permitted; 
responsive to the determination that the first location corresponds to the one or more locations at which wagers are legally permitted, cause a wager to be placed on behalf of the user based on one or more wager parameters;  obtain an outcome of the
wager;  initiate a particular session of an interactive media application;  obtain one or more reveal parameters that specify a manner in which to reveal at least a portion of the outcome via the interactive media application;  and reveal the portion of
the outcome to the user at a second location different than the first location via the interactive media application, wherein the portion of the outcome is revealed based on the one or more reveal parameters to give the user the impression that user
interaction with the interactive media application resulted in the portion of the outcome.
 2.  The device of claim 1, wherein to cause the wager to be placed on behalf of the user, the device is programmed to: cause instructions to place the wager on behalf of the user to be transmitted to a third-party wagering system via a network.
 3.  The device of claim 1, wherein the device is further programmed to: receive physical user input indicating one or more wager parameters that specify information used to place the wager, wherein the wager is caused to be placed based on the
one or more wager parameters, the one or more wager parameters comprising an indication of a type of wager event in which to place the wager, a particular wager event in which to place the wager, a particular outcome that is wagered upon, an amount of
the wager, a number of wagers to place, and/or a risk level of the wager.
 4.  The device of claim 1, wherein to obtain the one or more reveal parameters, the device is programmed to: communicate a request for a reveal specification associated with the user to a computer system, wherein the reveal specification
includes the one or more reveal parameters and an indication of the portion of the outcome to be revealed via the interactive media application, and wherein the reveal specification is received from the computer system responsive to the request.
 5.  The device of claim 4, wherein the device further comprises a memory, and wherein the device is further programmed to: cause the wager to be placed on behalf of the user via a communication link between the device and the computer system; 
cause the reveal specification to be cached in the memory, wherein the reveal specification is received via the communication link;  and reveal, based on the reveal specification cached in the memory, the portion of the outcome to the user while the
communication link between the device and the computer system is disconnected.
 6.  The device of claim 5, wherein the interactive media application comprises a game, and wherein the device is further programmed to: receive physical user input related to gameplay of the game while the communication link between the device
and the computer system is disconnected, wherein the physical user input comprises the user interaction with the interactive media application;  and reveal the portion of the outcome to the user within the game while the communication link between the
device and the computer system is disconnected.
 7.  The device of claim 5, wherein the device is further programmed to: store, in the memory, information that indicates the portion or all of the amount that was indicated to the user;  and communicate, to the computer system, the information
that indicates the portion or all of the amount that was revealed to the user when a new communication link with the computer system has been established.
 8.  The device of claim 1, wherein the device is further programmed to: communicate an indication that the portion of the outcome was revealed via the interactive media application to the computer system, wherein a revealed balance of a user
account associated with the user is incremented responsive to the indication.
 9.  The device of claim 1, wherein the one or more reveal parameters comprise an indication of when a reveal may occur, an identification of one or more interactive media applications able to make the reveal, an amount of the reveal, a number of
reveals to make, a geo-location associated with the reveal, and/or a promotion associated with the reveal.
 10.  The device of claim 1, wherein the one or more reveal parameters indicate that the portion of the outcome is to be revealed based on completion of an objective related to the interactive media application, wherein the device is further
programmed to: determine that the user has completed the objective based on user interaction with the interactive media application, wherein the portion of the outcome is revealed responsive to the completed objective to give the user the impression that
the user won the portion of the outcome based on the completed objective.
 11.  The device of claim 10, wherein the interactive media application comprises a game and the objective comprises an objective within the game, wherein the device is further programmed to: obtain a level of completion of the objective based on
the user interaction, wherein the user interaction comprises physical user input received via the device;  determine that the level of completion satisfies a minimum level of completion of the objective within the game for revealing at least the portion
of the outcome based on the one or more reveal parameters;  and responsive to the determination that the level of completion satisfies the minimum level of completion, reveal the portion of the outcome to the user via the interactive media application to
give the appearance that the user won the revealed portion based on the level of completion of the objective within the game.
 12.  The device of claim 10, wherein completion of the objective requires downloading media content, wherein the device is further programmed to: receive a request from the user via the interactive media application to download the media
content;  cause the media content to be downloaded via the interactive media application based on the request;  and responsive to the media content being successfully downloaded via the interactive media application, reveal the portion of the outcome to
the user via the interactive media application to give the appearance that the user won the revealed portion based on the download of the media content via the interactive media application.
 13.  The device of claim 1, wherein the one or more reveal parameters indicate that the portion of the outcome is to be revealed when a location associated with the user corresponds to a location associated with a reveal opportunity, wherein the
device is further programmed to: determine that the user is at the second location;  determine that the second location corresponds to the location associated with the reveal opportunity;  and responsive to the determination that the second location
corresponds to the location associated with the reveal opportunity, reveal the portion of the outcome to the user via the interactive media application.
 14.  The device of claim 13, wherein the determination that the second location corresponds to the location associated with the reveal opportunity comprises a determination that the second location is the same as the location associated with the
reveal opportunity.
 15.  The device of claim 13, wherein the determination that the second location corresponds to the location associated with the reveal opportunity comprises a determination that the second location is in a proximity to the location associated
with the reveal opportunity.
 16.  The device of claim 1, wherein to initiate the particular session of the interactive media application, the device is further programmed to: obtain one or more authentication credentials associated with the user;  and authenticate the user
using the one or more authentication credentials, wherein the outcome of the wager and the one or more revealed parameters are obtained based on the authenticated user.
 17.  The device of claim 1, wherein to cause the wager to be placed, the device is further programmed to: cause a random number generator to generate a random number on which the outcome is based.
 18.  The device of claim 1, wherein the reveal parameters specify a first portion of the outcome to be potentially revealed via the interactive media application and a second portion of the outcome to be potentially revealed via a second
interactive media application, wherein the device is further programmed to: determine that the first portion of the outcome is to be revealed to the user based on the initiation of the particular session of the interactive media application such that the
portion of the outcome to reveal is customized based on the interactive media application, wherein the portion fo the outcome comprises the first portion of the outcome.
 19.  The device of claim 1, wherein the second location corresponds to a location at which placing the wager is not legally permitted.
 20.  The device of claim 1, wherein the device further comprises a GPS component, wherein the GPS component is configured to: generate location information indicating a location of the device at a given point in time, wherein the first location
of the device is determined based on the location information.
 21.  A device that that reveals outcomes of wagers placed on behalf of a user, wherein the wagers are placed via a communication link between the device and a computer system, the device comprising: one or more physical processors programmed
with computer program instructions that, when executed by the one or more physical processors, program the device to: receive one or more wager parameters for a wager;  transmit the one or more wager parameters to the computer system via the
communication link;  obtain one or more reveal parameters that specify a manner in which to reveal at least a portion of the outcome;  and initiate an interactive media application after the communication link has been disconnected;  and reveal, via the
interactive media application, the portion of the outcome to the user based on the one or more reveal parameters, wherein the portion of the outcome is revealed to give the user the impression that user interaction with the interactive media application
resulted in the portion of the outcome.
 22.  The device of claim 21, wherein the device further comprises a memory, wherein the device is further programmed to: cause the one or more reveal parameters to be cached in the memory;  and obtain the one or more reveal parameters from the
memory to reveal the portion of the outcome.
 23.  The device of claim 22, wherein to obtain the one or more reveal parameters, the device is further programmed to obtain the one or more reveal parameters from the computer system via the communication link. 
"
"10,187,405","
     January 22, 2019
","Macro grid governance and communication
"," A governance apparatus and a communication method for communicating
     within the governance apparatus. The governance apparatus includes a
     Government. The Government includes Councils such that a macro grid
     including an artificial intelligence and the Government is configured to
     respond to an alert pertaining to an event through use of the artificial
     intelligence and the Government. The governance apparatus also includes
     an enhanced Transmission Control Protocol/Internet Protocol (TCP/IP)
     communication stack of layers including a Governance Layer and an
     Intelligence Layer. The Intelligence Layer includes intelligence software
     configured to process data pertaining to the event, data pertaining to
     the alert, and data pertaining to the Government. The Governance Layer
     includes governance software configured to filter data in a TCP/IP packet
     header structure through data security and data integrity algorithms,
     both to and from the intelligence software in the Intelligence Layer, to
     protect the artificial intelligence from attack.
",H04L 63/145 (20130101); G06N 5/02 (20130101); H04W 4/90 (20180201); H04L 69/161 (20130101); H04L 63/1416 (20130101); H04L 69/16 (20130101); H04L 9/40 (20220501); G06N 5/047 (20130101); G06N 20/00 (20190101),G06E 1/00 (20060101); H04L 29/06 (20060101); G06E 3/00 (20060101); G06N 3/04 (20060101); G06N 5/02 (20060101); H04W 4/90 (20180101); G06N 5/04 (20060101); G06N 99/00 (20100101),"[['\n5243704', '\nSeptember 1993'], ['\n5253141', '\nOctober 1993'], ['\n5301089', '\nApril 1994'], ['\n5537111', '\nJuly 1996'], ['\n6242984', '\nJune 2001'], ['\n6598124', '\nJuly 2003'], ['\n7194445', '\nMarch 2007'], ['\n7343222', '\nMarch 2008'], ['\n7461130', '\nDecember 2008'], ['\n7521138', '\nApril 2009'], ['\n7594015', '\nSeptember 2009'], ['\n7603441', '\nOctober 2009'], ['\n7962425', '\nJune 2011'], ['\n8035337', '\nOctober 2011'], ['\n8080999', '\nDecember 2011'], ['\n8429381', '\nApril 2013'], ['\n8664911', '\nMarch 2014'], ['\n8738832', '\nMay 2014'], ['\n8819395', '\nAugust 2014'], ['\n9154362', '\nOctober 2015'], ['\n2005/0160424', '\nJuly 2005'], ['\n2007/0073861', '\nMarch 2007'], ['\n2008/0027591', '\nJanuary 2008'], ['\n2008/0133052', '\nJune 2008'], ['\n2008/0216094', '\nSeptember 2008'], ['\n2009/0089078', '\nApril 2009'], ['\n2010/0145536', '\nJune 2010'], ['\n2013/0194737', '\nAugust 2013'], ['\n2015/0304348', '\nOctober 2015']]",[0]," The claims are as follows:  1.  A governance apparatus, comprising: a Government comprising a plurality of governmental components, said governmental components collectively comprising a
plurality of Councils such that a macro grid comprising an artificial intelligence and the Government is configured to respond to an alert pertaining to an event through use of the artificial intelligence and the Government, each governmental component
being an either an Executive or a Parliament;  an enhanced Transmission Control Protocol/Internet Protocol (TCP/IP) communication stack of layers comprising a Governance Layer and an Intelligence Layer, wherein the Intelligence Layer comprises
intelligence software configured to process data pertaining to the event, data pertaining to the alert, and data pertaining to the Government, and wherein the Governance Layer comprises governance software configured to filter data in a TCP/IP packet
header structure through data security and data integrity algorithms, both to and from the intelligence software in the Intelligence Layer, to protect the artificial intelligence from attack.
 2.  The governance apparatus of claim 1, further comprising: a plurality of micro grid apparatuses, each micro grid apparatus being wirelessly connected to another micro grid apparatus of the plurality of micro grid apparatuses, each micro grid
apparatus comprising a unique governmental component of the plurality of governmental components, each Executive consisting of a unique processor of a plurality of processors disposed in a unique simple micro grid apparatus of the plurality of micro grid
apparatuses, each Parliament consisting of a unique processor of each plurality of processors of at least two pluralities of processors, each processor of each plurality of processors of each micro grid apparatus having its own operating system, each
unique processor in each Executive or Parliament in the Government being a Council of the plurality of Councils and having a unique operating system differing from the operating system of each other processor in the plurality of processors that comprises
said each unique processor.
 3.  The governance apparatus of claim 2, wherein the plurality of micro grid apparatuses consists of a plurality of simple micro grid apparatuses;  wherein the plurality of governmental components consists of a plurality of Executives;  and
wherein each simple micro grid apparatus of the plurality of micro grid apparatuses comprises a respective Executive of the plurality of Executives.
 4.  The governance apparatus of claim 2, wherein the plurality of micro grid apparatuses comprise a first complex micro grid apparatus consisting of a first connectivity structure;  wherein the Government comprises a first Parliament within the
first connectivity structure.
 5.  The governance apparatus of claim 4, wherein the first connectivity structure is a bridge structure that comprises a first micro grid system and a second micro grid system physically linked together by a bridge module;  wherein the first
micro grid system and the second micro grid system each comprise at least one Council of the plurality of Councils;  wherein the first Parliament comprises the at least one Council in the first micro grid system and the at least one Council in the second
micro grid system;  wherein the bridge module comprises a first bridge unit and a second bridge unit connected together by a bridge hinge;  and wherein the bridge hinge provides the bridge module with sufficient physical flexibility to enable the first
bridge unit to dock and be ensconced into a first bridge docking bay of the first micro grid system and the second bridge unit to dock and be ensconced into a second bridge docking bay of the second micro grid system.
 6.  The governance apparatus of claim 5, wherein the first micro grid system is a micro grid structure comprising a first central area and radial arms external to and integral with the first central area to define first docking bays that include
the first bridge docking bay;  wherein the at least one Council in the first micro grid system consist of a first Council;  wherein the first central area comprises a first plurality of processors that include the first Council;  wherein the second micro
grid system is a power hub comprising a second central area and radial arms external to and integral with the second central area to define second docking bays that include the second bridge docking bay;  wherein the at least one Council in the second
micro grid system comprises a second Council;  wherein a micro grid processor docking bay of the second docking bays comprises a second plurality of processors that include the second Council;  wherein the power hub comprises a plurality of rechargeable
batteries that provide electrical power for at least the second plurality of processors;  and wherein a battery docking bay of the second docking bays comprises a failsafe battery to provide back up power for the rechargeable batteries in the power hub
or additional power to supplement the power provided by the rechargeable batteries in the power hub.
 7.  A communication method, said method comprising: providing a governance apparatus, said governance apparatus comprising: a Government comprising a plurality of governmental components, said governmental components collectively comprising a
plurality of Councils such that a macro grid comprising an artificial intelligence and the Government is configured to respond to an alert pertaining to an event through use of the artificial intelligence and the Government, each governmental component
being an either an Executive or a Parliament;  and communicating between governance entities within the governance apparatus, said Government responding to the alert, each governance entity being a Council of the plurality of Councils, said communicating
comprising a first Council of the plurality of Councils sending a message to a second Council of the plurality of Councils in accordance with an enhanced Transmission Control Protocol/Internet Protocol (TCP/IP) communication stack of layers and a TCP/IP
packet header structure comprising an enhanced IP header, an enhanced TCP header, and a Data Area, wherein the TCP/IP communication stack of layers comprises a Governance Layer and an Intelligence Layer, wherein the Intelligence Layer comprises
intelligence software configured to process data pertaining to the event, data pertaining to the alert, and data pertaining to the Government, and wherein the Governance Layer comprises governance software configured to filter data in the TCP/IP packet
header structure through data security and data integrity algorithms, both to and from the intelligence software in the Intelligence Layer, to protect the artificial intelligence from attack.
 8.  The method of claim 7, further comprising assigning the artificial intelligence a primary Class E IP address having its five high order bits set to 1 1 1 1 0, wherein the enhanced IP header comprises a source IP address pertaining to the
first Council and being linked as a first sub-IP address to the primary Class E IP address of the artificial intelligence, and wherein the enhanced IP header further comprises a destination IP address pertaining to the second Council and being linked as
a second sub-IP address to the primary Class E IP address of the artificial intelligence.
 9.  The method of claim 8, wherein the artificial intelligence is residing in a primary Council of the plurality of Councils having the primary Class E IP address, and wherein the governance software is further configured to relocate the
artificial intelligence to another Council of the plurality of Councils.
 10.  The method of claim 8, wherein the artificial intelligence is residing in a primary Council of the plurality of Councils having the primary Class E IP address and having an artificial intelligence responsibility for implementing the
artificial intelligence, and wherein the method further comprises: assigning a first mirror backup Council to the primary Council;  after said assigning the mirror backup Council, ascertaining that the primary Council cannot be located;  responsive to
said ascertaining, said mirror backup Council becoming a replacement primary Council by assuming the artificial intelligence responsibility and inheriting the primary Class E IP address;  and assigning a second mirror backup Council to the replacement
primary Council.
 11.  The method of claim 7, wherein the method further comprises said artificial intelligence placing instructive data into the Data Area to request application software to undertake application tasks across the Government for responding to the
alert.
 12.  The method of claim 7, wherein the enhanced TCP header comprises a multi bit identifier consisting of a sequence of bits that identify a governance entity type in whose behalf the message is being sent by the first Council.
 13.  The method of claim 7, wherein the method further comprises said intelligence software writing Kind and Descriptor data into the enhanced TCP header, wherein the Kind and Descriptor data comprises parameters pertaining to the event, the
alert, and/or the Government.
 14.  The method of claim 13, wherein the method further comprises said intelligence software configured to read the Kind and Descriptor data in the TCP header upon being implemented by the second Council after the second Council receives the
message.
 15.  The method of claim 13, wherein the Kind and Descriptor data comprise a scale of the event, a scale of the response to the event, and GPS data which identify a location of the event or a location of a source of the event.
 16.  The method of claim 13, wherein the Kind and Descriptor data comprise data pertaining to resource processors which are micro grid processors that have been assigned to the artificial intelligence such that each resource processor is not a
Council of the plurality of Councils.
 17.  The method of claim 13, wherein the Kind and Descriptor data comprise data identifying a total number of mobile Councils in the Government, a total number of fixed Councils in the Government, a total number of mobile Parliaments in the
Government, and a total number of fixed Parliaments in the Government.
 18.  The method of claim 7, said governance apparatus further comprising: a plurality of micro grid apparatuses, each micro grid apparatus being either a simple micro grid apparatus or a complex micro grid apparatus, each complex micro grid
apparatus being a connectivity structure, each micro grid apparatus being wirelessly connected to another micro grid apparatus of the plurality of micro grid apparatuses, each micro grid apparatus comprising a unique governmental component of the
plurality of governmental components, each Executive consisting of a unique processor of a plurality of processors disposed in a unique simple micro grid apparatus of the plurality of micro grid apparatuses, each Parliament consisting of a unique
processor of each plurality of processors of at least two pluralities of processors, each processor of each plurality of processors of each micro grid apparatus having its own operating system, each unique processor in each Executive or Parliament in the
Government being a Council of the plurality of Councils and having a unique operating system differing from the operating system of each other processor in the plurality of processors that comprises said each unique processor.
 19.  A method of using a governance apparatus, said governance apparatus comprising a Government, said method comprising: forming the Government, said Government comprising a plurality of governmental components, said governmental components
collectively comprising a plurality of Councils such that a macro grid comprising an artificial intelligence and the Government is configured to respond to an alert pertaining to an event through use of the artificial intelligence and the Government,
each governmental component being an either an Executive or a Parliament;  forming an enhanced Transmission Control Protocol/Internet Protocol (TCP/IP) communication stack of layers comprising a Governance Layer and an Intelligence Layer;  and
processing, using intelligence software in the Intelligence Layer, data pertaining to the event, data pertaining to the alert, and data pertaining to the Government, wherein the Intelligence Layer comprises intelligence software configured to process
data pertaining to the event, data pertaining to the alert, and data pertaining to the Government, and wherein the Governance Layer comprises governance software configured to filter data in a TCP/IP packet header structure through data security and data
integrity algorithms, both to and from the intelligence software in the Intelligence Layer, to protect the artificial intelligence from attack;  and filtering, using governance software in the Governance Layer, data in a TCP/IP packet header structure
through data security and data integrity algorithms, both to and from the intelligence software in the Intelligence Layer, to protect the artificial intelligence from attack.
 20.  The method of claim 19, said governance apparatus further comprising a plurality of micro grid apparatuses, said method further comprising: forming the plurality of micro grid apparatuses, each micro grid apparatus being either a simple
micro grid apparatus or a complex micro grid apparatus, each complex micro grid apparatus being a connectivity structure, each micro grid apparatus being wirelessly connected to another micro grid apparatus of the plurality of micro grid apparatuses,
each micro grid apparatus comprising a unique governmental component of the plurality of governmental components, each Executive consisting of a unique processor of a plurality of processors disposed in a unique simple micro grid apparatus of the
plurality of micro grid apparatuses, each Parliament consisting of a unique processor of each plurality of processors of at least two pluralities of processors, each processor of each plurality of processors of each micro grid apparatus having its own
operating system, each unique processor in each Executive or Parliament in the Government being a Council of the plurality of Councils and having a unique operating system differing from the operating system of each other processor in the plurality of
processors that comprises said each unique processor.  "
"10,191,970","
     January 29, 2019
","Systems and methods for customized data parsing and paraphrasing
"," Methods and apparatus, including computer program products, implementing
     and using techniques for customized data parsing and paraphrasing. A
     communications module receives content from several resources. An
     analytics engine parses the content based on a user query for content. An
     artificial intelligence engine determines a confidence ranking for the
     parsed content and determines a set of prioritized parsed content from
     the parsed content, based on the confidence ranking for the parsed
     content. A natural language engine converts, using a natural language
     processing technique, the set of prioritized parsed content into a format
     for user interface. A user interface presents a summarized output
     including the converted set of prioritized parsed content based on
     information associated with the user query.
",G06F 16/355 (20190101); G06F 16/335 (20190101); G06N 20/00 (20190101); G06F 16/338 (20190101); G06F 16/3344 (20190101); G06F 16/345 (20190101),G06N 7/00 (20060101); G06N 99/00 (20100101),"[['\n5386556', '\nJanuary 1995'], ['\n7027975', '\nApril 2006'], ['\n8438472', '\nMay 2013'], ['\n8639650', '\nJanuary 2014'], ['\n8903755', '\nDecember 2014'], ['\n8954342', '\nFebruary 2015'], ['\n9666098', '\nMay 2017'], ['\n9830328', '\nNovember 2017'], ['\n9911001', '\nMarch 2018'], ['\n9916538', '\nMarch 2018'], ['\n2002/0083179', '\nJune 2002'], ['\n2003/0131063', '\nJuly 2003'], ['\n2003/0216929', '\nNovember 2003'], ['\n2003/0217052', '\nNovember 2003'], ['\n2006/0200432', '\nSeptember 2006'], ['\n2006/0200434', '\nSeptember 2006'], ['\n2006/0200435', '\nSeptember 2006'], ['\n2006/0259360', '\nNovember 2006'], ['\n2007/0016563', '\nJanuary 2007'], ['\n2007/0055564', '\nMarch 2007'], ['\n2007/0203872', '\nAugust 2007'], ['\n2008/0249967', '\nOctober 2008'], ['\n2008/0249968', '\nOctober 2008'], ['\n2009/0043824', '\nFebruary 2009'], ['\n2010/0070448', '\nMarch 2010'], ['\n2010/0180029', '\nJuly 2010'], ['\n2010/0257028', '\nOctober 2010'], ['\n2010/0299334', '\nNovember 2010'], ['\n2011/0035390', '\nFebruary 2011'], ['\n2011/0059423', '\nMarch 2011'], ['\n2011/0173225', '\nJuly 2011'], ['\n2011/0208822', '\nAugust 2011'], ['\n2011/0301941', '\nDecember 2011'], ['\n2012/0156667', '\nJune 2012'], ['\n2012/0195261', '\nAugust 2012'], ['\n2012/0254074', '\nOctober 2012'], ['\n2012/0254099', '\nOctober 2012'], ['\n2012/0290518', '\nNovember 2012'], ['\n2013/0086079', '\nApril 2013'], ['\n2013/0108994', '\nMay 2013'], ['\n2013/0185074', '\nJuly 2013'], ['\n2013/0204886', '\nAugust 2013'], ['\n2013/0290233', '\nOctober 2013'], ['\n2013/0290234', '\nOctober 2013'], ['\n2013/0317966', '\nNovember 2013'], ['\n2013/0330008', '\nDecember 2013'], ['\n2014/0058812', '\nFebruary 2014'], ['\n2014/0074629', '\nMarch 2014'], ['\n2014/0079297', '\nMarch 2014'], ['\n2014/0108156', '\nApril 2014'], ['\n2014/0143252', '\nMay 2014'], ['\n2014/0176603', '\nJune 2014'], ['\n2014/0201126', '\nJuly 2014'], ['\n2014/0310595', '\nOctober 2014'], ['\n2014/0358825', '\nDecember 2014'], ['\n2014/0358828', '\nDecember 2014'], ['\n2015/0095278', '\nApril 2015'], ['\n2015/0149455', '\nMay 2015'], ['\n2015/0199229', '\nJuly 2015'], ['\n2015/0302763', '\nOctober 2015'], ['\n2015/0317610', '\nNovember 2015'], ['\n2015/0339573', '\nNovember 2015'], ['\n2015/0363795', '\nDecember 2015'], ['\n2016/0042661', '\nFebruary 2016'], ['\n2016/0063871', '\nMarch 2016'], ['\n2016/0063881', '\nMarch 2016'], ['\n2016/0110316', '\nApril 2016'], ['\n2016/0112531', '\nApril 2016'], ['\n2016/0127010', '\nMay 2016'], ['\n2016/0132789', '\nMay 2016'], ['\n2016/0180248', '\nJune 2016'], ['\n2016/0188725', '\nJune 2016'], ['\n2016/0193500', '\nJuly 2016'], ['\n2016/0364444', '\nDecember 2016'], ['\n2017/0032298', '\nFebruary 2017'], ['\n2017/0041388', '\nFebruary 2017'], ['\n2017/0235786', '\nAugust 2017'], ['\n2017/0256179', '\nSeptember 2017'], ['\n2018/0046623', '\nFebruary 2018']]",[0]," The invention claimed is:  1.  A system comprising: a communications module configured to receive a plurality of content from a plurality of resources;  an analytics engine comprising at least
one processor and a memory configured to receive a user query associated with a user and parse the plurality of content received from the communications module based on at least one of an administration rule, a user profile corresponding to the user, and
historical data corresponding to the user;  an artificial intelligence engine comprising the at least one processor and the memory and configured to: determine a confidence ranking for each of the plurality of parsed content;  and select a set of
prioritized parsed content from the plurality of parsed content based on the confidence ranking for each of the plurality of parsed content;  a natural language engine comprising the at least one processor and the memory and configured to: convert, using
a natural language processing technique, the set of prioritized parsed content into a format suitable for a user interface, wherein the format is based on a writing style of the user and a skill level of the user;  identify a learning style of the user
based on information associated with the user query, wherein the learning style is selected from a group consisting of a visual learning style, an auditory learning style, a reading-writing learning style, and a tactile learning style;  filter the
converted set of prioritized parsed content based on the learning style of the user;  and combine the filtered set of prioritized parsed content into a summarized output;  and a user interface configured to present the summarized output to the user based
on the information associated with the user query.
 2.  The system of claim 1, wherein the plurality of resources comprises a content repository.
 3.  The system of claim 1, wherein the analytics engine is configured to parse the plurality of content based on an administrative rule comprising at least one of system level criteria and a preference setting of the user associated with the
user query.
 4.  The system of claim 1, wherein the analytics engine is configured to parse the plurality of content based on historical data comprising at least one of a browsing history, electronic mail messages, and prior search history results associated
with the user associated with the user query.
 5.  The system of claim 1, wherein the artificial intelligence engine is further configured to determine the confidence ranking for each of the plurality of parsed content by: for each of the plurality of parsed content: determine a type of
resource associated with the parsed content;  determine a popularity metric associated with the parsed content;  and assign the confidence ranking for the parsed content based on the determined type of resource and the determined popularity metric.
 6.  The system of claim 5, wherein the type of resource comprises at least one of an Internet resource type, technical document type, discussion forum type, social media type, and a multimedia type.
 7.  The system of claim 5, wherein the popularity metric comprises a number of click-throughs associated with the parsed content.
 8.  The system of claim 5, wherein the popularity metric comprises a number of recommendations associated with the parsed content.
 9.  The system of claim 5, wherein the popularity metric comprises a number of downloads associated with the parsed content.
 10.  The system of claim 5, wherein the popularity metric comprises a number of shares associated with the parsed content.
 11.  The system of claim 1, wherein the artificial intelligence engine is further configured to: compare the confidence ranking for each of the plurality of parsed content to each of the other plurality of parsed content;  sort the plurality of
parsed content based on the comparison of the confidence rankings for each of the plurality of parsed content;  and select the set of prioritized parsed content from the sorted plurality of parsed content, for determining the set of prioritized parsed
content from the plurality of parsed content.
 12.  The system of claim 11, wherein the artificial intelligence engine is further configured to sort the plurality of parsed content from at least one of the plurality of parsed content having a high confidence ranking to at least one other of
the plurality of parsed content having a low confidence ranking.
 13.  The system of claim 12, wherein the artificial intelligence engine is further configured to select the set of prioritized parsed content from the plurality of parsed content having a high confidence ranking.
 14.  The system of claim 1, wherein the natural language engine is further configured to filter the converted set of prioritized content based on the information associated with the user query.
 15.  The system of claim 14, wherein the information associated with the user query comprises at least one of a user profile, an administrative rule, and historical data of the user associated with the user query.
 16.  A method comprising: receiving a user query at a computing device comprising at least one processor and a memory;  receiving a plurality of content from a plurality of resources at a communications module;  parsing the plurality of content
using an analytics engine based on at least one of an administration rule, a user profile corresponding to a user, and historical data corresponding to the user;  parsing the plurality of content based on a user query associated with the user; 
determining a confidence ranking for each of the plurality of parsed content using an artificial intelligence engine;  selecting a set of prioritized parsed content from the plurality of parsed content based on the confidence ranking for each of the
plurality of parsed content;  converting, using a natural language processing technique, the set of prioritized parsed content into a format suitable for a user interface, wherein the format is based on at least a writing style of the user and a skill
level of the user;  identifying a learning style of the user based on information associated with the user query, wherein the learning style is selected from a group consisting of a visual learning style, an auditory learning style, a reading-writing
learning style, and a tactile learning style;  filtering the converted set of prioritized parsed content based on the learning style of the user;  combining the filtered set of prioritized parsed content into a summarized output;  and presenting the
summarized output to the user.  "
"10,192,163","
     January 29, 2019
","Audio processing method and apparatus based on artificial intelligence
"," The present disclosure discloses an audio processing method and apparatus
     based on artificial intelligence. A specific embodiment of the method
     comprises: converting a to-be-processed audio to a to-be-processed
     picture; extracting a content characteristic of the to-be-processed
     picture; determining a target picture based on a style characteristic and
     the content characteristic of the to-be-processed picture, the style
     characteristic being obtained from a template picture converted from a
     template audio; and converting the target picture to a processed audio.
     The present embodiment achieves the processing effect that the processed
     audio takes a template audio style, improves the efficiency and the
     flexibility of audio processing, while without changing the content of
     the to-be-processed audio.
",G06N 3/0454 (20130101); G06N 3/10 (20130101); H04N 1/00129 (20130101); G06F 16/50 (20190101); G06V 40/13 (20220101); H04N 1/00204 (20130101); G06N 3/08 (20130101); G06F 16/60 (20190101),G06N 3/10 (20060101); H04N 1/00 (20060101); G06K 9/00 (20060101),"[['\n2015/0213808', '\nJuly 2015'], ['\n2018/0144194', '\nMay 2018']]",[0]," What is claimed is:  1.  An audio processing method based on artificial intelligence, comprising: converting a to-be-processed audio to a to-be-processed picture;  extracting a content
characteristic of the to-be-processed picture;  determining a target picture based on a style characteristic and the content characteristic of the to-be-processed picture, the style characteristic being obtained from a template picture converted from a
template audio;  and converting the target picture to a processed audio, wherein the extracting a content characteristic of the to-be-processed picture comprises: inputting the to-be-processed picture into a pre-trained convolutional neural network, the
convolutional neural network being used for extracting an image characteristic;  and determining a matrix output by at least one convolutional layer in the convolutional neural network as the content characteristic of the to-be-processed picture.
 2.  The method according to claim 1, wherein the converting a to-be-processed audio to a to-be-processed picture comprises: dividing the to-be-processed audio into audio clips at a preset interval;  and determining an audiogram, a spectrum, or a
spectrogram of the audio clips as the to-be-processed picture.
 3.  The method according to claim 1, wherein the style characteristic is determined through the following: inputting the template picture into a pre-trained convolutional neural network, the convolutional neural network being used for extracting
an image characteristic;  and determining a matrix output by at least one convolutional layer in the convolutional neural network as the style characteristic of the template picture.
 4.  The method according to claim 1, wherein the determining a target picture based on a style characteristic and the content characteristic of the to-be-processed picture comprises: importing the content characteristic of the to-be-processed
picture to a preset style transfer model, and acquiring an output of the style transfer model as the target picture.
 5.  The method according to claim 1, wherein the determining a target picture based on a style characteristic and the content characteristic of the to-be-processed picture comprises: extracting a content characteristic and a style characteristic
of an initial target picture;  determining a content loss function based on the content characteristic of the to-be-processed picture and the content characteristic of the initial target picture;  determining a style loss function based on the style
characteristic of the template picture and the style characteristic of the initial target picture;  determining a total loss function based on the content loss function and the style loss function;  and obtaining the target picture by adjusting the
initial target picture based on the total loss function.
 6.  The method according to claim 5, wherein the content loss function is obtained based on a mean square error of the content characteristic of the to-be-processed picture and the content characteristic of the initial target picture.
 7.  The method according to claim 5, wherein the style loss function is determined according to the following steps: determining a Gram matrix of the template picture and a Gram matrix of the initial target picture respectively, based on the
style characteristic of the template picture and the style characteristic of the initial target picture;  and determining the style loss function based on a mean square error of the Gram matrix of the template picture and the Gram matrix of the initial
target picture.
 8.  The method according to claim 5, wherein the total loss function is obtained based on a weighted sum of the content loss function and the style loss function.
 9.  The method according to claim 5, wherein the obtaining the target picture by adjusting the initial target picture based on the total loss function further comprises: obtaining a minimum value of the total loss function by adjusting the
initial target picture based on a gradient descent method and the total loss function;  and determining the adjusted picture corresponding to the minimum value of the total loss function as the target picture.
 10.  An audio processing apparatus based on artificial intelligence, comprising: at least one processor;  and a memory storing instructions, which when executed by the at least one processor, cause the at least one processor to perform
operations, the operations comprising: converting a to-be-processed audio to a to-be-processed picture;  extracting a content characteristic of the to-be-processed picture;  determining a target picture based on a style characteristic and the content
characteristic of the to-be-processed picture, the style characteristic being obtained from a template picture converted from a template audio;  and converting the target picture to a processed audio, wherein the extracting a content characteristic of
the to-be-processed picture comprises: inputting the to-be-processed picture into a retrained convolutional neural network, the convolutional neural network being used for extracting an image characteristic;  and determining a matrix output by at least
one convolutional layer in the convolutional neural network as the content characteristic of the to-be-processed picture.
 11.  The apparatus according to claim 10, wherein the converting a to-be-processed audio to a to-be-processed picture comprises: dividing the to-be-processed audio into audio clips at a preset interval;  and determining an audiogram, a spectrum,
or a spectrogram of the audio clips as the to-be-processed picture.
 12.  The apparatus according to claim 10, wherein the style characteristic is determined through the following: inputting the template picture into a pre-trained convolutional neural network, the convolutional neural network being used for
extracting an image characteristic;  and determining a matrix output by at least one convolutional layer in the convolutional neural network as the style characteristic of the template picture.
 13.  The apparatus according to claim 10, wherein the determining a target picture based on a style characteristic and the content characteristic of the to-be-processed picture comprises: importing the content characteristic of the
to-be-processed picture to a preset style transfer model, and acquiring an output of the style transfer model as the target picture.
 14.  The apparatus according to claim 10, wherein the determining a target picture based on a style characteristic and the content characteristic of the to-be-processed picture comprises: extracting a content characteristic and a style
characteristic of an initial target picture;  determining a content loss function, based on the content characteristic of the to-be-processed picture and the content characteristic of the initial target picture;  determining a style loss function based
on the style characteristic of the template picture and the style characteristic of the initial target picture;  determining a total loss function based on the content loss function and the style loss function;  and obtaining the target picture by
adjusting the initial target picture based on the total loss function.
 15.  The apparatus according to claim 14, wherein the content loss function is obtained based on a mean square error of the content characteristic of the to-be-processed picture and the content characteristic of the initial target picture.
 16.  The apparatus according to claim 14, wherein the style loss function is determined according to the following steps: determining a Gram matrix of the template picture and a Gram matrix of the initial target picture respectively, based on
the style characteristic of the template picture and the style characteristic of the initial target picture;  and determining the style loss function based on a mean square error of the Gram matrix of the template picture and the Gram matrix of the
initial target picture.
 17.  The apparatus according to claim 14, wherein the total loss function is obtained based on a weighted sum of the content loss function and the style loss function.
 18.  The apparatus according to claim 14, wherein the obtaining the target picture by adjusting the initial target picture based on the total loss function further comprises: obtaining a minimum value of the total loss function by adjusting the
initial target picture based on a gradient descent method and the total loss function;  and determining the adjusted picture corresponding to the minimum value of the total loss function as the target picture.
 19.  A non-transitory computer storage medium storing a computer program, which when executed by one or more processors, cause the one or more processors to perform operations, the operations comprising: converting a to-be-processed audio to a
to-be-processed picture;  extracting a content characteristic of the to-be-processed picture;  determining a target picture based on a style characteristic and the content characteristic of the to-be-processed picture, the style characteristic being
obtained from a template picture converted from a template audio;  and converting the target picture to a processed audio, wherein the extracting a content characteristic of the to-be-processed picture comprises;  inputting the to-be-processed picture
into a pre-trained convolutional neural network, the convolutional neural network being used for extracting an image characteristic;  and determining a matrix output by at least one convolutional layer in the convolutional neural network as the content
characteristic of the to-be-processed picture.  "
"10,192,392","
     January 29, 2019
","Method for configuring casino operations
"," Embodiments of the present invention are directed to a method of
     optimizing at least one performance variable, such as revenue or
     profitability, indicative of the performance of a casino. An initial set
     of operating parameters is set. At least one environmental variable, such
     as traffic, weather, time, cost of transportation, etc. are monitored as
     is the performance variable. An artificial intelligence program changes
     operating parameters, such as player-tracking rewards, bonuses, comps,
     advertising, etc. The program determines the effect of the change, and
     further changes the operating parameter as a function of the effect and
     the environmental variable.
",G07F 17/32 (20130101); G07F 17/3232 (20130101); G07F 17/3223 (20130101); G07F 17/3213 (20130101); G07F 17/3255 (20130101); G07F 17/3239 (20130101); G07F 17/34 (20130101); G07F 17/3234 (20130101),A63F 9/24 (20060101); G07F 17/32 (20060101); G07F 17/34 (20060101),"[['\n2669389', '\nFebruary 1954'], ['\n3124355', '\nMarch 1964'], ['\n3124674', '\nMarch 1964'], ['\n3684290', '\nAugust 1972'], ['\n3727213', '\nApril 1973'], ['\n3751040', '\nAugust 1973'], ['\n4240635', '\nDecember 1980'], ['\n4254404', '\nMarch 1981'], ['\n4433844', '\nFebruary 1984'], ['\n4624459', '\nNovember 1986'], ['\n4657256', '\nApril 1987'], ['\n4836546', '\nJune 1989'], ['\n4887813', '\nDecember 1989'], ['\n5022653', '\nJune 1991'], ['\n5024439', '\nJune 1991'], ['\n5027102', '\nJune 1991'], ['\n5031914', '\nJuly 1991'], ['\n5078405', '\nJanuary 1992'], ['\n5152529', '\nOctober 1992'], ['\n5178395', '\nJanuary 1993'], ['\n5221083', '\nJune 1993'], ['\n5265880', '\nNovember 1993'], ['\n5342049', '\nAugust 1994'], ['\n5364104', '\nNovember 1994'], ['\n5377973', '\nJanuary 1995'], ['\n5380008', '\nJanuary 1995'], ['\n5490670', '\nFebruary 1996'], ['\n5536016', '\nJuly 1996'], ['\n5564700', '\nOctober 1996'], ['\n5584485', '\nDecember 1996'], ['\n5586766', '\nDecember 1996'], ['\n5655961', '\nAugust 1997'], ['\n5674128', '\nOctober 1997'], ['\n5695402', '\nDecember 1997'], ['\n5697844', '\nDecember 1997'], ['\n5743798', '\nApril 1998'], ['\n5758875', '\nJune 1998'], ['\n5766076', '\nJune 1998'], ['\n5816918', '\nOctober 1998'], ['\n5830064', '\nNovember 1998'], ['\n5836816', '\nNovember 1998'], ['\n5836817', '\nNovember 1998'], ['\n5851147', '\nDecember 1998'], ['\n5910048', '\nJune 1999'], ['\n5913726', '\nJune 1999'], ['\n5934998', '\nAugust 1999'], ['\n5941770', '\nAugust 1999'], ['\n5960406', '\nSeptember 1999'], ['\n5984779', '\nNovember 1999'], ['\n6003013', '\nDecember 1999'], ['\n6012983', '\nJanuary 2000'], ['\n6024642', '\nFebruary 2000'], ['\n6030109', '\nFebruary 2000'], ['\n6032955', '\nMarch 2000'], ['\n6045130', '\nApril 2000'], ['\n6048272', '\nApril 2000'], ['\n6059659', '\nMay 2000'], ['\n6077163', '\nJune 2000'], ['\n6086477', '\nJuly 2000'], ['\n6106395', '\nAugust 2000'], ['\n6110041', '\nAugust 2000'], ['\n6110043', '\nAugust 2000'], ['\n6135884', '\nOctober 2000'], ['\n6146273', '\nNovember 2000'], ['\n6165071', '\nDecember 2000'], ['\n6168521', '\nJanuary 2001'], ['\n6183362', '\nFebruary 2001'], ['\n6186892', '\nFebruary 2001'], ['\n6186893', '\nFebruary 2001'], ['\n6196918', '\nMarch 2001'], ['\n6210276', '\nApril 2001'], ['\n6217448', '\nApril 2001'], ['\n6224482', '\nMay 2001'], ['\n6234900', '\nMay 2001'], ['\n6254483', '\nJuly 2001'], ['\n6264560', '\nJuly 2001'], ['\n6270409', '\nAugust 2001'], ['\n6289382', '\nSeptember 2001'], ['\n6293866', '\nSeptember 2001'], ['\n6293868', '\nSeptember 2001'], ['\n6302793', '\nOctober 2001'], ['\n6315662', '\nNovember 2001'], ['\n6315666', '\nNovember 2001'], ['\n6319122', '\nNovember 2001'], ['\n6319125', '\nNovember 2001'], ['\n6336859', '\nJanuary 2002'], ['\n6347996', '\nFebruary 2002'], ['\n6364314', '\nApril 2002'], ['\n6368216', '\nApril 2002'], ['\n6371852', '\nApril 2002'], ['\n6375567', '\nApril 2002'], ['\n6425823', '\nJuly 2002'], ['\n6428002', '\nAugust 2002'], ['\n6443456', '\nSeptember 2002'], ['\n6454648', '\nSeptember 2002'], ['\n6457045', '\nSeptember 2002'], ['\n6471588', '\nOctober 2002'], ['\n6485367', '\nNovember 2002'], ['\n6485368', '\nNovember 2002'], ['\n6520856', '\nFebruary 2003'], ['\n6565434', '\nMay 2003'], ['\n6565436', '\nMay 2003'], ['\n6569013', '\nMay 2003'], ['\n6575832', '\nJune 2003'], ['\n6592457', '\nJuly 2003'], ['\n6599186', '\nJuly 2003'], ['\n6599193', '\nJuly 2003'], ['\n6606615', '\nAugust 2003'], ['\n6620046', '\nSeptember 2003'], ['\n6634922', '\nOctober 2003'], ['\n6648757', '\nNovember 2003'], ['\n6652378', '\nNovember 2003'], ['\n6656047', '\nDecember 2003'], ['\n6695700', '\nFebruary 2004'], ['\n6697165', '\nFebruary 2004'], ['\n6702670', '\nMarch 2004'], ['\n6709331', '\nMarch 2004'], ['\n6712693', '\nMarch 2004'], ['\n6712695', '\nMarch 2004'], ['\n6722985', '\nApril 2004'], ['\n6749510', '\nJune 2004'], ['\n6751657', '\nJune 2004'], ['\n6755420', '\nJune 2004'], ['\n6758754', '\nJuly 2004'], ['\n6760595', '\nJuly 2004'], ['\n6780104', '\nAugust 2004'], ['\n6786824', '\nSeptember 2004'], ['\n6800026', '\nOctober 2004'], ['\n6800027', '\nOctober 2004'], ['\n6802778', '\nOctober 2004'], ['\n6811482', '\nNovember 2004'], ['\n6811486', '\nNovember 2004'], ['\n6860808', '\nMarch 2005'], ['\n6860810', '\nMarch 2005'], ['\n6939227', '\nSeptember 2005'], ['\n6944509', '\nSeptember 2005'], ['\n6948171', '\nSeptember 2005'], ['\n6965868', '\nNovember 2005'], ['\n6973665', '\nDecember 2005'], ['\nRE38982', '\nFebruary 2006'], ['\n6997380', '\nFebruary 2006'], ['\n6998806', '\nFebruary 2006'], ['\n7037195', '\nMay 2006'], ['\n7056210', '\nJune 2006'], ['\n7069232', '\nJune 2006'], ['\n7090579', '\nAugust 2006'], ['\n7094149', '\nAugust 2006'], ['\n7094150', '\nAugust 2006'], ['\n7103560', '\nSeptember 2006'], ['\n7131908', '\nNovember 2006'], ['\n7144322', '\nDecember 2006'], ['\n7169052', '\nJanuary 2007'], ['\n7175521', '\nFebruary 2007'], ['\n7182690', '\nFebruary 2007'], ['\n7184965', '\nFebruary 2007'], ['\n7186181', '\nMarch 2007'], ['\n7192346', '\nMarch 2007'], ['\n7195243', '\nMarch 2007'], ['\n7201654', '\nApril 2007'], ['\n7251805', '\nJuly 2007'], ['\n7300351', '\nNovember 2007'], ['\n7329185', '\nFebruary 2008'], ['\n7338372', '\nMarch 2008'], ['\n7361089', '\nApril 2008'], ['\n7374486', '\nMay 2008'], ['\n7410422', '\nAugust 2008'], ['\n7416186', '\nAugust 2008'], ['\n7458892', '\nDecember 2008'], ['\n7594851', '\nSeptember 2009'], ['\n7601060', '\nOctober 2009'], ['\n7628691', '\nDecember 2009'], ['\n7674180', '\nMarch 2010'], ['\n7717788', '\nMay 2010'], ['\n7765121', '\nJuly 2010'], ['\n7775876', '\nAugust 2010'], ['\n7780520', '\nAugust 2010'], ['\n7811167', '\nOctober 2010'], ['\n7846018', '\nDecember 2010'], ['\n7874911', '\nJanuary 2011'], ['\n7963844', '\nJune 2011'], ['\n8052517', '\nNovember 2011'], ['\n2001/0004609', '\nJune 2001'], ['\n2001/0024015', '\nSeptember 2001'], ['\n2001/0046893', '\nNovember 2001'], ['\n2001/0048193', '\nDecember 2001'], ['\n2002/0013173', '\nJanuary 2002'], ['\n2002/0016202', '\nFebruary 2002'], ['\n2002/0019253', '\nFebruary 2002'], ['\n2002/0032052', '\nMarch 2002'], ['\n2002/0034981', '\nMarch 2002'], ['\n2002/0039923', '\nApril 2002'], ['\n2002/0055381', '\nMay 2002'], ['\n2002/0086726', '\nJuly 2002'], ['\n2002/0090987', '\nJuly 2002'], ['\n2002/0094855', '\nJuly 2002'], ['\n2002/0103018', '\nAugust 2002'], ['\n2002/0107072', '\nAugust 2002'], ['\n2002/0123376', '\nSeptember 2002'], ['\n2002/0132664', '\nSeptember 2002'], ['\n2002/0142825', '\nOctober 2002'], ['\n2002/0143652', '\nOctober 2002'], ['\n2002/0147040', '\nOctober 2002'], ['\n2002/0147043', '\nOctober 2002'], ['\n2002/0152120', '\nOctober 2002'], ['\n2002/0167126', '\nNovember 2002'], ['\n2002/0177480', '\nNovember 2002'], ['\n2002/0177483', '\nNovember 2002'], ['\n2002/0187834', '\nDecember 2002'], ['\n2002/0193162', '\nDecember 2002'], ['\n2003/0003989', '\nJanuary 2003'], ['\n2003/0013512', '\nJanuary 2003'], ['\n2003/0017865', '\nJanuary 2003'], ['\n2003/0032474', '\nFebruary 2003'], ['\n2003/0036425', '\nFebruary 2003'], ['\n2003/0054878', '\nMarch 2003'], ['\n2003/0054881', '\nMarch 2003'], ['\n2003/0060276', '\nMarch 2003'], ['\n2003/0064769', '\nApril 2003'], ['\n2003/0064771', '\nApril 2003'], ['\n2003/0067116', '\nApril 2003'], ['\n2003/0078101', '\nApril 2003'], ['\n2003/0083943', '\nMay 2003'], ['\n2003/0087685', '\nMay 2003'], ['\n2003/0092484', '\nMay 2003'], ['\n2003/0100360', '\nMay 2003'], ['\n2003/0114217', '\nJune 2003'], ['\n2003/0119575', '\nJune 2003'], ['\n2003/0135304', '\nJuly 2003'], ['\n2003/0144048', '\nJuly 2003'], ['\n2003/0157978', '\nAugust 2003'], ['\n2003/0178774', '\nSeptember 2003'], ['\n2003/0186733', '\nOctober 2003'], ['\n2003/0187736', '\nOctober 2003'], ['\n2003/0190944', '\nOctober 2003'], ['\n2003/0195029', '\nOctober 2003'], ['\n2003/0199295', '\nOctober 2003'], ['\n2003/0199312', '\nOctober 2003'], ['\n2003/0204474', '\nOctober 2003'], ['\n2003/0207711', '\nNovember 2003'], ['\n2003/0209853', '\nNovember 2003'], ['\n2003/0211884', '\nNovember 2003'], ['\n2003/0216169', '\nNovember 2003'], ['\n2003/0220138', '\nNovember 2003'], ['\n2003/0220139', '\nNovember 2003'], ['\n2003/0220143', '\nNovember 2003'], ['\n2003/0228901', '\nDecember 2003'], ['\n2003/0232640', '\nDecember 2003'], ['\n2003/0234489', '\nDecember 2003'], ['\n2003/0236110', '\nDecember 2003'], ['\n2004/0009808', '\nJanuary 2004'], ['\n2004/0038735', '\nFebruary 2004'], ['\n2004/0038736', '\nFebruary 2004'], ['\n2004/0048650', '\nMarch 2004'], ['\n2004/0053657', '\nMarch 2004'], ['\n2004/0053681', '\nMarch 2004'], ['\n2004/0063484', '\nApril 2004'], ['\n2004/0072609', '\nApril 2004'], ['\n2004/0092315', '\nMay 2004'], ['\n2004/0103013', '\nMay 2004'], ['\n2004/0106449', '\nJune 2004'], ['\n2004/0121833', '\nJune 2004'], ['\n2004/0142742', '\nJuly 2004'], ['\n2004/0158536', '\nAugust 2004'], ['\n2004/0166940', '\nAugust 2004'], ['\n2004/0180722', '\nSeptember 2004'], ['\n2004/0198485', '\nOctober 2004'], ['\n2004/0203611', '\nOctober 2004'], ['\n2004/0204213', '\nOctober 2004'], ['\n2004/0204216', '\nOctober 2004'], ['\n2004/0204222', '\nOctober 2004'], ['\n2004/0214637', '\nOctober 2004'], ['\n2004/0219967', '\nNovember 2004'], ['\n2004/0224750', '\nNovember 2004'], ['\n2004/0229671', '\nNovember 2004'], ['\n2004/0229683', '\nNovember 2004'], ['\n2004/0229700', '\nNovember 2004'], ['\n2004/0235542', '\nNovember 2004'], ['\n2004/0248642', '\nDecember 2004'], ['\n2004/0254010', '\nDecember 2004'], ['\n2004/0266517', '\nDecember 2004'], ['\n2005/0014558', '\nJanuary 2005'], ['\n2005/0026674', '\nFebruary 2005'], ['\n2005/0043072', '\nFebruary 2005'], ['\n2005/0043088', '\nFebruary 2005'], ['\n2005/0043092', '\nFebruary 2005'], ['\n2005/0043094', '\nFebruary 2005'], ['\n2005/0049028', '\nMarch 2005'], ['\n2005/0054438', '\nMarch 2005'], ['\n2005/0059467', '\nMarch 2005'], ['\n2005/0070356', '\nMarch 2005'], ['\n2005/0075164', '\nApril 2005'], ['\n2005/0096121', '\nMay 2005'], ['\n2005/0096124', '\nMay 2005'], ['\n2005/0101375', '\nMay 2005'], ['\n2005/0101379', '\nMay 2005'], ['\n2005/0119052', '\nJune 2005'], ['\n2005/0124411', '\nJune 2005'], ['\n2005/0124415', '\nJune 2005'], ['\n2005/0148380', '\nJuly 2005'], ['\n2005/0148383', '\nJuly 2005'], ['\n2005/0153773', '\nJuly 2005'], ['\n2005/0164764', '\nJuly 2005'], ['\n2005/0181856', '\nAugust 2005'], ['\n2005/0181860', '\nAugust 2005'], ['\n2005/0181862', '\nAugust 2005'], ['\n2005/0187014', '\nAugust 2005'], ['\n2005/0208995', '\nSeptember 2005'], ['\n2005/0215311', '\nSeptember 2005'], ['\n2005/0215314', '\nSeptember 2005'], ['\n2005/0215316', '\nSeptember 2005'], ['\n2005/0233794', '\nOctober 2005'], ['\n2005/0239541', '\nOctober 2005'], ['\n2005/0239545', '\nOctober 2005'], ['\n2005/0251440', '\nNovember 2005'], ['\n2005/0255902', '\nNovember 2005'], ['\n2005/0266905', '\nDecember 2005'], ['\n2006/0009284', '\nJanuary 2006'], ['\n2006/0025205', '\nFebruary 2006'], ['\n2006/0025207', '\nFebruary 2006'], ['\n2006/0025210', '\nFebruary 2006'], ['\n2006/0030400', '\nFebruary 2006'], ['\n2006/0040723', '\nFebruary 2006'], ['\n2006/0040730', '\nFebruary 2006'], ['\n2006/0046830', '\nMarch 2006'], ['\n2006/0046835', '\nMarch 2006'], ['\n2006/0052160', '\nMarch 2006'], ['\n2006/0058095', '\nMarch 2006'], ['\n2006/0058097', '\nMarch 2006'], ['\n2006/0068898', '\nMarch 2006'], ['\n2006/0068899', '\nMarch 2006'], ['\n2006/0068903', '\nMarch 2006'], ['\n2006/0073872', '\nApril 2006'], ['\n2006/0073887', '\nApril 2006'], ['\n2006/0079310', '\nApril 2006'], ['\n2006/0079314', '\nApril 2006'], ['\n2006/0084496', '\nApril 2006'], ['\n2006/0094493', '\nMay 2006'], ['\n2006/0100009', '\nMay 2006'], ['\n2006/0105836', '\nMay 2006'], ['\n2006/0116201', '\nJune 2006'], ['\n2006/0121972', '\nJune 2006'], ['\n2006/0128467', '\nJune 2006'], ['\n2006/0135249', '\nJune 2006'], ['\n2006/0148559', '\nJuly 2006'], ['\n2006/0149632', '\nJuly 2006'], ['\n2006/0154714', '\nJuly 2006'], ['\n2006/0174270', '\nAugust 2006'], ['\n2006/0183530', '\nAugust 2006'], ['\n2006/0183536', '\nAugust 2006'], ['\n2006/0199631', '\nSeptember 2006'], ['\n2006/0211486', '\nSeptember 2006'], ['\n2006/0217175', '\nSeptember 2006'], ['\n2006/0229127', '\nOctober 2006'], ['\n2006/0247034', '\nNovember 2006'], ['\n2006/0247041', '\nNovember 2006'], ['\n2006/0252510', '\nNovember 2006'], ['\n2006/0252512', '\nNovember 2006'], ['\n2006/0258422', '\nNovember 2006'], ['\n2006/0258425', '\nNovember 2006'], ['\n2006/0258432', '\nNovember 2006'], ['\n2006/0287034', '\nDecember 2006'], ['\n2006/0287045', '\nDecember 2006'], ['\n2006/0287098', '\nDecember 2006'], ['\n2006/0287102', '\nDecember 2006'], ['\n2007/0001396', '\nJanuary 2007'], ['\n2007/0010309', '\nJanuary 2007'], ['\n2007/0010315', '\nJanuary 2007'], ['\n2007/0050256', '\nMarch 2007'], ['\n2007/0060252', '\nMarch 2007'], ['\n2007/0060274', '\nMarch 2007'], ['\n2007/0060323', '\nMarch 2007'], ['\n2007/0060387', '\nMarch 2007'], ['\n2007/0105615', '\nMay 2007'], ['\n2007/0105618', '\nMay 2007'], ['\n2007/0106553', '\nMay 2007'], ['\n2007/0111776', '\nMay 2007'], ['\n2007/0112609', '\nMay 2007'], ['\n2007/0117619', '\nMay 2007'], ['\n2007/0117623', '\nMay 2007'], ['\n2007/0129147', '\nJune 2007'], ['\n2007/0135214', '\nJune 2007'], ['\n2007/0143156', '\nJune 2007'], ['\n2007/0167210', '\nJuly 2007'], ['\n2007/0191087', '\nAugust 2007'], ['\n2007/0197247', '\nAugust 2007'], ['\n2007/0205556', '\nSeptember 2007'], ['\n2007/0259709', '\nNovember 2007'], ['\n2007/0275777', '\nNovember 2007'], ['\n2008/0015004', '\nJanuary 2008'], ['\n2008/0039190', '\nFebruary 2008'], ['\n2008/0058105', '\nMarch 2008'], ['\n2008/0064495', '\nMarch 2008'], ['\n2008/0076576', '\nMarch 2008'], ['\n2008/0090651', '\nApril 2008'], ['\n2008/0096639', '\nApril 2008'], ['\n2008/0102921', '\nMay 2008'], ['\n2008/0102935', '\nMay 2008'], ['\n2008/0113749', '\nMay 2008'], ['\n2008/0113773', '\nMay 2008'], ['\n2008/0113779', '\nMay 2008'], ['\n2008/0113811', '\nMay 2008'], ['\n2008/0132320', '\nJune 2008'], ['\n2008/0146331', '\nJune 2008'], ['\n2008/0153564', '\nJune 2008'], ['\n2008/0171586', '\nJuly 2008'], ['\n2008/0176647', '\nJuly 2008'], ['\n2008/0182655', '\nJuly 2008'], ['\n2008/0207313', '\nAugust 2008'], ['\n2008/0220861', '\nSeptember 2008'], ['\n2008/0234035', '\nSeptember 2008'], ['\n2008/0242394', '\nOctober 2008'], ['\n2008/0242398', '\nOctober 2008'], ['\n2008/0248851', '\nOctober 2008'], ['\n2008/0254886', '\nOctober 2008'], ['\n2008/0261699', '\nOctober 2008'], ['\n2008/0268959', '\nOctober 2008'], ['\n2008/0280674', '\nNovember 2008'], ['\n2008/0287186', '\nNovember 2008'], ['\n2008/0293467', '\nNovember 2008'], ['\n2008/0318656', '\nDecember 2008'], ['\n2009/0005170', '\nJanuary 2009'], ['\n2009/0036202', '\nFebruary 2009'], ['\n2009/0070081', '\nMarch 2009'], ['\n2009/0075728', '\nMarch 2009'], ['\n2009/0088239', '\nApril 2009'], ['\n2009/0117981', '\nMay 2009'], ['\n2009/0124327', '\nMay 2009'], ['\n2009/0124364', '\nMay 2009'], ['\n2009/0131175', '\nMay 2009'], ['\n2009/0170608', '\nJuly 2009'], ['\n2009/0176580', '\nJuly 2009'], ['\n2009/0233682', '\nSeptember 2009'], ['\n2009/0239601', '\nSeptember 2009'], ['\n2009/0239622', '\nSeptember 2009'], ['\n2009/0239628', '\nSeptember 2009'], ['\n2009/0247284', '\nOctober 2009'], ['\n2009/0253477', '\nOctober 2009'], ['\n2009/0253478', '\nOctober 2009'], ['\n2009/0253490', '\nOctober 2009'], ['\n2009/0270168', '\nOctober 2009'], ['\n2009/0286590', '\nNovember 2009'], ['\n2009/0299833', '\nDecember 2009'], ['\n2009/0325669', '\nDecember 2009'], ['\n2009/0325670', '\nDecember 2009'], ['\n2010/0016055', '\nJanuary 2010'], ['\n2010/0041464', '\nFebruary 2010'], ['\n2010/0048286', '\nFebruary 2010'], ['\n2010/0056248', '\nMarch 2010'], ['\n2010/0075741', '\nMarch 2010'], ['\n2010/0105454', '\nApril 2010'], ['\n2010/0105466', '\nApril 2010'], ['\n2010/0113130', '\nMay 2010'], ['\n2010/0124981', '\nMay 2010'], ['\n2010/0285867', '\nNovember 2010'], ['\n2010/0304834', '\nDecember 2010'], ['\n2011/0039615', '\nFebruary 2011'], ['\n2011/0081958', '\nApril 2011'], ['\n2011/0117987', '\nMay 2011'], ['\n2011/0165938', '\nJuly 2011'], ['\n2011/0218030', '\nSeptember 2011'], ['\n2011/0275438', '\nNovember 2011'], ['\n2011/0281632', '\nNovember 2011'], ['\n2011/0287826', '\nNovember 2011'], ['\n2011/0294563', '\nDecember 2011'], ['\n2012/0077565', '\nMarch 2012'], ['\n2012/0115566', '\nMay 2012'], ['\n2012/0190425', '\nJuly 2012']]",[0]," The invention claimed is:  1.  A method of automating at least some aspects of casino management in a casino having a plurality of gaming machines connected to a communication network, the method
comprising: generating a plurality of promotional codes that each entitle a recipient to a benefit of at least one of discounted wagers on at least some of the gaming machines and eligibility to participate in at least one promotion;  publishing
different promotional codes in a plurality of advertising venues, including transmitting the at least one code to a mobile computing device accessible by at least some of the recipients;  using a processor to store at least some of the codes in a memory
operatively connected to the communication network;  receiving one of the codes from each of at least some of the recipients via a player interface at a corresponding gaming machine selected for play by the recipient;  receiving value from each of the at
least some recipients for wagering on a respective one of each of at least some of the gaming devices via an accepting device configured to engage a physical item associated with a monetary value, the monetary establishing a credit balance that decreases
based at least on wagering activity;  tracking at least the amount wagered at each gaming machine using a meter associated with each gaming machine;  comparing the received code with the stored codes;  providing the benefit to the recipient;  tracking
the received codes;  tracking the wagers made by players using each code;  comparing the value of wagers made using each code with the cost of publishing the code;  and automatically changing at least one advertising venue where at least one of the codes
is published based on the comparison.
 2.  The method of claim 1 wherein the mobile computing device comprises a cellular telephone.
 3.  The method of claim 2 wherein transmitting the code to a mobile computing device comprises transmitting the code in an email.
 4.  The method of claim 2 wherein transmitting the code to a mobile computing device comprises transmitting the code in a text message.
 5.  The method of claim 2 wherein transmitting the code to a mobile computing device comprises transmitting via the Internet.
 6.  The method of claim 1 wherein the promotion comprises an incentive.
 7.  The method of claim 1 wherein the promotion comprises a bonus.
 8.  The method of claim 7 wherein the bonus comprises an increased award on at least one gaming machine.
 9.  The method of claim 1 wherein the promotion comprises a complementary amenity.
 10.  The method of claim 1 wherein the promotion comprises a give-away.
 11.  A gaming system comprising: a plurality of electronic gaming machines, each of which includes: a non-transitory memory device configured to store computer-readable instructions to facilitate play of the electronic gaming machine, an
accepting device configured to engage a physical item associated with a monetary value, the monetary value establishing a credit balance that is decreasable based on at least wagering activity, and at least one meter to track the amount of play on the
gaming machine;  a network operatively connecting the gaming machines on which data from the at least one meter is transmitted to a database;  a plurality of promotional codes that each entitle a recipient to a benefit of at least one of discounted
wagers on at least some of the gaming machines and eligibility to participate in at least one promotion, the codes having been published in a plurality of advertising venues;  at least one computing processor connected to the network, the at least one
processor configured to: receive one of the codes from each of at least some of the recipients via at least one of a mobile computing device and a player interface associated with a corresponding gaming machine selected for play by the recipient;  store
the at least some of the codes in a memory operatively connected to the communication network;  track at least the amount wagered at each corresponding gaming machine via the meter;  compare the received codes with the stored codes;  provide the benefit
to the recipient;  track the received codes;  track the wagers made by players using each code;  compare the value of wagers made using each code with the cost of publishing the code;  and change the advertising venue where at least some of the codes are
published based on the comparison.
 12.  The gaming system of claim 11 wherein the promotion comprises an incentive.
 13.  The gaming system of claim 11 wherein the promotion comprises a bonus.
 14.  The gaming system of claim 13 wherein the bonus comprises an increased award on at least one gaming machine.
 15.  The gaming system of claim 11 wherein the promotion comprises a complementary amenity.
 16.  The gaming system of claim 11 wherein the promotion comprises a give-away.
 17.  At least one non-transitory computer readable medium that stores a plurality of instructions, which when executed by at least one processor causes the at least one processor to: store a plurality of different codes in a memory operatively
connected to a network of electronic gaming machines, the codes having been published in a plurality of advertising venues;  receive one of the codes from each of at least some players of the gaming machines via a player interface at a corresponding
gaming machine selected for play by the player;  track at least the amount wagered at each gaming machine via the meter;  compare the received code with the stored codes;  provide a benefit to a player from which a code has been received, the benefit
comprising at least one of discounted wagers on at least some of the gaming machines and eligibility to participate in at least one promotion;  track the received codes;  track the wagers made by players using each code;  compare the value of wagers made
using each code with the cost of publishing the code;  and change the advertising venue where at least one of the codes is publicized based on the comparison.
 18.  The method at least one non-transitory computer readable medium of claim 17 wherein the promotion comprises an incentive.
 19.  The method at least one non-transitory computer readable medium of claim 17 wherein the promotion comprises a bonus.
 20.  The method at least one non-transitory computer readable medium of claim 17 wherein the bonus comprises an increased award on at least one gaming machine.
 21.  The method at least one non-transitory computer readable medium of claim 17 wherein the promotion comprises a complementary amenity.
 22.  The method at least one non-transitory computer readable medium of claim 17 wherein the promotion comprises a give-away.  "
"10,193,999","
     January 29, 2019
","Dynamic online game implementation on a client device
"," A system and method for providing online games on client devices based on
     capabilities of the client devices are disclosed. Device capability
     information may be obtained from the client devices during the online
     game. Values of one or more capability parameters may be determined based
     on the device capability information obtained. Device profiles may be
     assigned to the client devices based on the determined values of the
     capability parameters. One or more game features may be adjusted in
     accordance with the assigned device profile for implementing the online
     game on the client devices. The adjustable game features may include
     frame rate, level of detail of rendering, level of 3D graphics, level of
     collision effects, animation type, gameplay available for user
     interaction, gameplay content for user interaction, communication mode,
     physics effects available for display, level of artificial intelligence,
     and/or any other features of the online game.
",A63F 13/358 (20140902); H04L 67/131 (20220501); H04L 67/303 (20130101); A63F 13/77 (20140902); A63F 13/355 (20140902); H04L 67/01 (20220501); A63F 13/30 (20140902),A63F 13/355 (20140101); H04L 29/08 (20060101); A63F 13/30 (20140101); A63F 13/77 (20140101); H04L 29/06 (20060101),"[['\n8257177', '\nSeptember 2012'], ['\n8267792', '\nSeptember 2012'], ['\n8366554', '\nFebruary 2013'], ['\n9168459', '\nOctober 2015'], ['\n9440143', '\nSeptember 2016'], ['\n9539509', '\nJanuary 2017'], ['\n9623322', '\nApril 2017'], ['\n9744454', '\nAugust 2017'], ['\n2003/0031062', '\nFebruary 2003'], ['\n2006/0079329', '\nApril 2006'], ['\n2007/0294387', '\nDecember 2007'], ['\n2008/0057894', '\nMarch 2008'], ['\n2008/0146338', '\nJune 2008'], ['\n2010/0004045', '\nJanuary 2010'], ['\n2010/0135544', '\nJune 2010'], ['\n2010/0162207', '\nJune 2010'], ['\n2011/0086701', '\nApril 2011'], ['\n2011/0263332', '\nOctober 2011'], ['\n2012/0015740', '\nJanuary 2012'], ['\n2012/0036003', '\nFebruary 2012'], ['\n2012/0123570', '\nMay 2012'], ['\n2012/0283008', '\nNovember 2012'], ['\n2013/0225282', '\nAugust 2013'], ['\n2013/0273996', '\nOctober 2013'], ['\n2013/0303288', '\nNovember 2013'], ['\n2013/0304584', '\nNovember 2013'], ['\n2013/0310084', '\nNovember 2013'], ['\n2013/0316811', '\nNovember 2013'], ['\n2014/0039990', '\nFebruary 2014'], ['\n2014/0258474', '\nSeptember 2014'], ['\n2014/0370969', '\nDecember 2014'], ['\n2015/0005055', '\nJanuary 2015'], ['\n2015/0011277', '\nJanuary 2015'], ['\n2015/0045105', '\nFebruary 2015'], ['\n2015/0194013', '\nJuly 2015'], ['\n2016/0038835', '\nFebruary 2016'], ['\n2016/0292810', '\nOctober 2016'], ['\n2016/0309292', '\nOctober 2016'], ['\n2017/0103410', '\nApril 2017']]","[6, '11,331,582', '11,303,730', '11,207,601', '11,103,778', '10,938,959', '10,498,860']"," What is claimed is:  1.  A system for providing online games on client devices based on capabilities of the client devices, the system comprising one or more physical processors configured by
machine-readable instructions to: host an online game, and transmit information to individual client devices for implementation and presentation of the online game on the individual client devices for user interaction, the individual client devices
including a first client device;  obtain device capability information regarding capabilities of the individual client devices such that first capability information regarding a first set of capabilities of the first client device is obtained;  determine
values of capability parameters for the first set of capabilities of the first client device such that a first value of a first capability parameter is determined;  determine the first value of the first capability parameter has breached a first
threshold value for the first capability parameter;  assign a first device profile to the first client device based on the determination that the first value of the first capability parameter has breached the first threshold value for the first
capability parameter;  transmit the first device profile to the first client device;  and select one or more of a frame rate for rendering the online game, a level of detail for rendering the online game, an audio quality for providing the online game, a
level of graphic effects for implementation of the online game, and/or a level of physics effects for implementation of the online game, wherein the selection is based on the assigned first device profile, wherein the online game is provided on the first
client device in accordance with the first device profile assigned to the first client device, and wherein providing the first game on the first client device in accordance with the first device profile includes one or more of rendering the online game
at the selected frame rate, rendering the online game at the selected level of detail, providing the online game with the selected audio quality, implementing the online game with the selected level of graphics effects, and/or implementing the online
game with the selected level of physics effects.
 2.  The system of claim 1, wherein the one or more physical processors are configured such that a second value of a second capability parameter is determined to have breached a second threshold value for the second capability parameter, and
wherein the assignment of the first device profile is further based on the determination that the value of the second capability parameter has breached the second threshold value for the second capability parameter.
 3.  The system of claim 2, wherein the assignment of the first device profile is based on a weighted sum of the first threshold value and the second threshold value.
 4.  The system of claim 2, wherein the assignment of the first device profile is based on the difference between the value of the first capability parameter and the first threshold value, and the difference between the second value of the second
capability parameter and the second threshold value.
 5.  The system of claim 1, wherein the capability information regarding the device capabilities of the client devices are obtained intermittently such the first set of capabilities of the first device is obtained intermittently, and wherein the
first value of the first capability parameter is determined intermittently in response to the first set of capabilities of the first device being obtained intermittently.
 6.  The system of claim 5, wherein the first capability information regarding the first client device is obtained after a first event completed in the online game by a first user associated with the first client device, and is obtained after a
second event completed in the online game by the first user associated with the first client device.
 7.  The system of claim 1, wherein the first set of capabilities include processing power, graphics processing speed, display capability, transmission speed, and remaining battery power of the first client device.
 8.  A method for providing online games on client devices based on capabilities of the client devices, the method being implemented in one or more physical processors configured to execute machine-readable instructions, the method comprising:
hosting an online game, and transmitting information to individual client devices for implementation and presentation of the online game on the individual client devices for user interaction, the individual client devices including a first client device; obtaining device capability information regarding capabilities of the individual client devices such that first capability information regarding a first set of capabilities of the first client device is obtained;  determining values of capability
parameters for the first set of capabilities of the first client device such that a first value of a first capability parameter is determined;  determining the first value of the first capability parameter has breached a first threshold value for the
first capability parameter;  assigning a first device profile to the first client device based on the determination that the first value of the first capability parameter has breached the first threshold value for the first capability parameter; 
transmitting the first device profile to the first client device;  and selecting one or more of a frame rate for rendering the online game, a level of detail for rendering the online game, an audio quality for providing the online game, a level of
graphic effects for implementation of the online game, and/or a level of physics effects for implementation of the online game, wherein the selection is based on the assigned first device profile, wherein the online game is provided on the first client
device in accordance with the first device profile assigned to the first client device, and wherein providing the first game on the first client device in accordance with the first device profile includes one or more of rendering the online game at the
selected frame rate, rendering the online game at the selected level of detail, providing the online game with the selected audio quality, implementing the online game with the selected level of graphics effects, and/or implementing the online game with
the selected level of physics effects.
 9.  The method of claim 8, further comprising determining a second value of a second capability parameter has breached a second threshold value for the second capability parameter, and wherein the assignment of the first device profile is
further based on the determination that the value of the second capability parameter has breached the second threshold value for the second capability parameter.
 10.  The method of claim 9, wherein the assignment of the first device profile is based on a weighted sum of the first threshold value and the second threshold value.
 11.  The method of claim 9, wherein the assignment of the first device profile is based on the difference between the value of the first capability parameter and the first threshold value, and the difference between the second value of the
second capability parameter and the second threshold value.
 12.  The method of claim 8, wherein the capability information regarding the device capabilities of the client devices are obtained intermittently such the first set of capabilities of the first device is obtained intermittently, and wherein the
first value of the first capability parameter is determined intermittently in response to the first set of capabilities of the first device being obtained intermittently.
 13.  The method of claim 12, wherein the first capability information regarding the first client device is obtained after a first event completed in the online game by a first user associated with the first client device, and is obtained after a
second event completed in the online game by the first user associated with the first client device.
 14.  The method of claim 8, wherein the first set of capabilities include processing power, graphics processing speed, display capability, transmission speed, and remaining battery power of the first client device. 
"
"10,213,152","
     February 26, 2019
","System and method for real-time measurement of sleep quality
"," An embodiment of the invention is directed to a method for the real time
     monitoring and measurement of sleep quality of a subject comprising the
     steps of; obtaining information from the subject using sensory signals;
     analyzing the information contained within the signals using signal
     processing and artificial intelligence; and using the analyzed
     information to create a protocol to improve the sleep quality of the
     subject. Another embodiment of the invention is directed to a system for
     real time monitoring and measurement of sleep quality of a subject
     comprising: a means for obtaining sensory information from the subject; a
     means for transmitting the sensory information; a means for analyzing the
     sensory information; and a means for creating a protocol to improve the
     sleep quality of the subject.
",A61B 5/0205 (20130101); A61B 5/4815 (20130101); A61B 5/4806 (20130101); A61B 5/0022 (20130101); A61B 5/0024 (20130101); A61B 5/14551 (20130101); A61B 2560/045 (20130101); A61B 5/349 (20210101),A61B 5/00 (20060101); A61B 5/0452 (20060101); A61B 5/0205 (20060101); A61B 5/1455 (20060101),"[['\n5047930', '\nSeptember 1991'], ['\n5299118', '\nMarch 1994'], ['\n7285090', '\nOctober 2007'], ['\n7343198', '\nMarch 2008'], ['\n7462150', '\nDecember 2008'], ['\n7654948', '\nFebruary 2010'], ['\n7959567', '\nJune 2011'], ['\n8157731', '\nApril 2012'], ['\n8348840', '\nJanuary 2013'], ['\n8398546', '\nMarch 2013'], ['\n8562526', '\nOctober 2013'], ['\n8641612', '\nFebruary 2014'], ['\n8708904', '\nApril 2014'], ['\n8852098', '\nOctober 2014'], ['\n8968196', '\nMarch 2015'], ['\n2004/0133081', '\nJuly 2004'], ['\n2004/0152957', '\nAugust 2004'], ['\n2005/0113650', '\nMay 2005'], ['\n2005/0267362', '\nDecember 2005'], ['\n2006/0041201', '\nFebruary 2006'], ['\n2006/0111635', '\nMay 2006'], ['\n2006/0235315', '\nOctober 2006'], ['\n2006/0253004', '\nNovember 2006'], ['\n2007/0032733', '\nFebruary 2007'], ['\n2007/0213624', '\nSeptember 2007'], ['\n2008/0161654', '\nJuly 2008'], ['\n2008/0161655', '\nJuly 2008'], ['\n2008/0167536', '\nJuly 2008'], ['\n2008/0167537', '\nJuly 2008'], ['\n2008/0167538', '\nJuly 2008'], ['\n2008/0167539', '\nJuly 2008'], ['\n2008/0171919', '\nJuly 2008'], ['\n2008/0171920', '\nJuly 2008'], ['\n2008/0171921', '\nJuly 2008'], ['\n2008/0171922', '\nJuly 2008'], ['\n2008/0275309', '\nNovember 2008'], ['\n2009/0131764', '\nMay 2009'], ['\n2009/0131803', '\nMay 2009'], ['\n2009/0203972', '\nAugust 2009'], ['\n2010/0049008', '\nFebruary 2010'], ['\n2011/0190594', '\nAugust 2011'], ['\n2013/0046151', '\nFebruary 2013'], ['\n2013/0158367', '\nJune 2013'], ['\n2013/0158368', '\nJune 2013'], ['\n2014/0206955', '\nJuly 2014'], ['\n2014/0213855', '\nJuly 2014'], ['\n2014/0213856', '\nJuly 2014'], ['\n2014/0213857', '\nJuly 2014'], ['\n2014/0221769', '\nAugust 2014'], ['\n2014/0221770', '\nAugust 2014'], ['\n2014/0221774', '\nAugust 2014'], ['\n2014/0221784', '\nAugust 2014'], ['\n2014/0221791', '\nAugust 2014'], ['\n2014/0222174', '\nAugust 2014'], ['\n2014/0223406', '\nAugust 2014'], ['\n2014/0223407', '\nAugust 2014'], ['\n2014/0257055', '\nSeptember 2014'], ['\n2014/0257540', '\nSeptember 2014'], ['\n2014/0324459', '\nOctober 2014'], ['\n2014/0330094', '\nNovember 2014'], ['\n2014/0342328', '\nNovember 2014'], ['\n2015/0302722', '\nOctober 2015'], ['\n2016/0120464', '\nMay 2016'], ['\n2017/0173262', '\nJune 2017'], ['\n2017/0265765', '\nSeptember 2017']]",[0]," What is claimed is:  1.  A method for real time monitoring and measurement of sleep quality of a subject comprising: receiving, at a smartphone, a sensor signal corresponding to an
electrocardiogram (ECG) from a subject and context information from one or more contextual sensors;  analyzing, at the smartphone or a server, information contained within the sensor signal using signal processing and artificial intelligence, the
analyzing comprising: segmenting the sensor signal corresponding to the ECG into signal segments of a specified epoch;  and for each signal segment: detecting a set of characteristic points from the signal segment;  extracting a feature set of elements
from the signal segment using the set of characteristic points, time-based parameters, and frequency-based parameters, wherein the time-based parameters comprise ECG derived respiration (EDR) time series and RR time series, and the frequency-based
parameters comprise EDR time series and RR time series;  and classifying the signal segment as containing an apnea event or not containing the apnea event according to the extracted feature set, wherein classifying the signal segment comprises training a
classifier with both self-learning algorithms and machine learning algorithms, the training including using the context information obtained automatically by the one or more contextual sensors and including optimizing the classifier for the subject
through continued use by the subject of a device providing the sensor signal such that accuracy increases for classifying apnea events occurring for the subject;  and outputting, via the smartphone, a sleep quality index of the subject based on at least
one signal segment that has been classified as containing the apnea event or not containing the apnea event, wherein the sleep quality index is a total number of apnea events over an amount of sleep time.
 2.  The method according to claim 1, further comprising: receiving, at the smartphone, at least one signal selected from an electroencephalogram signal, a respiratory monitor signal, an electromyogram signal, an electrooculogram signal, a
microphone signal, a video camera signal, and an oxygen saturation signal;  and analyzing, at the smartphone or the server, information contained within the at least one signal;  and combining results of the analyzing of the information contained within
the at least one signal with results of the analyzing of the information contained within the sensor signal corresponding to the ECG.
 3.  The method according to claim 1, further comprising: outputting an alert indicating an identification of the sleep apnea event in response to classifying a particular signal segment as containing the apnea event or not containing the apnea
event.
 4.  The method according to claim 1, further comprising: classifying the signal segment as a particular stage of sleep;  and indicating overall sleep index of the subject based on at least one segment that has been classified having the
particular stage of sleep, wherein the classifier is optimized for the subject through continued use by the subject of the device providing the sensor signal such that accuracy increases for classifying the stage of sleep for the subject.
 5.  The method according to claim 1, wherein the specified epoch is between 30 seconds to 1 minute, inclusive.
 6.  The method according to claim 1, wherein detecting the set of characteristic points comprises performing an automated wavelet based analysis for QRS complex, P wave, and either T wave or R-wave characteristic points.
 7.  The method according to claim 1, wherein extracting the feature set comprises: extracting, at the server, a full feature set of elements from the signal segment.
 8.  The method according to claim 1, wherein extracting the feature set comprises: extracting, at the smartphone, a reduced feature set of elements from the signal segment.
 9.  The method according to claim 1, wherein classifying the signal segment comprises using a support vector machine (SVM) classifier as the artificial intelligence.
 10.  The method according to claim 1, further comprising receiving, at the smartphone, a photoplethysmogram signal from an oximeter, and analyzing, at the smartphone or the server, information contained within the photoplethysmogram signal by at
least: segmenting the photoplethysmogram signal into photoplethysmogram signal segments of the specified epoch;  and for each photoplethysmogram signal segment: detecting characteristic points from the photoplethysmogram signal segment;  and extracting a
corresponding feature set for the photoplethysmogram signal segment using the characteristic points and time-based parameters for the photoplethysmogram signal segment.
 11.  The method according to claim 10, wherein extracting the corresponding feature set for the photoplethysmogram signal segment comprises: extracting, at the server, a full feature set of elements from the photoplethysmogram signal segment.
 12.  The method according to claim 10, wherein extracting the corresponding feature set for the photoplethysmogram signal segment comprises: extracting, at the smartphone, a reduced feature set of elements from the photoplethysmogram signal
segment.
 13.  The method according to claim 10, wherein detecting the characteristic points comprises detecting oxygen desaturation indices.
 14.  A system for real-time measurement of sleep quality, comprising: a computing system having stored thereon computer software that, when executed by a processor of the computing system, directs the computing system to: receive a sensor signal
comprising sensory information corresponding to at least an electrocardiogram (ECG) and context information from one or more context sensors;  segment the sensor signal into signal segments of a specified epoch;  and for each signal segment: detect a set
of characteristic points from the signal segment;  extract a feature set of elements from the signal segment using the set of characteristic points, time-based parameters, and frequency-based parameters, wherein the time-based parameters comprise ECG
derived respiration (EDR) time series and RR time series, and the frequency-based parameters comprise EDR time series and RR time series;  and classify the signal segment as containing an apnea event or not containing the apnea event according to the
extracted feature set, wherein the computer software further directs the computing system to train a classifier used to classify the signal segment with both self-learning algorithms and machine learning algorithms, the training including using context
information obtained automatically by the one or more context sensors and including optimizing the classifier for the subject through continued use by the subject of a device providing the sensor signal such that accuracy increases for classifying apnea
events occurring for the subject;  and output a sleep quality index of the subject based on at least one signal segment that has been classified as containing the apnea event or not containing the apnea event, wherein the sleep quality index is a total
number of apnea events over an amount of sleep time.
 15.  The system according to claim 14, wherein the computing system comprises a server, wherein the computer software that directs the computing system to extract the feature set of elements directs the server to extract a full feature set of
elements.
 16.  The system according to claim 14, wherein the computing system comprises a smartphone, wherein the computer software that directs the computing system to extract the feature set of elements directs the smartphone to extract a reduced
feature set of elements.
 17.  The system according to claim 14, wherein the computer software that, when executed by the processor of the computing system, further directs the computing system to: classify the signal segment as a particular stage of sleep;  and indicate
overall sleep index of the subject based on at least one segment that has been classified having the particular stage of sleep, wherein the classifier is optimized for the subject through continued use by the subject of the device providing the sensor
signal such that accuracy increases for classifying the stage of sleep for the subject.
 18.  The system according to claim 14, wherein the specified epoch is between 30 seconds to 1 minute, inclusive.
 19.  The system according to claim 14, wherein the computer software that directs the computing system to detect the set of characteristic points directs the computing system to perform an automated wavelet based analysis for QRS complex, P
wave, and either T wave or R-wave characteristic points.
 20.  The system according to claim 14, further comprising: one or more sensors including a sensor providing the ECG;  and a body area network receiving sensory information from the one or more sensors and transmitting the sensory information for
receipt by the computing system.  "
"10,217,179","
     February 26, 2019
","System and method for classification and authentication of identification
     documents using a machine learning based convolutional neural network
"," The present disclosure describes systems and methods to classify and
     authenticate ID documents based on the information contained on the face
     of the ID document. This present system can automatically classify,
     authenticate, and extracting data from documents using artificial
     intelligence (AI) based machine learning for image recognition. In some
     implementations, the machine learning techniques include a convolutional
     neural network. The system can also determine the authenticity of other
     documents, such as currency, stamps, and invoices.
",G06N 3/0454 (20130101); G06N 3/02 (20130101); G06V 10/40 (20220101); G06V 30/40 (20220101); G06K 19/06037 (20130101); G06K 7/1417 (20130101); G06Q 50/265 (20130101); G06V 10/243 (20220101); G06N 3/084 (20130101); G06K 19/06028 (20130101); G06V 10/454 (20220101); G06V 20/95 (20220101),G06Q 50/26 (20120101); G06K 9/32 (20060101); G06N 3/02 (20060101); G06K 19/06 (20060101); G06K 7/14 (20060101); G06K 9/00 (20060101); G06K 9/46 (20060101),"[['', '']]","[2, '11,238,276', '10,635,898']"," What is claimed:  1.  A method to determine that a physical identification document is authentic, comprising: receiving, by an authentication manager, an image of a physical identification
document to be authenticated;  extracting, by the authentication manager, a set of characteristics of the physical identification document from the image;  determining, by the authentication manager, a class of the physical identification document based
on the set of characteristics of the physical identification document;  selecting, by the authentication manager and based on the class of the physical identification document, a kernel to generate pixels of feature maps by processing input pixels from
images of physical identification documents;  generating, by the authentication manager, a feature map of the image using the kernel;  determining, by the authentication manager, a score based on the feature map;  and providing, by the authentication
manager, an indication that the physical identification document is authentic based on the score.
 2.  The method of claim 1, further comprising: dividing, by the authentication manager, the image of the physical identification document into a plurality of channels.
 3.  The method of claim 2, further comprising: selecting, by the authentication manager, a kernel for each of the plurality of channels;  generating, by the authentication manager, a respective feature map for each of the plurality of channels
based on the respective kernel for each of the plurality of channels;  and determining, by the authentication manager, the score based on the respective feature map for each of the plurality of channels.
 4.  The method of claim 2, further comprising: generating, by the authentication manager, a respective feature map for each of the plurality of channels using the kernel.
 5.  The method of claim 1, further comprising: training a convolutional neural network with a first plurality of scores from previously authenticated physical identification documents and a second plurality of scores from previously invalidated
physical identification documents;  and determining the indication that the physical identification is authentic using the convolutional neural network.
 6.  The method of claim 1, further comprising: determining, by the authentication manager, a class of a first face of the physical identification document;  determining, by the authentication manager, a class of a second face of the physical
identification document;  and determining, by the authentication manager, the score based on a comparison of the class of the first face of the physical identification document and the class of the second face of the physical identification document.
 7.  The method of claim 1, further comprising: normalizing, by the authentication manager, the image of the physical identification document.
 8.  The method of claim 7, wherein normalizing the image of the physical identification document comprises at least one of removing a background from the image, rotating the image, deskewing the image, removing a glare from the image, correcting
an exposure of the image, or correcting a blur of the image.
 9.  The method of claim 1, further comprising: determining, by the authentication manager, a subclass of the physical identification document based on the set of characteristics of the physical identification document;  and selecting the kernel
based on the subclass of the physical identification document.
 10.  The method of claim 1, wherein the set of characteristics of the physical identification document comprises a physical size of a barcode, a location of a barcode, or a location of a security feature.
 11.  A system to determine a physical identification document is authentic, the system comprising an authentication manger executable by one or more processors to: receive an image of a physical identification document to be authenticated; 
extract a set of characteristics of the physical identification document from the image;  determine a class of the physical identification document based on the set of characteristics of the physical identification document;  select, based on the class
of the physical identification document, a kernel to generate pixels of feature maps by processing input pixels from images of physical identification documents;  generate a feature map of the image using the kernel;  determine a score based on the
feature map;  and provide an indication that the physical identification document is authentic based on the score.
 12.  The system of claim 11, wherein the authentication manager is executable by the one or more processors to divide the image of the physical identification document into a plurality of channels.
 13.  The system of claim 12, wherein the authentication manager is executable by the one or more processors to: select a kernel for each of the plurality of channels;  generate a respective feature map for each of the plurality of channels based
on the respective kernel for each of the plurality of channels;  and determine the score based on the respective feature map for each of the plurality of channels.
 14.  The system of claim 12, wherein the authentication manager is executable by the one or more processors to generate a respective feature map for each of the plurality of channels using the kernel.
 15.  The system of claim 11, wherein the authentication manager is executable by the one or more processors to: train a convolutional neural network with a first plurality of scores from previously authenticated physical identification documents
and a second plurality of scores from previously invalidated physical identification documents;  and determine the indication that the physical identification is authentic using the convolutional neural network.
 16.  The system of claim 11, wherein the authentication manager is executable by the one or more processors to: determine a class of a first face of the physical identification document;  determine a class of a second face of the physical
identification document;  and determine the score based on a comparison of the class of the first face of the physical identification document and the class of the second face of the physical identification document.
 17.  The system of claim 11, wherein the authentication manager is executable by the one or more processors to normalize the image of the physical identification document.
 18.  The system of claim 17, wherein normalizing the image of the physical identification document comprises at least one of removing a background from the image, rotating the image, deskewing the image, removing a glare from the image,
correcting an exposure of the image, or correcting a blur of the image.
 19.  The system of claim 11, wherein the authentication manager is executable by the one or more processors to: determine a subclass of the physical identification document based on the set of characteristics of the physical identification
document;  and select the kernel based on the subclass of the physical identification document.
 20.  The system of claim 11, wherein the set of characteristics of the physical identifications document comprises a physical size of a barcode, a location of a barcode, or a location of a security feature. 
"
"10,217,285","
     February 26, 2019
","HUD object design and method
"," The invention features a rectangular 3-D modeling grid called a display
     environment that may be mapped to one or more sensor(s) to provide a
     heads up display device the ability to generate and view an Augmented
     Reality first; person view of custom 3-D objects. Location sensors create
     the positioning and perimeter of the display environment. The Display
     Environment may be navigated by the combination of the display device's
     physical movement sensed by motion sensors and the display device's
     physical location based on its proximity to synchronized location
     sensors. Sensors on the display device recognize when the device is
     moving with respect to the Display Environment to initiate re-rendering
     of its 3-D model being displayed. Movement of the display device enable
     first person 3-D model illustrative and perspective views which may also
     be used to design 3-D models with customizable scale, orientation,
     positioning physics, and artificial intelligence.
",G02B 27/017 (20130101); G06T 19/20 (20130101); G06T 19/006 (20130101); G02B 30/00 (20200101); G06T 2219/024 (20130101); G02B 2027/0134 (20130101); G06T 2219/2016 (20130101); G02B 2027/0178 (20130101); G02B 2027/014 (20130101); G02B 2027/0187 (20130101); G02B 2027/0138 (20130101),G06T 19/00 (20110101); G02B 27/01 (20060101); G06T 19/20 (20110101); G02B 27/22 (20180101),"[['\n9129404', '\nSeptember 2015'], ['\n2002/0175911', '\nNovember 2002'], ['\n2003/0179218', '\nSeptember 2003'], ['\n2011/0216060', '\nSeptember 2011'], ['\n2011/0304703', '\nDecember 2011'], ['\n2012/0287284', '\nNovember 2012'], ['\n2013/0083173', '\nApril 2013'], ['\n2014/0306993', '\nOctober 2014']]",[0]," What is claimed is:  1.  A computer program product comprising a non-transitory computer usable medium having control logic stored therein for causing a computer to enable a first person
augmented reality view of 3-dimensional objects, comprising: computer readable program code for initializing communication between a display device and at least one location sensor, wherein said at least one location sensor is positioned externally away
from said display device;  computer readable program code for receiving sensor data from said at least one location sensor;  computer readable program code for generating an augmented reality environment using data from said at least one location sensor,
wherein said augmented reality environment comprises a spatial origin point that is determined from an actual position of said at least one location sensor;  computer readable program code for incorporating a 3-dimensional target object within said
augmented reality environment;  computer readable program code for applying a Cartesian coordinate grid to said augmented reality environment wherein the Cartesian coordinate grid is defined from the spatial origin point and the display device
synchronizes with said at least one location sensor and a CPU of said display device, and by execution of the computer readable code, calculates a distance between said at least one location sensor and said display device using a computer generated
lateral line and wherein said lateral line intersects a computer generated vertical line passing through said spatial origin point;  and computer readable program code for displaying said target object within said augmented reality environment in a first
person augmented reality view.
 2.  The computer program product of claim 1, further comprising computer readable program code for enabling manipulation of said target object.
 3.  The computer program product of claim 1, further comprising computer readable program code for changing the display of said target object to reflect a change in position or orientation of said display device.
 4.  The computer program product of claim 1, wherein said sensor data includes data regarding said augmented reality environment's physical characteristics, scale, position and orientation.
 5.  The computer program product of claim 1, further comprising computer readable program code for enabling superimposition of a 3-dimensional image over said augmented reality environment.
 6.  The computer program product of claim 1, wherein said augmented reality environment is generated by virtue of said at least one location sensor's positioning.
 7.  The computer program product of claim 1, further comprising computer readable program code for providing a virtual representation of said Cartesian coordinate grid.
 8.  The computer program product of claim 7, wherein said virtual representation is done by synchronization between said display device and said at least one location sensor.
 9.  The computer program product of claim 1, further comprising computer readable program code for rendering real time effects to simulate photorealistic user interfaces.
 10.  The computer program product of claim 1, further comprising computer readable program code for simulating a user's presence within said augmented reality environment.
 11.  The computer program product of claim 1, further comprising computer readable program code for enabling application of physical attributes to said target object.
 12.  The computer program product of claim 11, further comprising computer readable program code for simulating effects of said application of physical attributes on said target object.
 13.  The computer program product of claim 12, further comprising computer readable program code for displaying said simulated effects of said physical attributes on said target object.
 14.  The computer program product of claim 1, wherein said target object is an image of an actual object as captured by said display device.
 15.  The computer program product of claim 1, wherein said target object is a 3-dimensional design created within said augmented reality environment by a user.
 16.  The computer program product of claim 1, further comprising computer readable program code for enabling motion capturing and proximity sensing by said display device.
 17.  The computer program product of claim 1, further comprising computer readable program code for enabling participation of multiple users within said augmented reality environment.
 18.  The computer program product of claim 17, further comprising computer readable program code for enabling co-designing by said multiple users.
 19.  The computer program product of claim 1, further comprising computer readable program code for generating an inner dimension user point of view of said augmented reality environment thereby enabling said user to view and navigate within
said augmented reality environment.
 20.  The computer program product of claim 19, wherein said computer readable program code comprises computer readable program code for combining said display device's location and said augmented reality environment's properties with said
display device's focal point.
 21.  The computer program product of claim 1, further comprising computer readable program code for enabling simulation of zooming in towards or zooming out from said target object by said display device.
 22.  The computer program product of claim 1, further comprising computer readable program code for enabling navigation of said augmented reality environment.
 23.  The computer program product of claim 22, wherein said computer readable program code comprises computer readable program code for sensing motion by said display device;  and computer readable program code for determining position of said
display device in relation to its proximity to said at least one location sensor.
 24.  The computer program product of claim 1, further comprising computer readable program code for generating and displaying possible target object outcome based on application of physical properties on said target object.
 25.  The computer program product of claim 1, further comprising computer readable program code for generating sound playback based on said display device's change in its proximity to said augmented reality environment.
 26.  The computer program product of claim 1, further comprising computer readable program code for determining said spatial origin point.
 27.  The computer program product of claim 26, wherein said computer readable program code further comprises: computer readable program code for determining the width and length variables of a positioning layout formed by said at least one
location sensor;  and computer readable program code for dividing said width and length variables by 2.  "
"10,223,638","
     March 5, 2019
","Control system, method and device of intelligent robot based on artificial
     intelligence
"," The present disclosure provides a control system, a control method and a
     control device of an intelligent robot based on artificial intelligence.
     The system includes: a decision engine, disposed on the intelligent
     robot, and configured to generate cloud processing information according
     to a multimodal input signal, and to send the cloud processing
     information; and a cloud control center, configured to receive the cloud
     processing information, to obtain a user demand by analyzing the cloud
     processing information, and to return the user demand, such that the
     decision engine controls the intelligent robot according to at least one
     of the user demand and the multimodal input signal. The control system
     may make full use of great online information, enhance the capability of
     the intelligent robot for storage, calculation and processing complex
     decisions, and meanwhile may respond to the user's instruction timely,
     rapidly and intelligently, and improve the user experience.
",G06N 5/022 (20130101); H04W 4/029 (20180201); G06V 40/172 (20220101); H04W 4/33 (20180201); B25J 11/0005 (20130101); G10L 15/04 (20130101); G10L 15/22 (20130101); G10L 2015/228 (20130101),G06N 5/02 (20060101); G06K 9/00 (20060101); G10L 15/04 (20130101); G10L 15/22 (20060101); B25J 11/00 (20060101),"[['\n8594845', '\nNovember 2013'], ['\n2012/0268580', '\nOctober 2012'], ['\n2014/0337007', '\nNovember 2014']]",[0]," What is claimed is:  1.  A control system of an intelligent robot based on artificial intelligence, comprising: a decision engine, disposed on the intelligent robot, and configured to receive a
preset speech input by the user, to control the intelligent robot to enter a speech instruction monitoring mode based on the preset speech, to receive a speech input by the user under the speech instruction monitoring mode, to divide the speech into a
plurality of speech segments as cloud processing information, and to send the plurality of speech segments to a cloud control center for analyzing, to obtain a user demand as a movement control demand obtained by the cloud control center based on the
speech segments corresponding to the speech, to obtain a location of the user based on the movement control demand, to generate a map of surrounding area according to at least one of an image signal received by the intelligent robot and an environment
sensor signal received by the intelligent robot, to sense surrounding environment, and to control the intelligent robot to move on the user demand according to the location of the user, the map of surrounding area and the surrounding environment;  and
the cloud control center, configured to receive the plurality of speech segments, to perform a speech recognition on the plurality of speech segments respectively and to perform one or more of natural language understanding, semantic analysis and
sentiment analysis on a result of the speech recognition, so as to obtain the user demand, and to return the user demand to the decision engine.
 2.  The system according to claim 1, wherein the user demand is a way-finding demand, and the decision engine is configured to: obtain a current location and a target location of the intelligent robot, and control the intelligent robot to
indicate directions or guide directions via speech interaction, according to the map of surrounding area as well as the current location and the target location of the intelligent robot.
 3.  The system according to claim 1, wherein the decision engine is configured to detect a movement of the user according to at least one of the image signal and the environment sensor signal and to take a movement detection result as the cloud
processing information according to a preset rule, the cloud control center is configured to perform a statistical analysis on the movement detection result and to compare an analysis result with a corresponding knowledge base, so as to obtain the user
demand.
 4.  The system according to claim 3, wherein the decision engine is configured to control the intelligent robot to send a prompt message to a preset mobile terminal according to the user demand.
 5.  The system according to claim 1, wherein the decision engine is further configured to: perform a face detection on the image signal;  if the image signal is a face image signal, send the face image signal to the cloud control center for face
recognition;  and determine identity information of the user according to a result of the face recognition.
 6.  A control method of an intelligent robot based on artificial intelligence, comprising: receiving, by a decision engine of the intelligent robot, a preset speech input by the user;  controlling, by the decision engine of the intelligent
robot, the intelligent robot to enter a speech instruction monitoring mode based on the preset speech;  receiving, by the decision engine of the intelligent robot, a speech input by the user under the speech instruction monitoring mode;  dividing, by the
decision engine of the intelligent robot, the speech into a plurality of speech segments as cloud processing information;  sending, by the decision engine of the intelligent robot, the plurality of speech segments to a cloud control center for analyzing,
to obtain a user demand as a movement control demand obtained by the cloud control center based on the speech segments corresponding to the speech;  obtaining, by the decision engine of the intelligent robot, a location of the user based on the movement
control demand;  generating, by the decision engine of the intelligent robot, a map of surrounding area according to at least one of an image signal received by the intelligent robot and an environment sensor signal received by the intelligent robot; 
sensing, by the decision engine of the intelligent robot, surrounding environment;  and controlling, by the decision engine of the intelligent robot, the intelligent robot to move on the user demand according to the location of the user, the map of
surrounding area and the surrounding environment.
 7.  The method according to claim 6, wherein the user demand is a way-finding demand, and the method further comprises: obtaining a current location and a target location of the intelligent robot by the decision engine;  controlling the
intelligent to indicate directions or guide directions via speech interaction, by the decision engine according to the map of surrounding area as well as the current location and target location of the intelligent robot.
 8.  The method according to claim 6, wherein, generating by a decision engine of the intelligent robot cloud processing information according to a multimodal input signal received by the intelligent robot comprises: detecting a movement of the
user according to at least one of the image signal and the environment sensor signal, and taking a movement detection result as the cloud processing information according to a preset rule.
 9.  The method according to claim 8, wherein controlling the intelligent robot by the decision engine according to at least one of the user demand and the multimodal input signal comprises: controlling the intelligent robot to send a prompt
information to a preset mobile terminal according to the user demand.
 10.  The method according to claim 6, further comprising: performing a face detection on the image signal by the decision engine;  if the image signal is a face image signal, sending the face image signal to the cloud control center for face
recognition;  and determining identity information of the user according to a result of the face recognition.
 11.  A control device of an intelligent robot based on artificial intelligence, comprising: a processor;  a memory configured to store an instruction executable by the processor;  wherein the processor is configured to: receive a preset speech
input by the user;  control the intelligent robot to enter a speech instruction monitoring mode based on the preset speech;  receive a speech input by the user under the speech instruction monitoring mode;  divide the speech into a plurality of speech
segments as cloud processing information;  send the plurality of speech segments to a cloud control center for analyzing, to obtain a user demand as a movement control demand obtained by the cloud control center based on the speech segments corresponding
to the speech;  obtain a location of the user based on the movement control demand;  generate a map of surrounding area according to at least one of an image signal received by the intelligent robot and an environment sensor signal received by the
intelligent robot;  sense surrounding environment;  and control the intelligent robot to move on the user demand according to the location of the user, the map of surrounding area and the surrounding environment.
 12.  The device according to claim 11, wherein the user demand is a way-finding demand, and the processor is configured to: obtain a current location and a target location of the intelligent robot, and control the intelligent robot to indicate
directions or guide directions via speech interaction, according to the map of surrounding area as well as the current location and the target location of the intelligent robot.  "
"10,223,640","
     March 5, 2019
","Utilizing artificial intelligence for data extraction
"," Solved diagnosis case data is stored by utilizing a redundant
     discrimination net as a dynamic memory. The stored diagnosis case data is
     incorporated to form scientific descriptions within a medical knowledge
     base and heuristics within an empirical knowledge base. Diagnosis
     hypotheses are generated using an initial symptom description, the
     dynamic memory, and the medical knowledge base. The initial symptom
     description is received from an end user. A subset of the diagnosis
     hypotheses is created to form one or more solution cases. The one or more
     solution cases are presented to a subject matter expert. A diagnosis
     success or a diagnosis failure identifying, based on a response received
     from the subject matter expert, to form an assessed solution case. An
     assessed solution case is converted into experiences. The experiences are
     inputted into the dynamic memory. Data containing the assessed solution
     case is transmitted to a medical artificial intelligence analytics
     application.
",G16H 50/70 (20180101); G06N 5/022 (20130101); G06N 5/045 (20130101); G16H 50/20 (20180101); G16H 70/60 (20180101),G06N 5/04 (20060101); G16H 50/20 (20180101),"[['\n4945476', '\nJuly 1990'], ['\n6212528', '\nApril 2001'], ['\n6527713', '\nMarch 2003'], ['\n6571251', '\nMay 2003'], ['\n7899764', '\nMarch 2011'], ['\n8296247', '\nOctober 2012'], ['\n8337409', '\nDecember 2012'], ['\n8548828', '\nOctober 2013'], ['\n9153142', '\nOctober 2015'], ['\n9594878', '\nMarch 2017'], ['\n2014/0279746', '\nSeptember 2014'], ['\n2015/0066378', '\nMarch 2015'], ['\n2016/0259893', '\nSeptember 2016']]",[1]," What is claimed is:  1.  A computer-implemented process for generating ailment, disorder, and disease diagnostic data for consumption by a medical artificial intelligence analytics application,
the computer-implemented process comprising: storing solved diagnosis case data utilizing a redundant discrimination net as a dynamic memory, wherein the solved diagnosis case data is in a form of an electronic document inputted by a subject matter
expert, wherein the electronic document was ingested from the Internet by a natural language processor accessed by utilizing a web crawling application designed to locate medical data on the Internet;  incorporating, by utilizing automated reasoning, the
stored diagnosis case data to form scientific descriptions within a medical knowledge base and heuristics within an empirical knowledge base, wherein the descriptions and heuristics include ailments, disorders, and diseases;  generating diagnosis
hypotheses using an initial symptom description, the dynamic memory, and the medical knowledge base, the initial symptom description being received from an end user, wherein the medical knowledge base includes one or more cases extracted from medical
journals and medical dictionaries, wherein the generating diagnosis hypotheses comprises: receiving an initial symptom description from the end user, wherein the initial symptom description may be in the form of at least one of text, audio, or a video
format;  transforming, utilizing natural language processing, at least one of text, audio, or a video format into data structures;  and comparing the data structures to substantially similar data structures stored in case memory;  transmitting, to the
subject matter expert, the medical journals and medical dictionaries in the form of electronic documents;  creating a subset of the diagnosis hypotheses to form one or more solution cases;  presenting, after generating the diagnosis hypotheses based on
the initial symptom description being received from the end user and after creating a subset of the diagnosis hypotheses to form the one or more solution cases, the one or more solution cases to a subject matter expert;  identifying, based on a response
to the transmitted electronic documents and the presented solution cases received from the subject matter expert, a diagnosis success or a diagnosis failure to form an assessed solution case;  converting assessed solution case into experiences; 
inputting the experiences into the dynamic memory, wherein the dynamic memory is a self-organizing process that incorporates new knowledge as a result and improves performance in future diagnostic case sessions;  transmitting data containing the assessed
solution case to a medical artificial intelligence analytics application, wherein the transmitted data contains updated and validated solution case data;  incorporating, using a self-organizing process, new knowledge against existing scientific
descriptions and heuristics;  proposing corrections and enhancements to the medical knowledge base by analyzing the new knowledge incorporated against existing scientific descriptions and heuristics, wherein the medical knowledge base maintaining module
keeps the empirical knowledge base up to date with recent scientific and empirical data inputted by the subject matter expert, wherein the recent scientific and empirical data includes medical websites, scientific papers, published studies;  generating
the updated and validated solution case data;  receiving a request, by an analytics system administrator, for the generated updated and validated solution case data according to a predetermined format criteria;  and exporting data containing the
generated updated and validated solution case data based on to the predetermined format criteria to the analytics system administrator.  "
"10,223,641","
     March 5, 2019
","Mood detection with intelligence agents
"," Embodiments of the present invention provide systems and methods for
     increasing the quality of interactions between two or more entities.
     These entities are either individuals (e.g., human beings using a
     computer device) or artificial intelligence (AI) agents. The interactions
     between all of the entities within a computing environment are mapped and
     analyzed. Based on the mapped interactions, a relationship model is
     generated in order to run multiple applications within a computing
     environment.
",G06N 5/022 (20130101); G06N 3/006 (20130101); G06N 5/043 (20130101),G06N 5/04 (20060101),"[['\n8565922', '\nOctober 2013'], ['\n8621416', '\nDecember 2013'], ['\n8838505', '\nSeptember 2014'], ['\n8965828', '\nFebruary 2015'], ['\n9098109', '\nAugust 2015'], ['\n9160773', '\nOctober 2015'], ['\n9277375', '\nMarch 2016'], ['\n2014/0288704', '\nSeptember 2014'], ['\n2015/0086949', '\nMarch 2015'], ['\n2015/0189085', '\nJuly 2015'], ['\n2016/0117593', '\nApril 2016'], ['\n2016/0162474', '\nJune 2016'], ['\n2016/0300570', '\nOctober 2016'], ['\n2017/0228520', '\nAugust 2017'], ['\n2018/0075205', '\nMarch 2018'], ['\n2018/0196796', '\nJuly 2018']]","[2, '10,586,160', '10,586,159']"," What is claimed:  1.  A method comprising: observing, by one or more processors, interactions between a plurality of individuals and a plurality of artificial intelligence (AI) agents;  creating,
by one or more processors, a cognitive profile for each respective individual of the plurality of individuals and each respective AI agent of the plurality of AI agents using social media data, observation protocols, and biometric data for the plurality
of individuals and special testing data for the plurality of AI agents, wherein the observation protocols comprise monitoring conversation tone and facial expressions during a respective interaction, and wherein the special testing data comprises sensor
data, text data, and speech recognition data obtained during a respective interaction;  mapping, by one or more processors, a set of interactions to each cognitive profile for each respective individual and AI agent based on the observed interactions
between the plurality of individuals and the plurality of AI agents, wherein each respective individual and AI agent is represented as a data object and each interaction is represented as a data link;  generating, by one or more processors, a plurality
of relationship models based on the mapped set of interactions, wherein each relationship model is associated with a single interaction within the mapped set of interactions;  completing, by one or more processors, a mood analysis for each respective
interaction, wherein the mood analysis for each respective interaction comprises: identifying, by one or more processors, a level of positive sentiment and a level of negative sentiment;  identifying, by one or more processors, a behavioral trigger that
lead to a change in behavior by either the individual or the AI agent, and interpreting, by one or more processors, facial expressions of either the individual or the AI agent;  updating, by one or more processors, the cognitive profile for each
respective individual of the plurality of individuals and each respective AI agent of the plurality of AI agents based on the plurality of relationship models and the mood analysis;  generating, by one or more processors, an action operation for each
respective individual and AI agent based on the cognitive profile for the respective individual and respective AI agent to ameliorate respective interactions between the plurality of individuals and plurality of AI agents;  and implementing, by one or
more processors, the action operation for each respective individual and AI agent.  "
"10,223,842","
     March 5, 2019
","System for controlling remotely connected vehicle
"," Disclosed is a system for controlling a vehicle using a remote artificial
     intelligence (AI) server. A vehicle communicates with an artificial
     intelligence server for noise, vibration and harshness (NVH) issue
     diagnosis. The vehicle controls its fuel combustion condition for
     improving NVH based on an NVH diagnosis result using the AI.
",B60W 40/09 (20130101); G07C 5/008 (20130101); G10K 15/02 (20130101); H04L 67/12 (20130101); G07C 5/0808 (20130101); G01H 3/00 (20130101); G01H 1/00 (20130101); G10K 2210/1282 (20130101),B60K 28/00 (20060101); G07C 5/00 (20060101); G10K 15/02 (20060101); G07C 5/08 (20060101); H04L 29/08 (20060101); B60W 40/09 (20120101),"[['\n6481271', '\nNovember 2002'], ['\n7013213', '\nMarch 2006'], ['\n2003/0088346', '\nMay 2003'], ['\n2008/0234964', '\nSeptember 2008'], ['\n2010/0082274', '\nApril 2010'], ['\n2013/0103238', '\nApril 2013'], ['\n2013/0225092', '\nAugust 2013'], ['\n2014/0303905', '\nOctober 2014'], ['\n2015/0148654', '\nMay 2015'], ['\n2017/0238505', '\nAugust 2017']]",[1]," What is claimed is:  1.  A vehicle control total management system comprising: a noise sensor configured to generate a noise data signal based on measured noise;  a vibration sensor configured to
generate a vibration data signal based on measured noise;  and a diagnosis unit configured to analyze the noise data signal, the vibration data signal, and driving condition data by using artificial intelligence (AI)-learning, to diagnose a noise
vibration harshness (NVH) problem of the vehicle and to control a combustion condition of the vehicle for addressing the NVH problem.
 2.  The vehicle control total management system of claim 1, wherein: the diagnosis unit is configured to monitor the noise data signal, the vibration data signal, and a combustion pressure measurement result after controlling the combustion
condition, and to control the combustion condition based on the monitored result, and AI-learns data based on the combustion condition control and the monitored result to construct AI for determining the combustion condition.
 3.  The vehicle control total management system of claim 2, wherein: the diagnosis unit is further configured to transmit information on the noise data signal, the vibration data signal, the combustion pressure measurement result, and the
combustion condition control to a central artificial intelligence server during the AI learning.
 4.  The vehicle control total management system of claim 1, wherein: the diagnosis unit is further configured to constrict an artificial intelligence neural network for the NVH diagnosis by learning the noise data signal, the vibration data
signal, and the driving condition data by a deep learning scheme and is further configured to perform the NVH diagnosis by the artificial intelligence neural network.
 5.  The vehicle control total management system of claim 4, wherein: the diagnosis unit is further configured to covert the noise data signal and the vibration data signal into image data through digital signal processing and performs at least
one of a time domain image analysis algorithm and a frequency domain image analysis algorithm by using a gabor filter with respect to the image data.
 6.  The vehicle control total management system of claim 4, wherein: the diagnosis unit is further configured to apply a deep neural network (DNN) learning machine technique or a convolution neural network (CNN) learning machine technique to the
noise data signal and the vibration data signal.
 7.  The vehicle control total management system of claim 1, further comprising: a signal processing controller configured to control an engine, a transmission, and the like of a vehicle, wherein the diagnosis unit is configured to transmit an
instruction for the combustion condition to the signal processing controller.
 8.  The vehicle control total management system of claim 1, wherein: when the NVH diagnosis result is an abnormal state, reservation information in which a maintenance service is providable is received based on a GPS position of the vehicle, and
a navigation interlocks with the reservation information.
 9.  The vehicle control total management system of claim 1, wherein: classification categories according to the NVH diagnosis result include at least one selected from the group consisting of transmission noise, gasoline knocking, piston noise,
oil pump noise, high pressure pump noise, vacuum pump noise, timing chain noise, timing belt noise, valve system noise, turbocharger noise, injector noise, crank system noise, fuel pulsation noise, alternator noise, auxiliary machinery belt noise,
driveline noise, intake/exhaust system noise, water pump noise, power train related noise, and power train false noise.
 10.  The vehicle control total management system of claim 1, wherein: parameters for the NVH diagnosis include whether the parameter is an engine state which is noise and vibration data measured in the engine room based on a position where noise
and vibration are measured or whether the parameter is a vehicle state which is noise and vibration data measured in a vehicle interior, whether a temperature condition in which noise and vibration are measured is a cold or hot condition, and whether the
parameter is an idle condition in a stopped state, an acceleration driving mode in a driving condition which is a driving state, a deceleration driving mode, or a constant speed driving mode.
 11.  A central artificial intelligence server receiving NVH diagnosis results for multiple vehicles, comprising: a central diagnosis unit configured to classify the NVH diagnosis results received from the multiple vehicles for each vehicle type; and a database configured to store the classified NVH diagnosis results, wherein the central diagnosis unit configured to update parameter values for artificial intelligence of the multiple vehicles by learning data stored in the database.
 12.  The central artificial intelligence server of claim 11, wherein: the central diagnosis unit is configured to determine and update parameter values for an artificial intelligence neural network by learning the data of the database by deep
learning, and wherein the server is further configured to transmit the updated parameter values to a vehicle.
 13.  The central artificial intelligence server of claim 11, wherein: an abnormality result is notified to a driver of a vehicle in which the diagnosis result is abnormal among the multiple vehicles and information on the abnormal vehicle is
transmitted to a service network in order to provide a maintenance service.
 14.  A vehicle control total management system comprising: a driving pattern database configured to store data on a driving pattern and a shift pattern for each road condition and data on an acceleration pedal usage pattern of the driver for
each traffic situation;  and a tone control unit configured to construct artificial intelligence by performing AI learning by inputting the data of the driving pattern database, recognizing the driving pattern based on a road type and real-time traffic
information received by using the artificial intelligence, and configured to set a target tone according to the recognized driving pattern.
 15.  The vehicle control total management system of claim 14, wherein: the tone control unit is c an artificial intelligence neural network by deep-learning the data of the database and recognizes the driving pattern based on the road type and
the real-time traffic information by using the constructed artificial intelligence neural network.
 16.  The vehicle control total management system of claim 14, wherein: the tone control unit is configured: to set the target tone so as to emphasize a quiet tone when the recognized driving pattern is a country road, to set the target tone so
as to emphasize a grand tone when the recognized driving pattern is a tunnel, and to set the target tone so as to emphasize a sporty tone when the recognized driving pattern is a highway.
 17.  The vehicle control total management system of claim 14, wherein: the tone control unit is configured to determine an engine order component for the target tone and controls a speaker tone of a vehicle to match the target tone by using the
extracted engine order component.
 18.  The vehicle control total management system of claim 17, wherein: the tone control unit is configured to determine a grade and a level of the engine order component generated by vibration of an engine of the vehicle, and the engine order
component is a physical phenomenon occurring in a rotating engine.
 19.  The vehicle control total management system of claim 18, wherein the tone control unit is configured to select and arrange an order influencing the target tone among the extracted engine vibration order components, wherein the tone control
unit is configured to determine an amplification level of the arranged order component, to determine a characteristic of the engine tone, and to amplify an output amplitude of the order component through signal processing.
 20.  The vehicle control total management system of claim 17, wherein when the target tone emphasizes the grand tone, tone control in which the engine order component in a low-frequency band is emphasized is performed and when the target tone
emphasizes the sporty tone, tone control in which an engine integer order component is emphasized is performed.  "
"10,225,701","
     March 5, 2019
","Multicast expert system information dissemination system and method
"," Multicast expert system information dissemination systems and methods
     making use of artificial intelligence are provided. The systems and
     methods include a wireless device for receiving RF multicast information
     messages from a content provider wherein said information is descriptive
     of objects potentially of interest to users of the device. Received
     multicast messages may include information parameters about objects of
     potentially interest to the user. The wireless device also includes a
     knowledge base prestored in the wireless device descriptive of the user's
     level of interest in various objects. Artificial intelligence expert
     system control is used to evaluate a combination of the user's level of
     interest in the object information and distance from the user to the
     location where the object may be obtained. The artificial intelligence
     expert system derives a user advisory action index. In one embodiment the
     artificial intelligence may be implemented using fuzzy logic inference
     engine apparatus.
",H04L 12/1845 (20130101); H04W 4/18 (20130101); H04W 4/14 (20130101); H04W 4/025 (20130101); H04L 12/189 (20130101),H04W 24/00 (20090101); H04W 4/18 (20090101); H04W 4/14 (20090101); H04L 12/18 (20060101); H04W 4/02 (20180101),"[['\n3662401', '\nMay 1972'], ['\n4852001', '\nJuly 1989'], ['\n5301320', '\nApril 1994'], ['\n5745687', '\nApril 1998'], ['\n5768506', '\nJune 1998'], ['\n5862346', '\nJanuary 1999'], ['\n2936571', '\nAugust 1999'], ['\n5958071', '\nSeptember 1999'], ['\n6317058', '\nNovember 2001'], ['\n6334137', '\nDecember 2001'], ['\n6636884', '\nOctober 2003'], ['\n7024669', '\nApril 2006'], ['\n7408907', '\nAugust 2008'], ['\n7693486', '\nApril 2010'], ['\n7856360', '\nDecember 2010'], ['\n8229458', '\nJuly 2012'], ['\n8364171', '\nJanuary 2013'], ['\n8437776', '\nMay 2013'], ['\n8447331', '\nMay 2013'], ['\n8515459', '\nAugust 2013'], ['\n8566236', '\nOctober 2013'], ['\n8595824', '\nNovember 2013'], ['\n8626194', '\nJanuary 2014'], ['\n8635645', '\nJanuary 2014'], ['\n8639263', '\nJanuary 2014'], ['\n8826175', '\nSeptember 2014'], ['\n9024783', '\nMay 2015'], ['\n2002/0038228', '\nMarch 2002'], ['\n2002/0147642', '\nOctober 2002'], ['\n2005/0249119', '\nNovember 2005'], ['\n2007/0027583', '\nFebruary 2007'], ['\n2007/0281716', '\nDecember 2007'], ['\n2008/0064446', '\nMarch 2008'], ['\n2008/0133336', '\nJune 2008'], ['\n2008/0294690', '\nNovember 2008'], ['\n2010/0002075', '\nJanuary 2010'], ['\n2010/0207787', '\nAugust 2010'], ['\n2012/0036529', '\nFebruary 2012'], ['\n2012/0057716', '\nMarch 2012'], ['\n2012/0240185', '\nSeptember 2012'], ['\n2012/0246650', '\nSeptember 2012'], ['\n2012/0270573', '\nOctober 2012'], ['\n2013/0095864', '\nApril 2013'], ['\n2013/0122851', '\nMay 2013'], ['\n2013/0295901', '\nNovember 2013'], ['\n2014/0092751', '\nApril 2014'], ['\n2015/0161913', '\nJune 2015'], ['\n2015/0371659', '\nDecember 2015'], ['\n2016/0101786', '\nApril 2016'], ['\n2016/0195856', '\nJuly 2016'], ['\n2017/0049785', '\nFebruary 2017']]",[0]," The embodiments of the invention in which an exclusive property or privilege is claimed are defined as follows:  1.  A specifically programmed mobile communication computer system, with at least
one specialized communication computer machine including artificial intelligence expert system decision making electronic capability, comprising: at least one RF multicast transceiver for receiving multicast information transmissions, a non-transient
memory having at least one portion for storing data and at least one portion for storing particular computer executable program code;  and at least one processor for executing the particular program code stored in the memory, wherein the particular
program code is configured to at least perform the following operations upon the execution: electronically receiving, by the specifically programmed mobile communication computer system, RF multicast addressed signals from a content provider, wherein the
RF multicast addressed signals include information descriptive of objects registered with said content provider by the user of the specifically programmed mobile communication computer system as being of interest to said user of the specifically
programmed mobile communication computer system;  electronically determining, by the specifically programmed mobile communication computer system whether the received multicast RF addressed signal is intended for reception by the specifically programmed
mobile communication computer system;  wherein the specifically programmed mobile communication computer system is configured to electronically decode the received multicast RF signal to determine information parameters about a registered object of
interest to the user;  wherein the information parameters about a registered object of interest to the user comprise one or more information parameters identifying the object location and one or more of the following: i) at least one first information
parameter identifying object type, ii) at least one second information parameter identifying object description, iii) at least one third information parameter identifying object price;  and electronically determining, h the specifically programmed mobile
communication computer system the location of the specifically programmed mobile communication computer system;  wherein the specifically programmed mobile communication computer system is configured to electronically determine the location of the
specifically programmed mobile communication computer system by using a location sensor;  wherein the computer-implemented system does not require content providers determination or knowledge of the location of a user of the specifically programmed
mobile communication computer system;  performing, by the specifically programmed mobile communication computer system, artificial intelligence expert system operations comprising at least the following: electronic analysis of the received registered
object information and object information prestored in said specifically programmed mobile communication computer system descriptive of the user's level of interest selected from ranges of possible levels of interest in that object;  electronic
generation of results of said analysis of the received registered object information and object information prestored in said specifically programmed mobile communication computer system descriptive of the user's level of interest selected from ranges of
possible levels of interest in that object;  electronic analysis to determine the distance or travel time between the location of said specifically programmed mobile communication computer system and the location where the object may be obtained wherein
the travel time may take into account factors such as road conditions, traffic congestion, police or other emergency alerts;  and, electronic generation of an electronic communication by the specifically programmed mobile communication computer system to
the user of the specifically programmed communication computer system;  wherein said electronic communication includes an advisory action index to advise the user of the specifically programmed mobile communication computer system of specific artificial
intelligence expert system derived advice of recommended user actions concerning the object, and wherein said advisory action index is based on an artificial intelligence expert system evaluation of a combination of the user's level of interest in the
object, the relative locations of or the travel time between the user and the object of interest and the information parameters determined by the specifically programmed mobile communication computer system, and further wherein artificial intelligence
expert systems decision making is based on expert input with multiple propositional expert system instructions defining multiple ranges of user level of interest in said object and multiple ranges of distance or travel time between the user and said
object and on expert knowledge and inference procedures defining multiple conditional parameter relationships;  and further wherein said artificial intelligence expert system decision making provides an integrated composite user advisory index indicating
advisability of additional inquiry or pursuit of said object based on user level of interest parameters and distance or travel time from user location to said object thereby reducing cognitive distraction to said user of said system by presenting said
composite advisory index to said user without requiring said user to make cognitive distractive evaluation of multiple combinations of multiple parameters in attempt to make a decision on the advisability of additional inquiry or pursuit of said object.
 2.  The system of claim 1 wherein the specifically programmed mobile communication computer system communicates with said content provider using the cellular telephone network.
 3.  The system of claim 2 wherein the specifically programmed mobile communication computer system is a cellular telephone.
 4.  The system of claim 2 wherein the specifically programmed mobile communication computer system comprises a laptop computer, tablet computer, PDA, iPad, a vehicle equipped with cellular telephone communications capability or other cellular
communication device.
 5.  The system of claim 1 wherein said transceiver receives said RF multicast signals from one or more cellular telephone towers in defined proximity and connected to the content provider through one or more telephone network switching or
control centers, service providers or aggregators.
 6.  The system of claim 5 wherein specifically programmed mobile communication computer systems are organized in one or more subgroups with common interests to receive multicast advisory messages intended for individual subgroups without any of
these systems disclosing their location to the content provider or other network switching centers, control centers, service providers or aggregators.
 7.  The system of claim 1 wherein in response to the advisory message the user may do nothing, request further information from the content provider regarding the advisory message, or travel to the location of the content provider to further
understand or take advantage of the opportunity presented in the advisory message.
 8.  The system of claim 1 where the specifically programmed mobile communication computer system comprises an information data base that may be stored locally in said system or may be accessible from remote servers or Internet cloud computing
and/or storage systems through the Internet or cellular telephone communication networks.
 9.  The system of claim 1 wherein the content provider is not informed of the location of said system.
 10.  The system of claim 1 wherein the user of said apparatus registers with the content provider to receive selected object information corresponding to objects of interest to the user.
 11.  The system of claim 1 wherein said artificial intelligence expert system control circuitry comprises fuzzy logic inference control circuitry.
 12.  A computer-implemented method executed by a specifically programmed mobile communication computer system, with at least one specialized communication computer machine including artificial intelligence expert system decision making
electronic capability, comprising: at least one RF multicast transceiver for receiving multicast information transmissions, a non-transient memory having at least one portion for storing data and at least for storing particular computer executable
program code;  and at least one processor for executing the particular program code stored in the memory, wherein the particular program code is configured to at least perform the following operations upon the execution: the step of electronically
receiving, by the specifically programmed mobile communication computer system, RF multicast addressed signals from a content provider, wherein the RF multicast addressed signals include information descriptive of objects registered with said content
provider by a user of the specifically programmed mobile communication computer system as being of interest to said user of the specifically programmed mobile communication computer system;  the step of electronically determining, by the specifically
programmed mobile communication computer system, whether the received multicast RF addressed signal is intended for reception by the specifically programmed mobile communication computer system;  wherein the specifically programmed mobile communication
computer system is configured to electronically decode the received multicast RF signal to determine information parameters about a registered object of interest to the user;  wherein the information parameters about a registered object of interest to
the user comprises one or more information parameters identifying the object location and one or more of the following: i) at least one first information parameter identifying object type, ii) at least one second information parameter identifying object
description, iii) at least one third information parameter identifying object price, and the step of electronically determining, by the specifically programmed mobile communication computer system the location of the specifically programmed mobile
communication computer system;  wherein the specifically programmed mobile communication computer method electronically determines the location of the specifically programmed mobile communication computer system by using a location sensor;  wherein the
computer-implemented method does not require content providers determination or knowledge of the location of a user of the specifically programmed mobile communication computer system;  the step of performing, by the specifically programmed mobile
communication computer system, artificial intelligence expert system operations comprising at least the following: electronic analysis of the received registered object information with object information prestored in said specifically programmed mobile
communication computer system descriptive of the user's level of interest selected from ranges of possible levels of interest in that object;  electronic generation of results of said analysis of the received registered object information and object
information prestored in said specifically programmed mobile communication computer system descriptive of the user's level of interest selected from ranges of possible levels of interest in that object;  electronic analysis to determine the distance or
travel time between the location of said specifically programmed mobile communication computer system and the location of the where the registered object may be obtained wherein the travel time may take into account factors such as road conditions,
traffic congestion, police or other emergency alerts;  and, electronic generation of an electronic communication by the specifically programmed mobile communication computer system to the user of the specifically programmed communication computer system; wherein said electronic communication includes an advisory action index to advise the user of the specifically programmed mobile communication computer system of specific artificial intelligence expert system derived advice of recommended user actions
concerning the object, and wherein said advisory action index is based on an artificial intelligence expert system evaluation of a combination of the user's level of interest in the registered object, the relative locations of or the travel time between
the user and the object of interest and the information parameters determined by the specifically programmed mobile communication computer system, further wherein artificial intelligence expert system method decision making is based on expert input with
multiple propositional expert system instructions defining multiple ranges of user level of interest in said object and multiple ranges of distance or travel time between the user and said object and on expert knowledge and inference procedures defining
multiple conditional parameter relationships;  and, further wherein said artificial intelligence expert system decision making method provides an integrated composite user advisory index indicating advisability of additional inquiry or pursuit of said
object based on user level of interest parameters and distance or travel time from the user location to said object thereby reducing cognitive distraction to said user of said system by presenting said composite advisory index to said user without
requiring said user to make cognitive distractive evaluation of multiple combinations of multiple parameters in attempt to make a decision on the advisability of additional inquiry or pursuit of said object.
 13.  The method of claim 12 wherein the specifically programmed mobile communication computer system communicates with said content provider using the cellular telephone network.
 14.  The method of claim 13 wherein the specifically programmed mobile communication computer system is a cellular telephone.
 15.  The method of claim 13 wherein the specifically programmed mobile communication computer system comprises a laptop computer, tablet computer, PDA, iPad, a vehicle equipped cellular telephone communications capability or other cellular
communication device.
 16.  The method of claim 12 wherein said transceiver receives said RF multicast signals from one or more cellular telephone towers in defined proximity and connected to the content provider through one or more telephone network switching or
control centers.
 17.  The method of claim 16 wherein specifically programmed mobile communication computer systems are organized in one or more subgroups with common interests to receive multicast advisory messages intended for individual subgroups without any
of these systems disclosing their location to the content provider or other network switching centers, control centers, service providers or aggregators.
 18.  The method of claim 12 wherein in response to the advisory message the user may do nothing, request further information from the content provider regarding the advisory message, or travel to the location of the content provider to further
understand or take advantage of the opportunity presented in the advisory message.
 19.  The method of claim 12 where the specifically programmed mobile communication computer system comprises an information data base that may be stored locally in said system or may be accessible from remote servers or Internet cloud computing
and/or storage systems through the Internet or cellular telephone communication networks.
 20.  The method of claim 12 wherein the content provider is not informed of the location of said system.
 21.  The method of claim 12 wherein the user of said method registers with the content provider to receive selected object information corresponding to objects of interest to the user.
 22.  The method of claim 12 wherein said artificial intelligence expert system control circuitry comprises fuzzy logic expert inference engine control circuitry.  "
"10,229,679","
     March 12, 2019
","Natural language user interface for computer-aided design systems
"," A natural language user interface for computer-aided design systems (CAD)
     comprises a natural language command module including a parser, language
     database and a CAD model analyzer, and a natural language server module
     including a second, increased capability parser, a second, preferably
     larger language database and a CAD context database. The CAD model
     analyzer analyzes and retrieves associated CAD model information related
     to a parsed voice command and the CAD context database provides specific
     CAD related contextual information to facilitate parsing and interpreting
     CAD specific commands. The natural language server program module may
     also include an artificial intelligence based query generator and
     communicate through a network or cloud with resource providers such as
     third party market places or suppliers to generate queries for retrieval
     of third party supplied information necessary to respond to or execute
     CAD specific voice commands.
",G10L 15/1815 (20130101); G10L 15/22 (20130101); G10L 15/1822 (20130101); G05B 19/4093 (20130101); G06F 30/00 (20200101); G10L 15/00 (20130101); G06F 3/0481 (20130101); G10L 2015/223 (20130101),G01L 15/00 (20060101); G10L 15/22 (20060101); G10L 15/18 (20130101); G06F 17/50 (20060101); G06F 3/0481 (20130101),"[['\n4495559', '\nJanuary 1985'], ['\n5117354', '\nMay 1992'], ['\n5465221', '\nNovember 1995'], ['\n5495430', '\nFebruary 1996'], ['\n5552995', '\nSeptember 1996'], ['\n5570291', '\nOctober 1996'], ['\n5655087', '\nAugust 1997'], ['\n5758328', '\nMay 1998'], ['\n5847971', '\nDecember 1998'], ['\n5870719', '\nFebruary 1999'], ['\n5937189', '\nAugust 1999'], ['\n6031535', '\nFebruary 2000'], ['\n6112133', '\nAugust 2000'], ['\n6295513', '\nSeptember 2001'], ['\n6341271', '\nJanuary 2002'], ['\n6343285', '\nJanuary 2002'], ['\n6611725', '\nAugust 2003'], ['\n6647373', '\nNovember 2003'], ['\n6701200', '\nMarch 2004'], ['\n6750864', '\nJune 2004'], ['\n6834312', '\nDecember 2004'], ['\n6836699', '\nDecember 2004'], ['\n6859768', '\nFebruary 2005'], ['\n6922701', '\nJune 2005'], ['\n6917847', '\nJuly 2005'], ['\n7006084', '\nFebruary 2006'], ['\n7058465', '\nJune 2006'], ['\n7079990', '\nJuly 2006'], ['\n7085687', '\nAugust 2006'], ['\n7089082', '\nAugust 2006'], ['\n7123986', '\nOctober 2006'], ['\n7134096', '\nNovember 2006'], ['\n7299101', '\nNovember 2007'], ['\n7305367', '\nDecember 2007'], ['\n7327869', '\nFebruary 2008'], ['\n7343212', '\nMarch 2008'], ['\n7359886', '\nApril 2008'], ['\n7366643', '\nApril 2008'], ['\n7369970', '\nMay 2008'], ['\n7418307', '\nAugust 2008'], ['\n7467074', '\nDecember 2008'], ['\n7496487', '\nFebruary 2009'], ['\n7496528', '\nFebruary 2009'], ['\n7499871', '\nMarch 2009'], ['\n7523411', '\nApril 2009'], ['\n7526358', '\nApril 2009'], ['\n7529650', '\nMay 2009'], ['\n7565139', '\nJuly 2009'], ['\n7565223', '\nJuly 2009'], ['\n7567849', '\nJuly 2009'], ['\n7568155', '\nJuly 2009'], ['\n7571166', '\nAugust 2009'], ['\n7574339', '\nAugust 2009'], ['\n7590466', '\nSeptember 2009'], ['\n7590565', '\nSeptember 2009'], ['\n7603191', '\nOctober 2009'], ['\n7606628', '\nOctober 2009'], ['\n7630783', '\nDecember 2009'], ['\n7656402', '\nFebruary 2010'], ['\n7689936', '\nMarch 2010'], ['\n7733339', '\nJune 2010'], ['\n7747469', '\nJune 2010'], ['\n7748622', '\nJuly 2010'], ['\n7761319', '\nJuly 2010'], ['\n7822682', '\nOctober 2010'], ['\n7836573', '\nNovember 2010'], ['\n7840443', '\nNovember 2010'], ['\n7908200', '\nMarch 2011'], ['\n7957830', '\nJune 2011'], ['\n7979313', '\nJuly 2011'], ['\n7993140', '\nAugust 2011'], ['\n8000987', '\nAugust 2011'], ['\n8024207', '\nSeptember 2011'], ['\n8140401', '\nMarch 2012'], ['\n8170946', '\nMay 2012'], ['\n8175933', '\nMay 2012'], ['\n8180396', '\nMay 2012'], ['\n8209327', '\nJune 2012'], ['\n8239284', '\nAugust 2012'], ['\n8249329', '\nAugust 2012'], ['\n8271118', '\nSeptember 2012'], ['\n8275583', '\nSeptember 2012'], ['\n8295971', '\nOctober 2012'], ['\n8417478', '\nApril 2013'], ['\n8441502', '\nMay 2013'], ['\n8515820', '\nAugust 2013'], ['\n8554250', '\nOctober 2013'], ['\n8571298', '\nOctober 2013'], ['\n8595171', '\nNovember 2013'], ['\n8700185', '\nApril 2014'], ['\n8706607', '\nApril 2014'], ['\n8768651', '\nJuly 2014'], ['\n8798324', '\nAugust 2014'], ['\n8806398', '\nAugust 2014'], ['\n8830267', '\nSeptember 2014'], ['\n8849636', '\nSeptember 2014'], ['\n8861005', '\nOctober 2014'], ['\n8874413', '\nOctober 2014'], ['\n8923650', '\nDecember 2014'], ['\n8977558', '\nMarch 2015'], ['\n9037692', '\nMay 2015'], ['\n9055120', '\nJune 2015'], ['\n9106764', '\nAugust 2015'], ['\n2001/0023418', '\nSeptember 2001'], ['\n2001/0047251', '\nNovember 2001'], ['\n2002/0065790', '\nMay 2002'], ['\n2002/0087440', '\nJuly 2002'], ['\n2002/0099579', '\nJuly 2002'], ['\n2002/0107673', '\nAugust 2002'], ['\n2002/0152133', '\nOctober 2002'], ['\n2003/0018490', '\nJanuary 2003'], ['\n2003/0069824', '\nApril 2003'], ['\n2003/0078846', '\nApril 2003'], ['\n2003/0139995', '\nJuly 2003'], ['\n2003/0149500', '\nAugust 2003'], ['\n2003/0163212', '\nAugust 2003'], ['\n2003/0172008', '\nSeptember 2003'], ['\n2003/0212610', '\nNovember 2003'], ['\n2003/0220911', '\nNovember 2003'], ['\n2004/0008876', '\nJanuary 2004'], ['\n2004/0113945', '\nJune 2004'], ['\n2004/0195224', '\nOctober 2004'], ['\n2005/0055299', '\nMarch 2005'], ['\n2005/0125092', '\nJune 2005'], ['\n2005/0144033', '\nJune 2005'], ['\n2005/0171790', '\nAugust 2005'], ['\n2005/0251478', '\nNovember 2005'], ['\n2005/0273401', '\nDecember 2005'], ['\n2006/0085322', '\nApril 2006'], ['\n2006/0185275', '\nAugust 2006'], ['\n2006/0253214', '\nNovember 2006'], ['\n2007/0016437', '\nJanuary 2007'], ['\n2007/0067146', '\nMarch 2007'], ['\n2007/0073593', '\nMarch 2007'], ['\n2007/0112635', '\nMay 2007'], ['\n2007/0198231', '\nAugust 2007'], ['\n2008/0120086', '\nMay 2008'], ['\n2008/0183614', '\nJuly 2008'], ['\n2008/0269942', '\nOctober 2008'], ['\n2008/0281678', '\nNovember 2008'], ['\n2009/0058860', '\nMarch 2009'], ['\n2009/0208773', '\nAugust 2009'], ['\n2009/0299799', '\nDecember 2009'], ['\n2009/0319388', '\nDecember 2009'], ['\n2011/0040542', '\nFebruary 2011'], ['\n2011/0047140', '\nFebruary 2011'], ['\n2011/0209081', '\nAugust 2011'], ['\n2011/0213757', '\nSeptember 2011'], ['\n2012/0016678', '\nJanuary 2012'], ['\n2012/0072299', '\nMarch 2012'], ['\n2012/0230548', '\nSeptember 2012'], ['\n2012/0316667', '\nDecember 2012'], ['\n2013/0055126', '\nFebruary 2013'], ['\n2013/0097259', '\nApril 2013'], ['\n2013/0100128', '\nApril 2013'], ['\n2013/0138529', '\nMay 2013'], ['\n2013/0144566', '\nJune 2013'], ['\n2013/0166470', '\nJune 2013'], ['\n2013/0218961', '\nAugust 2013'], ['\n2013/0293580', '\nNovember 2013'], ['\n2013/0297320', '\nNovember 2013'], ['\n2013/0297460', '\nNovember 2013'], ['\n2013/0311914', '\nNovember 2013'], ['\n2013/0325410', '\nDecember 2013'], ['\n2014/0042136', '\nFebruary 2014'], ['\n2014/0067333', '\nMarch 2014'], ['\n2014/0075342', '\nMarch 2014'], ['\n2014/0098094', '\nApril 2014'], ['\n2014/0157579', '\nJune 2014'], ['\n2014/0207605', '\nJuly 2014'], ['\n2014/0229316', '\nAugust 2014'], ['\n2014/0279177', '\nSeptember 2014'], ['\n2014/0379119', '\nDecember 2014'], ['\n2015/0055085', '\nFebruary 2015'], ['\n2015/0066189', '\nMarch 2015'], ['\n2015/0127480', '\nMay 2015'], ['\n2015/0234377', '\nAugust 2015']]",[0]," What is claimed is:  1.  A non-transitory machine-readable storage medium containing machine-executable instructions for performing a method of providing a natural language interface for a
computer-aided design (CAD) system, said machine-executable instructions comprising: a first set of machine-executable instructions for automatedly receiving, via an input device of a computing device, a user selection through a graphical user interface
(GUI) of a particular location of a displayed CAD model;  a second set of machine-executable instructions for automatedly receiving, via the computing device, a user voice input comprising a plurality of words and referring to the particular location;  a
third set of machine-executable instructions for automatedly, via the computing device, parsing the user voice input;  a fourth set of machine-executable instructions for automatedly, via the computing device, determining a meaning for the parsed user
voice input, the meaning including a reference to the particular location, one or more words associated with one or more program actions, and one or more words associated with one or more command topics, each of the particular location, program actions,
and command topics related to a specific item referred to in the voice input;  a fifth set of machine-executable instructions for automatedly, via the computing device and based on the determined meaning, identifying information comprising two or more
specifications for the specific item via a CAD context database, wherein the specifications comprise two or more of: a diameter, a length, a depth, a thickness, and a material;  a sixth set of machine-executable instructions for automatedly, via the
computing device, assembling a query for additional information based on the meaning, the identified information, and the CAD model;  a seventh set of machine-executable instructions for automatedly, via the computing device, providing the assembled
query to a resource provider service;  an eighth set of machine-executable instructions for automatedly, via the computing device, receiving the additional information from the resource provider service, the additional information comprising information
regarding at least one CAD model of the specific item to be placed at the particular location selected as a function of the meaning, the identified information, and the CAD model;  a ninth set of machine-executable instructions for automatedly, via the
computing device, prompting the user for missing information determined as a function of the additional information based on whether the specific item is compatible with one or more aspects of the CAD model when placed at the particular location;  a
tenth set of machine-executable instructions for automatedly, via the computing device, receiving a response from the user to the prompt for missing information;  an eleventh set of machine-executable instructions for automatedly, via the computing
device, providing a command to at least one CAD program module based on the additional information and the response from the user to the prompt for missing information;  and a twelfth set of machine-executable instructions for automatedly, via the
computing device, updating the GUI with respect to the CAD model based on the command.  "
"10,230,880","
     March 12, 2019
","LED light has built-in camera-assembly for colorful digital-data under
     dark environment
"," A LED Light device for house or stores or business application having
     built-in camera-assembly is powered by AC or-and DC power source for a
     lamp-holder, LED bulb, security light, flashlight, car torch light,
     garden, entrance door light or other indoor or outdoor LED light device
     connected to power source by (1) prongs or (2) male-base has conductive
     piece can be inserted into a female receiving-piece which connect with
     power source or (3) wired or AC-plug wires. The device has built-in
     camera-assembly has plurality functions to make different products and
     functions. The LED light device has at least one of (a) camera or DV
     (digital video) to take minimum MP4 or 4K, 8K image or photos, (b)
     digital data memory kits or cloud storage station, (c) wireless
     connection kits, Bluetooth or USB set for download function, (d) MCU or
     CPU or IC with circuit with desired motion sensor/moving
     detector(s)/other sensor, (e) camera-assembly for connecting Wi-Fi, Wi-Fi
     extend, or-and 3G/4G/5G internet or network or even settle-lite channel,
     (f) wireless-system to transmit or-and receiving wireless signal, (g)
     people had download APP or other platform incorporated with pre-programed
     or even AI (artificial intelligence) software to operate one or more of
     area-selections function to make screen or image comparison, detection,
     identification, recognition, tracing, purchase, or other pre-program
     following works including but not limited to detect moving object(s),
     face recognition or personal identification or-and habit or-and crime
     comparison, purchase, (h) LED light source to offer sufficient brightness
     under dark environment for camera-assembly take colorful or-and audio
     data, (i) other electric or mechanical parts & accessories, (j) has
     moving detector and software built-in to make comparison to judge the
     moving object(s) from the preferred screen selected-areas; to get desired
     function(s) for the said LED light device. The said camera-assembly has
     desired camera, wireless-system, sensor(s), part(s) and related module or
     circuit(s) or interface or-and backup power, and (k) camera-assembly may
     in separated housing incorporated with all kind of existing non-built-in
     camera light device so people can upgrade the non-camera device to has
     built-in camera and digital device for out-of-date non-camera all kind of
     light device including security light.
",G03B 15/03 (20130101); F21V 23/0435 (20130101); F21K 9/232 (20160801); F21V 33/0052 (20130101); H04N 5/2354 (20130101); F21V 21/30 (20130101); H04N 7/186 (20130101); H04N 5/2257 (20130101); H04N 5/2256 (20130101); G08B 13/1966 (20130101); G03B 29/00 (20130101); H04N 7/188 (20130101); F21Y 2115/10 (20160801); G03B 2215/0567 (20130101),H04N 7/18 (20060101); F21V 23/04 (20060101); G03B 15/03 (20060101); F21V 33/00 (20060101); H04N 5/235 (20060101); H04N 5/225 (20060101); G08B 13/196 (20060101),"[['\n5107120', '\nApril 1992'], ['\n5946404', '\nAugust 1999'], ['\n6100803', '\nAugust 2000'], ['\n6270228', '\nAugust 2001'], ['\n6686952', '\nFebruary 2004'], ['\n6812970', '\nNovember 2004'], ['\n7321783', '\nJanuary 2008'], ['\n7330649', '\nFebruary 2008'], ['\n7346196', '\nMarch 2008'], ['\n8461991', '\nJune 2013'], ['\n2003/0197807', '\nOctober 2003'], ['\n2004/0212678', '\nOctober 2004'], ['\n2005/0085131', '\nApril 2005'], ['\n2007/0013513', '\nJanuary 2007'], ['\n2007/0257195', '\nNovember 2007'], ['\n2008/0151050', '\nJune 2008'], ['\n2008/0225120', '\nSeptember 2008'], ['\n2008/0252730', '\nOctober 2008'], ['\n2009/0059603', '\nMarch 2009'], ['\n2009/0290023', '\nNovember 2009'], ['\n2011/0134239', '\nJune 2011'], ['\n2012/0288114', '\nNovember 2012']]",[1]," I claim:  1.  A LED light device has camera-assembly consist of: at least one camera is any kind of camera or digital video (DV) to take digital data for colorful image under the dark environment
by LED light source to supply sufficient LED light illumination, and at least one of camera-assembly has circuit and parts to make connection and deliver digital data captured by camera(s) and functions;  (1) connect with wifi, wifi-extender, internet,
connect 3G/4G/5G to wireless transmitting camera captured image, audio, or other digital data to cloud and give notice to phone or computer owner under pre-program setting, and (2) cloud storage station(s) digital data handle by owner of phone or
computer after owner of phone or computer download APP with pre-program software or-and AI software for comparison, recognition, tracking, payment, financial operation, other pre-program work(s), and (4) owner of phone or computer make screen or image
analysis and desired setting, adjust, reply, choice, control the said LED source(s) or-and camera or-and audio related functions, or-and (5) Owner of phone or computer connect through internet, or 3/4/5G, and wifi to wireless connection instruction back
to the said LED or-and camera to do preferred functions, the said desired following works is one or more work selected from analysis, comparison or calculation, identification, facial recognition, crime searching, financial account, tracking, built
relation, study habit from the screen or image captured by camera(s) of moving personal or merchandise, object(s) for desired purpose(s), the said owner of phone or computer down load APP or-and pre-programed software can handle one or more than one of
the said digital-data captured by camera including one or more image(s) or screen(s) or audio(s) to make image or screen selection-areas, or image-moving-detection, or comparison, tracking and other following work of moving personal or merchandise or
objects, where said camera-assembly for: (A) assembly with LED light source to form finished LED light has built-in camera-assembly product, or (B) the said camera-system has its own individual housing to sell individual for replacement or up-grade none
built-in camera light device to built-in camera light device, or (C) the said camera-assembly to replace out-of-date motion security light has any light source to upgrade to motion sensor security light has built-in camera-assembly, the said LED light
not for street or highway or airport or harbor application.
 2.  A LED light device has camera-assembly as claim 1, the said camera-assembly has wireless connection through Wi-Fi wireless transmitting the captured image, photo(s), video(s), or-and audio digital-data to cloud storage station and go through
the said APP and its software or-and AI to make one or more of function(s): (1) selection detected-area from screen or image to prevent non-necessary moving-object(s) detected and signal transmit to communication device, (2) comparison or detection or
tracking or calculation or other following work(s) basing on movements of personals, merchandise, products, objects, (3) software to get or-and analysis or-and take further following works for personal character(s) including face or body or walking or
shape recognition, (4) remote control or adjustment from APP software on screen or displayer or monitor for brightness, angle, timing, shooting angle, zooms, pixels or other functions of LED light source and camera-system, (5) setting, changing of
functions or features of LED light and camera-system.
 3.  A LED light device has camera-assembly as claim 1, the said the LED light device is one of LED motion security light and LED light source is trigger by motion or-and photo sensors and camera is trigger by desired motion sensor or moving
detector(s) to take at least one of photo, video, audio, or desired digital-data and wireless transmitting by Wi-Fi or-and Wi-Fi-extend to cloud storage-station under the pre-program software to get the comparison result from pre-selection screen-areas
or moving-detection area(s) and judge to wireless transmitting the pre-choice alarm or reminder signal to communication device owner or others.
 4.  A LED light device has camera-assembly as claim 1, the said LED light device is one of LED bulb and LED light source is turn on by preferred sensor or switch, and the camera-assembly is trigger by desired motion sensor or moving detector(s)
to take at least one of photo(s), video, image and wireless transmitting to cloud storage station by Wi-Fi or-and Wi-Fi-extend to go through pre-program software or-and AI to make one or plurality of screen selection-area(s) or area(s)-tracking or
area(s)-comparison or areas-identification for moving object, person, merchandise incorporated with pre-program software to make the identification, financial calculation, recording, recognition, identification, comparison or other following works and
get result under pre-programed setting to send out reminder or alarm signal to communication device to owner or others.
 5.  A LED light device has camera-assembly as claim 1, the said LED light device is one of LED motion security light or LED bulb or LED car torch light or LED garden light or LED entrance door light or LED doorbell light, LED indoor light device
which has preferred power source from AC or-and DC power source by wired, prong, AC plug-wire, AC-to-DC transformer, and each of application has desired switch, motion sensor, moving detectors, other sensors to turn on and turn off the said LED light
source, camera-assembly to get colorful digital data and also to connect with Wi-Fi, Wi-Fi-extend, cloud storage-station, 3G/4G/5G, APP with at least one of preferred following work selected from selection-area(s), function(s), comparison(s),
identification(s), payment(s), calculation(s), tracking(s), analysis(s), further procedures(s).
 6.  A LED light device has camera-assembly as claim 1, the said LED light device is a portable LED flashlight which has limited battery power source for none rechargeable or rechargeable batteries power so camera-assembly without wireless
transmitting or connection with Wi-Fi or wifi-extend capability and only can take desired combination digital-data including photos, video, audio to save into the camera or DV of captured digital-data save into moveable or downloadable storage-unit for
reply or display or shown on displayer or screen from storage-unit or memory cards including SD card or micro-SD card or others.
 7.  A LED light device has camera-assembly as claim 1, the said LED bulb has extendable and retractable bar, pole, tube, cylinder to arrange parts at least including the sensor(s), camera(s) which can overcome the block-object(s) which maybe a
glass shade, metal piece, recess-light surrounding wall because the sensor(s) or-and camera(s) head is located on extendable or retractable movable parts, and LED bulb has male bulb-base to assemble with female bulb-socket to build the electric-delivery.
 8.  A LED light device has camera-assembly as claim 1, the said camera-assembly has its own housing without light source inside the housing can assemble with any non-camera motion security light base's adjustable position screw-thread bar or
other assembly-joint bars and connect the non-camera motion sensor light its own light-source electric-wires to upgrade the said non-camera motion security light to become camera motion security light with old existing light source with proper circuit to
make desired brightness to take colorful digital-data.
 9.  A LED light device has camera-assembly as claim 1, the said the camera-assembly has its own individual-housing without light source inside the individual-housing which is individual camera-assembly can fit into non-camera motion sensor light
and sales by itself for universal to fit the non-camera existing motion security light device (may not LED light source).
 10.  A LED light device has camera-assembly as claim 1, the said LED light is LED bulb has extendable or bendable or twistable parts on front or rear or body to arrange sensor or detectors or camera-head(s) for overcome the block-object(s) which
has bulb-base to assemble with female bulb-socket to get power for both LED light source and camera-system to get sufficient LED light beam and desired colorful digital-data for indoor or outdoor under dark environment.
 11.  A LED light device has camera-assembly as claim 1, the said LED light is LED bulb which has built-in motion-sensor or moving-detector(s) can fit into any female bulb-base to become individual LED bulb has camera-assembly for colorful photos
or video or-and audio to take digital-data and send out signal through desired MCU or CPU or pre-program circuit and software to communication device owner.
 12.  A LED light device has camera-assembly as claim 1, the said moving detectors or motion sensor can be one or more than one detectors to detect the moving objects which is belong to hard-parts and non-related to the camera image or display
screen detection by software.
 13.  A LED light device has camera-assembly as claim 1, the said motion detector incorporated with desired software which capable to make following-works from camera captured image or screen image to apply pre-program or AI software to make the
detection basing on the 15 to 60 image per-second to make comparison or following work.
 14.  A LED light device has camera-assembly as claim 1, the said moving sensor is one or more than one of PIR type with one or more than one of area(s) fresnel-lens in flat or curved or dome shape.
 15.  A LED light device has camera-assembly as claim 1, the said camera system preferred take 60 images per second to reach the 4K grade so can easily to enlarge image still get enough pixel image to easily do following works.
 16.  A LED light device has camera-assembly as claim 1, the said APP incorporated with pre-program or AI software has all kind of setting, selection, adjustable the LED light construction, LED light source, camera, digital video, camera
construction for desired position, functions, brightness, color, pixel, orientation.
 17.  A LED light device has camera-assembly as claim 1, the said individual saleable camera-system has its safety certification so can sell individual for all kind of DC light source which has universal receiving-parts so can connect with
existing light device circuit to upgrade or replace existing motion sensor unit.
 18.  A LED light device has camera-assembly as claim 1, the said individual saleable camera-assembly which has audio parts including speaker or-and microphone or-and IC to wireless transmitting the audio to receiver(s).
 19.  A LED light having a built-in camera-assembly consist of: said LED light device has preferred LED light source(s) for emitting visible LED light beams offer sufficient brightness to camera-assembly to take colorful photo(s) or video(s) into
at least MP4 or 4K (60 image per second) or 8K format digital data with optional sound and other data under the dark environment, and said LED light device get power from AC-to-DC circuit to operate the DC LED light source, and optional circuitry and at
least one of a controller, switch, motion sensor or-and moving detector(s) or photo or other sensor, conductors, integrated circuit (IC), remote controller, and wireless connect system, communication assembly, and at least one of motion sensor including
PIR detector, or moving detectors with comparison functions by hardware detector(s), or by camera captured image or screen to make comparison, wherein, the said camera-assembly has (A) least one camera having function(s) for capturing images, data, or
sound at a desired shooting angle, distance, resolution, color, brightness, and sharpness in MP4 or more high-speed format to take photos or video for pre-determined time, period time, certain times, and (B) at least one MCU or CPU and software and
wireless-system hard parts to wireless transmit said images, photo, video, audio, sound into wireless cloud storage station and present to at least one of a communication device, computer device, phone, receiver device, display device which has
downloaded APP and pre-program software;  or-and display or replay or reply or answer or operate following works of cloud storage the images, data, or sound by the owner downloaded APP having pre-program software or by digital data device itself;  the
said LED light device having one or more than one functions select from: (1) Camera captured digital data go through the MCU, CPU, circuit, hardware or software to delivery by Wi-Fi or-and Wi-Fi extend or internet or 3/4/5G or desired wireless connect
system to cloud storage-station, and (2) camera-assembly wireless incorporate with download APP with software or AI software by owner of phone or computer or communication product to handle phone or computer's screen or image or digital data for moving
object(s) to do following work including select-areas, detection, comparison, analysis, calculation, facial recognition, tracing the said moving people, merchandise, object, or other following work under pre-programed software of download APP, or-and (3)
the owner of phone or computer or communication reply or give instruction to LED light source or camera-assembly from APP with software through wireless network, 3G or 4G or update wireless network, router, server(s) to wifi and received by said
camera-assembly to operate instruction from owner, (4) the said APP software can receiving or transmitting digital data including (i) image, photos, video, audio, message, alert, words, (ii) send out instruction, emails, line, skype, WhatsApp, or (iii)
handle following work selected from setting, adjust, selection, comparison, tracing, identification, facial recognition, calculation, financial, payment, detect moving people or products or objects, or-and (5) the said following work incorporate one of
APP or computer software to handle the moving object(s) behavior including detector, comparison, tracking, facial recognition, personal identification, purchase, crime comparison from colorful or HD 1080 or image took by all kind of camera equipment
which can connect with download App with software through 3G/4G/incoming 5G, internet or connect-system to communication device(s) including mobile phone, computer, monitor;  or (6) LED light device have multiple-ways communication assembly to talk,
hear, get other audio-effects by microphone or-and speaker through Wi-Fi or-and optional Wi-Fi-extend, internet, wireless network, 3G or 4G or update wireless network, router, server;  (7) LED light device while incorporate with related program(s)
software can selected moving detected-areas, compare selected-areas for moving or movement, or-and remote-control camera angle, position, orientation or-and selection, adjustment, changing setting, and talking or delivery audio-effects though LED light
device;  or (8) the LED light has motion or moving-unit including sensor/moving detector or other sensor which only has motion sensor/moving detector/or brightness or other sensor function and act as switch or comparison equipment just to turn on or turn
off all LED lighting or-and camera or-and digital data function(s), which motion or moving-unit can be (8-a) installed within camera-assembly individual housing so can sell individual or assembly on LED light wall-mounted base by join-parts just without
light source and installation kits, or (8-b) installed inside of LED light wall-mounted base with all other MPU or CPU hardware and software and camera-assembly which has wireless or transmitting to cloud-station and without its own housing, or (8-c) the
motion sensor or-and moving detector incorporated with all kind of camera install inside one individual housing to assembly on wall mounted base of said LED light device by screw-assembly and tighten kits, the said individual device has all functions
including camera, motion sensor, moving detectors, WI-FI, wireless communication, wireless delivery video, photo, audio, wireless connect to APP, 3G, 4G, 5G, cloud storage station, connect with preprogram APP has detected-areas or personal face
recognition or other comparison features for people, products, items, object to input to computer-system to get desired digital data and record and delivery desired signals for certain purpose;  and this kind of individual unit has any desired
combination except the light-source select from LED or the existing light source while the security light has no camera functions but has its own any of light-source (LED or any others);  or (9) LED light has camera-assembly and built-in with motion or
moving-unit is desired combination of motion sensor/moving detector(s) or other sensor unit which has separated own individual-housing not install inside LED light wall-mounted base, and (9-1) at least has built-in camera assembly has parts or all of
digital data operation including (9-1-1) communication to wireless Wi-Fi, 3G/4G/incoming 5G network, cloud storage station, APP or other programed software has optional screen or image-comparison to make sure not non-necessary movement so not send out
electric-signals for hundred times while trigger by cat, dog, street cars, or moving objects is no harm to residence or house or office, (9-1-2) deliver digital data to display, phone, computer, displayer, monitor functions has optional added screen
selection or-and screen-comparison or-and image-comparison for moving people, object, merchandise, products and send out pre-programed digital data or following pre-program(s) operation, (9-1-3) operate for other selected or added functions basing on all
kind of camera took colorful image or-and audio with pre-programed software to make comparison, identification, calculation, selection, payment, account management by related electric or mechanical parts and accessories so can fit the said separated
housing motion sensor or-and moving detectors or-and other sensor unit to non-camera LED light device including motion security light, or LED bulb, or car torch light, or flashlight or garden light or patio light not including street light to upgrade and
increase value and functions, said LED light device is connected with the power source by one or combination of (a) prongs, (b) conductive wire, (c) a conductive wire with a plug extending from the light device, (d) a USB adaptor, (e) a transformer
device, (f) a power fail backup power storage device (g) an inductor device for wireless charging inner power kits, (h) AC plug-wire, (i) male bulb-base assembly with female bulb-socket.
 20.  A LED light as claim in claim 19, where said LED light source and camera or DV is triggered by the motion or moving detector(s) sensor and light brightness sensor AT THE SAME TIME under the dark environment.
 21.  A LED light as claimed in claim 19, wherein said LED light is a LED bulb or Lamp holder includes an insert male-base arranged to be inserted into an existing light fixture or lamp-base female receiving end or socket or terminals.
 22.  A LED light as claimed in claim 19, wherein said light device is one LED lighting fixtures has camera-assembly and at least one non-lighting function controlled by MCU or CPU or software of said captured screen or image in addition to a
lighting function.
 23.  A LED light as claim 19, the said LED light device LED flasher light or LED strobe light which offer the sufficient light brightness good for take color photo and not big power consumption like plurality of night vision diode(s).
 24.  A LED light as claimed in claim 19, wherein the light device has extendable, retractable, foldable, or transformable body, housing, or parts with accessories that provide space to arrange at least one non-lighting function for (20-1)
wireless communication to make setting or adjustment, or (20-2) multiple-sides communications, control, conversation, adjust angle, focus, brightness, signal, or (6) other functions control by wireless technical including hardware or software including
APP or other update software incorporated with Wi-Fi, Wi-Fi-extend, 3G, 4G, 5G or future connection system or-and cloud storage station(s).
 25.  A LED light as claimed in claim 19, wherein said moving detector in hardware-parts or software for screen or image having more than one moving sensor head(s) or-and integrated circuit (IC) or MCU or CPU with pre-determined software
program(s) to detected and compare the status of camera-images or screen-image for moving objects.
 26.  A LED light as claimed in claim 19, wherein said device includes multiple cameras for capturing images of different locations, angle, orientation, height, position.
 27.  A LED light claimed in claim 19, wherein said LED light or individual separated camera-assembly includes;  (27-a) at least one of sensor or switch or trigger or software program to control the camera for capturing images and desired digital
data has MP4 or 4K or more higher rank digital data;  and (27-b) The said LED light or separated camera-assembly all parts get power by conductive male-base or conductive wires from (27-b-1) existing female bulb socket for AC or DC power, or (27-b-2)
built-in power fail or power-off backup DC batteries power, or (27-b-3) prong which insert into the outlet of wall or extension cord, (27-b-4) wired with electric signal or power, (27-b-5) AC plug-wire to get electric signal or power.
 28.  A LED light as claimed in claim 19, wherein said AC power source is an electrical utility power supply system for supplying electricity to homes, residences, and businesses.
 29.  A LED light as claimed in claim 19, wherein said DC power source is an electrical solar power supply system has built-inside energy storage device including batteries or rechargeable battery, capacitors, or battery, or DC power from any
AC-to-DC electric device including transformer, inverter, adaptors.
 30.  A LED light as claimed in claim 19, wherein said separated camera-assembly has built-in digital data device without light-source and not install within wall-mounted base;  and separate unit connect with power source has at least one or
preferred combination parts and function(s): (30-1) camera capture the MP4 or higher technical format digital-data, or-and (30-2) has MCU or CPU or IC to wireless transmit digital data to cloud storage station, or-and (30-3) has circuit, conductive piece
or conduct wire or conductive bulb-base to get power, or-and (30-4) wireless communication system to download, save, play or and image comparison, selection, identification, tracking, or other pre-programed desired functions, or-and (30-5) wireless
transmit through preferred wife, Wi-Fi-extend, 3G/4G/incoming 5G or higher technical network incorporated selected APP software for the data to communication device, computer, mobile phone, consumer electric products, laptop computer.
 31.  A LED light as claimed in claim 19, wherein said separated camera-assembly has built-in digital data device with its safety certification can assembled with market existing security light which has non-camera related functions so can sell
individually to consumer to upgrade non-camera security light.
 32.  A LED light as claimed in claim 19, wherein said separated camera-assembly has built-in digital data device with its safety certification can assembled within market existing non-camera security light can use (1) conductive wired, or
bulb-base, or (2) inductive or wireless or control to get, deliver or transmit the electric signal or digital-data (s) or power or drive the said light source set for pre-determined function(s), performance(s), light effect(s) or multiple-way
communications.
 33.  A LED light as claimed in claim 19, wherein said camera-assembly has built-in digital data device is a single unit not install on light device wall-mount base and without the LED light source has conductive wire, adaptor, plug, quickly
wiring kits or wireless communication kits to connect with LED light source or LED light circuit or LED light wall-mounted base for power or electric signal delivery, and sell individually to work with any other light source assembly.
 34.  A LED light as claimed in claim 19, wherein said discrete housing camera-assembly of the has built-in camera(s) can connect or assembly or join with the said light device by accessories selected from frame, installation plate, substrate by
screws, nail, hooks, glue or market available installation kits.
 35.  A individual discrete housing camera-assembly consisting of: the said camera-assembly trigger the said separated housing lighting-device which has preferred light source selected from Bulb, PAR38, CFL or LED while motion or moving sensor(s)
or moving detector(s) detected the moving objects turn on the light-device instantly under dark environment to let camera(s) to take colorful image, photo, video or-and audio digital data, and wherein said camera-assembly has parts and accessories
includes at least one camera having function(s) for capturing colorful images, data, or sound at a desired shooting angle, distance, resolution, color, brightness, and sharpness in MP4 or 4K or 8K video or-and audio digital data, and at least one of MCU
or CPU or pre-program software incorporate wireless transmitter through Wi-Fi or-and Wi-Fi-extend go through the said internet 3G/4G/5G or more high speed or technical network to deliver the camera captured digital data to cloud storage station and do
one or more of following work (1) inform owner of phone or computer or communication device, computer device, phone, receiver device, and display device, and (2) owner of phone or computer has download APP with software or AI software under desired
programs to handle the said captured digital data, and (3) a display or replay or reply or answer or give instruction relate to the camera(s) captured images, or sound after the owner of phone or computer apply the download APP with pre-programed
software or solid hardware finished one or more of following work related to all image or digital data shown on screen, including: (a) screen select-area(s), (b) screen-area(s) comparison, (c) moving detection, (d) person identification including facial
or walking or body, face recognition (e) selection-function including dial out phone, trigger alarm, conversation, (f) tracing moving person or merchandise or object(s), (g) purchase, amount calculation, deduct money from account, (h) behavior study,
built personal habits (i) tracking moving object(s), incorporate with more than one camera(s) (j) adjust, set, change of the said LED light-unit, light source, camera, communication, camera, sensor, wireless module or interface to desired requirement
then, the software program will wireless deliver the pre-set instruction or electric-signal to the from download APP with software though internet 3/4/5G to wifi and give to LED light unit or camera-assembly to let camera-assembly receiving signal and
make desired function related the said camera(s) captured image, photo, video, or-and audio.
 36.  A individual camera-assembly has built-in digital data device as claim 35, the said camera-assembly to assembly with any light device which can be any type including LED light-source, or LED flasher-light or other light-source or other
strobe-light which offer the sufficient light brightness good for take color photo and not big power consumption like plurality of night vision diode(s).
 37.  A individual camera-assembly has built-in digital data device as claim 35, the said camera-assembly to assembled with any other light device to help to take clear photos or video while power failure or power off, and the power is coming
from the unit's backup DC power source including batteries, or rechargeable batteries.
 38.  A individual camera-assembly has built-in digital data device as claim 15, the said DC power including the power fail backup batteries or rechargeable batteries or energy storage device to offer the power while power off or power failure
time.
 39.  A LED security light has built-in camera-assembly, consist of: at least one of camera-assembly has its own housing separated with main LED light device wall mounted base and LED light source, the said separated housing camera-assembly only
act as switch to turn on or turn off all LED lighting and camera and related digital data function including take colorful photo and go through inner MCU or CPU or IC or software to wireless transmitting the camera(s) captured digital-data go through
Wi-Fi or-and Wi-Fi-extend, 3/4/5G internet wireless transmit to cloud storage station under pre-programed software and owner of communication devices including phone or computer already download APP with pre-program software to make receiving camera
captured colorful image, and transmitting out signal or instruction back to camera-assembly after owner incorporate with APP with pre-program to finished desired one of more of operation, calculation, comparison, identification, recognition, setting,
tracing, detection of camera captured image through screen or displayer to send out pre-set electric signal or reminder or warning signal back to camera-assembly to make all setting, changing, conversation, send out message, call out phone or other works
of LED light source(s), video, audio, and camera(s), the said camera-assembly assemble with LED light source of LED light device mounted base by assembly set, joint set, screw-sets with adjustable arms or bar or tube or rotatable kit or adjustable kits,
the said LED security light device for indoor or outdoor application for wired or AC plug wire or quickly connector or conductive contactor to fixed on wall, or put on ground which get power from AC power source or DC power source or outside transformer
with proper circuit to get current for LED light source and camera-assembly all parts to make desired function.
 40.  A individual separated housing camera-assembly, consisting of: at least has one of motion sensor or moving detector hardware install within individual housing with other wireless transmitting and receiving, camera, sensor parts, and at
least one of built-in camera take colorful image or-and audio digital data at least MP4 format or 4K or 8K quality to operate one or more following function(s) including: (a) the LED light device has USB download set while camera-assembly has no wireless
system but has USB slot and memory card such as SD or micro SD card or others, or (b) the camera-assembly wireless transmitter camera captured digital data to cloud storage station and people incorporate people download APP with software to review and
make desired operation, or (c) the camera-assembly receiver people instruction or signal from download APP with software after people finish desired work from screen or displayer back to camera-assembly to do setting, changing, adjust, or other works, or
(d) the camera-assembly wireless transmitting from camera-assembly connect with Wi-Fi go through 3G/4G/incoming 5G internet to cloud storage station, and people had download APP with pre-programed software to show camera(s) captured colorful digital
data, and the camera-assembly wireless receiving people instruction basing on screen or display shown camera(s) captured digital data to select the areas, function, and finished all preferred operation and send back from APP with software through 3/4/5G
internet to wifi to camera-assembly to do setting, adjust, change, or desired work of LED light source, LED light unit, camera, or other audio or video parts, or (e) the said individual camera-assembly is replace the non-camera motion security light
device's simple motion sensor unit to upgrade non-camera security light become security light has built0in motion sensor or moving detector camera-assembly security light, (f) the said individual camera-assembly to assemble with LED light unit(s) on the
LED light device main base to become a finish security light has LED light source and motion-sensor or moving detector(s) built-in camera-assembly security light.
 41.  A LED door entrance light or security light device having camera-assembly consisting of: at least has one of camera-assembly unit install on LED light to turn on and turn off LED light source(s) and camera to active under predetermined
time, period of time, functions to get colorful image and sound or digital data under dark environment, and the said camera take at least MP4 format or 4K or 8K photo, video, or-and audio digital data and storage digital data device by memory kits for
LED device has no wireless interface or wireless-system device, or wireless deliver to cloud storage station while device has wireless system, (1) the said LED light device without wireless-system but has USB download set with USB slot and memory card
such as SD or micro SD card or others, or (2) the LED light device has wireless-system to wireless transmitter camera captured digital data to cloud storage station and people incorporate had download APP with software to review and make desired
operation of captured digital data, or (3) the LED light device receiver people instruction or signal from download APP with software after people finish desired work from screen or displayer back to camera-assembly to do setting, changing, adjust, or
other works, or (4) the LED light device Wireless transmitting digital data from camera-assembly connect with Wi-Fi go through 3G/4G/incoming 5G internet to cloud storage station, and people had download APP with pre-programed software to show camera(s)
captured colorful digital data, and The LED light device wireless receiving people instruction basing on screen or display shown camera(s) captured digital data to select the areas, function, and finished all preferred operation and send back from APP
with software through 3/4/5G internet to wifi to camera-assembly to do setting, adjust, change, or desired work of LED light source, LED light unit, camera, or other audio or video parts, or (5) The said individual camera-assembly is replace the
non-camera motion security light device's simple motion sensor unit to upgrade non-camera security light become security light has built0in motion sensor or moving detector camera-assembly security light, (6) The said individual camera-assembly to
assembled with LED light unit(s) on the LED light device main base to become a finish door-entrance or security light has LED light source and motion-sensor or moving detector(s) built-in camera-assembly security light.
 42.  A LED light security light has camera-assembly, consist of;  At least one camera is any kind of camera or digital video (DV) to take digital data for colorful image under the dark environment by LED light source to supply sufficient LED
light illumination, and at least one of camera-assembly has circuit and parts to make connection and deliver digital data captured by camera(s) and functions;  (1) connect with wifi, wifi-extender, internet, connect 3G/4G/5G to wireless transmitting
camera captured image, audio, or other digital data to cloud and give notice to phone or computer owner under pre-program setting, and (2) cloud storage station(s) digital data handle by owner of phone or computer after owner of phone or computer
download APP with pre-program software or-and AT software for comparison, recognition, tracking, payment, financial operation, other pre-program work(s), and (4) owner of phone or computer make screen or image analysis and desired setting, adjust, reply,
choice, control the said LED source(s) or-and camera or-and audio related functions, or-and (5) owner of phone or computer connect through internet, or 3/4/5G, and wifi to wireless connection instruction back to the said LED or-and camera to do preferred
functions, the said desired following works is one or more work selected from analysis, comparison or calculation, identification, facial recognition, crime searching, financial account, tracking, built relation, study habit from the screen or image
captured by camera(s) of moving personal or merchandise, object(s) for desired purpose(s), the said owner of phone or computer down load APP or-and pre-programed software can handle one or more than one of the said digital-data captured by camera
including one or more image(s) or screen(s) or audio(s) to make image or screen selection-areas, or image-moving-detection, or comparison, tracking and other following work of moving personal or merchandise or objects. 
"
"10,234,299","
     March 19, 2019
","Geo-location tracking system and method
"," A system and method that includes a user management device, a server, and
     executable instructions that provide geo-tracking routing from point to
     point in a geographical location and to geo-tracking of a user's progress
     by tracking user location via storing of waypoints along a route and
     developing a ""return home"" path for the user. The systems and methods
     described herein can be employed in a mapping system that may include
     various geo-tracking services, social networking services, and
     geo-tracking capabilities, and the determination of return paths, and
     utilizes Artificial Intelligence to map location, time, and user
     preferences. The method employs analysis of user trends and will
     recommend locations that the user will identify as useful. The user's
     interactive input will be age-agnostic.
",H04L 67/131 (20220501); H04W 4/40 (20180201); G06V 20/20 (20220101); H04W 4/021 (20130101); G01C 21/3476 (20130101); A63F 13/216 (20140902); G01C 21/20 (20130101); H04L 67/53 (20220501); A63F 2300/205 (20130101); H04W 4/029 (20180201); H04W 4/024 (20180201),H04W 24/00 (20090101); A63F 13/216 (20140101); H04W 4/40 (20180101); G01C 21/34 (20060101); H04W 4/021 (20180101); G06K 9/00 (20060101); H04L 29/06 (20060101); H04W 4/029 (20180101); H04L 29/08 (20060101); H04W 4/024 (20180101),"[['\n7353034', '\nApril 2008'], ['\n7912628', '\nMarch 2011'], ['\n7953549', '\nMay 2011'], ['\n8190362', '\nMay 2012'], ['\n8260549', '\nSeptember 2012'], ['\n8385964', '\nFebruary 2013'], ['\n8621374', '\nDecember 2013'], ['\n8930134', '\nJanuary 2015'], ['\n9070101', '\nJune 2015'], ['\n9116000', '\nAugust 2015'], ['\n9204251', '\nDecember 2015'], ['\n9319471', '\nApril 2016'], ['\n9538332', '\nJanuary 2017'], ['\n9599717', '\nMarch 2017'], ['\n9788156', '\nOctober 2017'], ['\n9940663', '\nApril 2018'], ['\n2004/0203860', '\nOctober 2004'], ['\n2013/0225205', '\nAugust 2013'], ['\n2014/0114561', '\nApril 2014'], ['\n2014/0180914', '\nJune 2014'], ['\n2015/0051994', '\nFebruary 2015']]",[0]," The invention claimed is:  1.  A system to track a user's geographical position and plot one or more return routes via waypoints, the system comprising: a user device operable to send and receive
requests, messages, and information over a communication network, the user device including a user interface capable of conveying requests, messages, and information to the user of the device;  and at least one application server and a data store that
includes hardware and software to integrate with the data store as needed to execute aspects of one or more applications for the user device, handling data access and business logic for the one or more applications, the application server operable to
provide access control services in cooperation with the data store to generate content comprising at least one from among text, graphics, audio, and video to be conveyed to the user via the user interface;  and executable instructions to be stored on the
user device and operable to generate digital identifiable waypoints to ensure path tracking and to establish location and to generate a return path to be conveyed to the user via the user interface, the executable instructions providing instructions to a
user for a route guidance that includes: (a) receiving route guidance instructions to one or more waypoints in a mapping system;  (b) in response to the instructions, determining a first specific waypoint of the one or more waypoints;  (c) generating an
indication for display to the user one or more instructions to enable navigating by the user on a first route to a location associated with a first waypoint;  (d) generating an indication for display to the user instructing the user to go to the first
waypoint;  (e) querying a mapping system to determine whether the user is located at the first waypoint;  (f) in response to determining the user has reached the first waypoint, generating an indication for display to the user that the first waypoint has
been reached;  (g) determining a second route for completion after the first waypoint has been successfully reached and the user is still on route, based on the received directions;  (h) generating an indication for display to the user instructions for
navigating to a second waypoint;  (i) querying the mapping system to determine whether the second waypoint has been reached;  (j) repeating (h) and (j) for additional waypoints.
 2.  The system of claim 1 wherein weigh points include links to third-party websites associated with respective waypoints.
 3.  A method of providing instructions to a user for route guidance, the method comprising: (a) receiving route guidance instructions to one or more waypoints in a mapping system;  (b) in response to the instructions, determining a first
specific waypoint of the one or more waypoints;  (c) generating an indication for display to the user one or more instructions to enable navigating by the user on a first route to a location associated with a first waypoint;  (d) generating an indication
for display to the user instructing the user to go to the first waypoint;  (e) querying a mapping system to determine whether the user is located at the first waypoint;  (f) in response to determining the user has reached the first waypoint, generating
an indication for display to the user that the first waypoint has been reached;  (g) determining a second route for completion after the first waypoint has been successfully reached and the user is still on route, based on the received directions;  (h)
generating an indication for display to the user instructions for navigating to a second waypoint;  (i) querying the mapping system to determine whether the second waypoint has been reached;  (j) repeating (h) and (j) for additional waypoints.
 4.  The method of claim 3, further comprising generating an indication for display to the user instructing the user to perform a task at the first or subsequent waypoint.
 5.  The method of claim 3, wherein the waypoint comprises at least one from among a public place, a landmark, and an address.
 6.  The method of claim 3 wherein querying the mapping system comprises checking for route integrity and verifying that a route is complete before moving on to additional waypoints on the route.
 7.  The method of claim 6, wherein verifying that a route is complete comprises receiving a user input indicating that a waypoint has been reached or that a route is complete.
 8.  The method of claim 6 wherein verifying the route is complete comprises the user providing an indication in the form of a code or photographic image.
 9.  The method of claim 3 wherein determining a second route comprises skipping one or more routes and waypoints received in the instructions in response to user input.
 10.  The method of claim 3, wherein querying the mapping system to determine if the second waypoint has been reached includes checking for user return progress to the second waypoint from a third waypoint.
 11.  A method of providing real time route guidance to a user, comprising: (a) receiving instructions for performing return routing waypoints in a mapping system;  (b) in response to the instructions, determining an ordered list of waypoints,
including at least a first specific return point based on one or more of how visible the waypoint is and proximity to a well-known waypoint;  (c) allocating routes from the list of waypoints to one or more multiple users depending on allocation of a
return route from the list of waypoints;  (d) generating instructions to a user assigned to the first route for navigating to a location associated with a nearest waypoint from the list of waypoints;  (e) tracking user movement from a point of initiation
to waypoints on the list of waypoints;  (f) generating a prompt to the user to determine whether the first waypoint has been reached;  (g) verifying by comparative analysis of the route traveled by the user whether the user has completed a route to the
first waypoint and in response to determining the route is incomplete or the waypoint has not been reached, generating an indication for display to the user that the route is incomplete or that the waypoint has not been reached, and in response to
verifying the route is complete or the waypoint has been reached, generating an indication for display to the user that the route is complete or that the way point has been reached;  (h) repeating (d)-(g) for all waypoints on the list of waypoints.
 12.  A stand-alone system to track a user's geographical position and plot one or more return routes via waypoints, the system comprising: (a) a portable user device having an internal communication network operable to send and receive internal
requests, messages, and information on the user device, the user device including a user interface capable of conveying requests, messages, and information to the user of the device;  and (b) at least one application server and a data store that both
reside on the user device and include hardware and software to execute aspects of one or more applications on the user device, to handle data access and business logic for the one or more applications, the at least one application server operable to
provide access control services in cooperation with the data store to generate content comprising at least one from among text, graphics, audio, and video to be conveyed to the user via the user interface, and to provide selected communications internal
to the user device via the communications network for stand-alone path tracking and waypoint identification;  and (c) executable instructions to be stored on the user device and operable to generate identifiable waypoints to be used in user path
tracking, to establish user location, and to generate a return path to a point of initiation to be conveyed to the user via the user interface, the executable instructions enable providing real time route guidance to the user, including: (i) receiving
instructions for performing return routing waypoints in a mapping system;  (ii) in response to the instructions, determining an ordered list of return routing waypoints, including at least a first specific return routing waypoint based on one or more of
how visible the return routing waypoint is and proximity to a well-known waypoint;  (iii) allocating routes from the list of return routing waypoints to one or more multiple users depending on allocation of a return route from the list of return routing
waypoints;  (iv) generating instructions to a user assigned to the first route for navigating to a location associated with a nearest return routing waypoint from the list of return routing waypoints;  (v) tracking user movement from a point of
initiation to return routing waypoints on the list of waypoints;  (vi) generating a prompt to the user to determine whether the first return routing waypoint has been reached;  (vii) verifying by comparative analysis of the route traveled by the user
whether the user has completed a route to the first return routing waypoint and in response to determining the route is incomplete or the return routing waypoint has not been reached, generating an indication for display to the user that the route is
incomplete or that the return routing waypoint has not been reached, and in response to verifying the route is complete or the return routing waypoint has been reached, generating an indication for display to the user that the route is complete or that
the return routing waypoint has been reached;  (viii) repeating (d)-(g) for all return routing waypoints on the list of return routing waypoints.
 13.  The system of claim 12 wherein the communication network includes a radio communication circuit capable of global positioning system communications.  "
"10,235,602","
     March 19, 2019
","Machine learning artificial intelligence system for identifying vehicles
"," An artificial intelligence system for identifying attributes in an image.
     The system may include a processor in communication with a client device;
     and a storage medium. The storage medium may store instructions that,
     when executed, configure the processor to perform operations including:
     extracting first features; categorizing the first images in a first group
     or a second group; modifying first metadata associated with each image in
     the first images to include a binary label; calculating a classification
     function; classifying a second plurality of images using the
     classification function; extracting second features from the second
     images classified in the first group; categorizing the second images in
     the first group by attribute; calculating an attribute identification
     function that identifies attributes of the second images; and identifying
     at least one attribute associated with a client image using the attribute
     identification function, the client image being received from the client
     device.
",G06K 9/6267 (20130101); G06V 10/17 (20220101); G06V 20/20 (20220101); G06K 9/6256 (20130101); G06N 3/0481 (20130101); G06N 20/00 (20190101); G06N 3/0454 (20130101); G06F 16/583 (20190101); G06N 3/02 (20130101); G06V 10/40 (20220101); G06V 10/454 (20220101); G06N 3/084 (20130101); G06N 5/003 (20130101); G06K 9/6218 (20130101); G06V 2201/10 (20220101); G06N 20/20 (20190101),G06K 9/46 (20060101); G06N 99/00 (20190101); G06N 3/02 (20060101); G06K 9/62 (20060101),"[['\n8774465', '\nJuly 2014'], ['\n8873807', '\nOctober 2014'], ['\n8965112', '\nFebruary 2015'], ['\n9792530', '\nOctober 2017'], ['\n9842496', '\nDecember 2017'], ['\n2004/0258313', '\nDecember 2004'], ['\n2005/0249418', '\nNovember 2005'], ['\n2006/0030985', '\nFebruary 2006'], ['\n2006/0104504', '\nMay 2006'], ['\n2008/0310737', '\nDecember 2008'], ['\n2009/0273677', '\nNovember 2009'], ['\n2010/0080440', '\nApril 2010'], ['\n2010/0158356', '\nJune 2010'], ['\n2010/0322534', '\nDecember 2010'], ['\n2012/0230548', '\nSeptember 2012'], ['\n2013/0070986', '\nMarch 2013'], ['\n2013/0261880', '\nOctober 2013'], ['\n2014/0266803', '\nSeptember 2014'], ['\n2015/0019204', '\nJanuary 2015'], ['\n2015/0036919', '\nFebruary 2015'], ['\n2015/0100448', '\nApril 2015'], ['\n2015/0242708', '\nAugust 2015'], ['\n2015/0248586', '\nSeptember 2015'], ['\n2015/0310365', '\nOctober 2015'], ['\n2016/0070986', '\nMarch 2016'], ['\n2016/0259995', '\nSeptember 2016'], ['\n2017/0024641', '\nJanuary 2017'], ['\n2017/0196497', '\nJuly 2017'], ['\n2017/0221229', '\nAugust 2017'], ['\n2017/0228616', '\nAugust 2017'], ['\n2017/0294118', '\nOctober 2017'], ['\n2017/0329755', '\nNovember 2017'], ['\n2018/0025392', '\nJanuary 2018'], ['\n2018/0129893', '\nMay 2018']]","[2, '11,361,358', '11,010,808']"," What is claimed is:  1.  A computer-implemented method for identifying attributes in an image of a vehicle, the method comprising: collecting images of vehicles from the online resource; 
classifying the collected images in a first group or a second group using a classifier function, the first group comprising images of vehicle exteriors and the second group comprising images of vehicle interiors;  modifying metadata associated with
images in the first group to indicate that the images in the first group display vehicle exteriors;  retrieving an original training data set of images and an original attribute identification function from a database;  generating an updated training
data set by adding the images in the first group to the original training data set;  calculating an updated attribution identification function based on the updated training data set and the original attribute identification function;  receiving a client
image of a vehicle from a client processor;  and identifying attributes in a client image of a vehicle using the updated attribute identification function.
 2.  A computer system for identifying attributes in an image of a vehicle, the system comprising: at least one processor in communication with an online resource and a database;  and at least one storage medium storing instructions that, when
executed, configure the processor to execute operations comprising: collecting images of vehicles from the online resource;  classifying the collected images in a first group or a second group using a classifier function, the first group comprising
images of vehicle exteriors and the second group comprising images of vehicle interiors;  modifying metadata associated with images in the first group to indicate that the images in the first group display vehicle exteriors;  retrieving an original
training data set of images and an original attribute identification function from the database;  generating an updated training data set by adding the images in the first group to the original training data set;  calculating an updated attribution
identification function based on the updated training data set and the original attribute identification function;  receiving a client image of a vehicle from a client processor;  and identifying attributes in the client image using the updated attribute
identification function.
 3.  The computer system of claim 2 wherein the operations further comprise generating a modified graphical user interface overlaying the identified attributes in the image received from the client processor.
 4.  The computer system of claim 2, wherein generating an updated training data set comprises generating augmented images based on the images in the first group by applying filters to the images in the first group, the filters being associated
with a random function.
 5.  The computer system of claim 4, wherein: the filters comprise a zoom filter and a rotation filter;  and the random function determines a zoom percentage of the zoom filter and a rotation angle of the rotation filter.
 6.  The computer system of claim 4, wherein generating augmented images comprises: determining a difference measurement between the augmented images and the images in the first group;  and discarding the augmented images when the difference
measurement is below a threshold.
 7.  The computer system of claim 2, wherein calculating an updated attribution identification function comprises extracting features of the images using a pre-trained convolutional neural network.
 8.  The computer system of claim 2, wherein the classifier function comprises a logistic regression classifier based on clustered images of vehicle interiors and vehicle exteriors.
 9.  The computer system of claim 2, wherein: the online resource comprises a website of at least one of a car dealer or a car manufacturer;  and the identified attributes comprise a vehicle make and a vehicle model.
 10.  The computer system of claim 9, wherein the operations further comprise: querying the online resource to determine a cost and a condition associated with the identified attributes in the client image;  and generating a graphical user
interface overlaying the identified attributes, the cost, and the condition.
 11.  The computer system of claim 10, wherein the operations further comprise: querying the online resource for availability of vehicles with the identified attributes;  receiving a location of the client processor;  and generating a graphical
user interface when the online resource indicates availability of vehicles with the identified attributes, the graphical user interface displaying at least one of a map to the car dealer or an identification of the car manufacturer associated with the
online resource.
 12.  The computer system of claim 2, wherein collecting images comprises identifying an occurrence of changes in the online resource;  and initializing a web scraper configured to collect images.
 13.  The computer system of claim 2, wherein the original attribute identification function comprises a convolutional neural network;  and calculating an updated attribute identification function comprises updating at least one of activation
functions or synapsis weights of the convolutional neural network.
 14.  The computer system of claim 13, wherein the convolutional neural network comprises a multi-output network comprising layers to identify a vehicle year and a vehicle trim level.
 15.  A non-transitory computer-readable medium storing instructions that, when executed by a processor, cause the processor to operate a system for identifying attributes in an image of a vehicle, the operations comprising: collecting images of
vehicles from an online resource;  classifying the collected images in a first group or a second group using a classifier function, the first group comprising images of vehicle exteriors and the second group comprising images of vehicle interiors; 
modifying metadata associated with images in the first group to indicate that the images in the first group display vehicle exteriors;  retrieving an original training data set of images and an original attribute identification function from a database; 
generating an updated training data set by adding the images in the first group to the original training data set;  calculating an updated attribution identification function based on the updated training data set and the original attribute
identification function;  receiving a client image of a vehicle from a client processor;  and identifying attributes in a client image using the updated attribute identification function.
 16.  The non-transitory computer-readable medium of claim 15, wherein the operations further comprise generating a modified graphical user interface overlaying the identified attributes in the image received from the client processor.
 17.  The non-transitory computer-readable medium of claim 15, wherein generating an updated training data set comprises generating augmented images based on the images in the first group by applying filters to the images in the first group, the
filters being associated with a random function.
 18.  The non-transitory computer-readable medium of claim 17, wherein the filters comprise a zoom filter and a rotation filter;  and the random function determines a zoom percentage of the zoom filter and a rotation angle of the rotation filter.
 19.  The non-transitory computer-readable medium of claim 15, wherein the classifier function comprises a logistic regression classifier based on clustered images of vehicle interiors and vehicle exteriors. 
"
"10,235,734","
     March 19, 2019
","Translation of artificial intelligence representations
"," Techniques for translating graphical representations of domain knowledge
     are provided. In one example, a computer-implemented method comprises
     receiving, by a device operatively coupled to a processor, a graphical
     representation of domain knowledge. The graphical representation
     comprises information indicative of a central concept and at least one
     chain of events associated with the central concept. The
     computer-implemented method further comprises translating, by the device,
     the graphical representation into an artificial intelligence planning
     problem. The artificial intelligence planning problem is expressed in an
     artificial intelligence description language. The translating comprises
     parsing the graphical representation into groupings of terms. A first
     grouping of terms of the grouping of terms comprises an event from the at
     least one chain of events and a second grouping of terms of the grouping
     of terms comprises the information indicative of the central concept. The
     computer-implemented method also comprises validating, by the device, the
     artificial intelligence planning problem.
",G06N 5/022 (20130101); G06N 5/02 (20130101); G06T 1/20 (20130101); G06F 16/532 (20190101); G06N 20/00 (20190101),G06T 1/20 (20060101); G06N 5/02 (20060101),"[['\n6012152', '\nJanuary 2000'], ['\n7006992', '\nFebruary 2006'], ['\n8082220', '\nDecember 2011'], ['\n9177060', '\nNovember 2015'], ['\n2007/0271263', '\nNovember 2007'], ['\n2013/0041712', '\nFebruary 2013'], ['\n2013/0144917', '\nJune 2013'], ['\n2014/0052494', '\nFebruary 2014'], ['\n2015/0339580', '\nNovember 2015'], ['\n2016/0321544', '\nNovember 2016']]",[0],
"10,241,104","
     March 26, 2019
","System for interacting with a cell
"," Disclosed is a system for interacting with a cell and further
     communicating over a communication network is provided. The system
     includes a controller, a frequency generator, a first electrode, and a
     cell-chip circuitry. The frequency generator provides modulated
     alternating electric field with variable frequency. The controller
     releases routing instructions and further communicates through the
     communication network. The first electrode receives alternating electric
     field charges from the controller. The cell-chip circuitry is
     capacitively coupled to the first electrode. The cell-chip circuitry
     includes a second electrode, a harvester, a processor, a pulser, an
     analog switch matrix, a bi-directional communication unit, a pit, an
     inherent artificial intelligence interpreter, a analyzer, a nano needle,
     a third electrode. The cell-chip circuitry measures the field strength of
     the received charges from the harvester and generates pulsed intervals
     depending upon the field strength. The cell-chip circuitry further
     measures and communicates data with the controller through the e-field.
     The pit receives the cell. The cell reacts to the e-field. The inherent
     artificial intelligence interpreter performs successive approximation to
     monitor the reaction on the cell inside the pit on receiving instructions
     from the controller. The analyzer measures analog values of the e-field
     on the cell under the command of the inherent artificial intelligence
     interpreter; the analyzer communicates data to the bi-directional
     communication unit. The nano needle is inserted in the pit for
     bi-directionally communicating the e-field on the cell, and the third
     electrode is configured to provide space for the pit and floats against
     the ground.
",A61N 1/40 (20130101); A61N 1/37282 (20130101); A61B 5/0538 (20130101); G01N 33/48728 (20130101); A61N 1/025 (20130101); G01N 27/60 (20130101); H04L 67/10 (20130101),A61N 1/02 (20060101); G01N 33/487 (20060101); A61B 5/053 (20060101); G01N 27/60 (20060101); A61N 1/40 (20060101); A61N 1/372 (20060101); A61B 5/00 (20060101); G01N 27/06 (20060101); H04L 29/08 (20060101),"[['\n2004/0182707', '\nSeptember 2004'], ['\n2007/0155016', '\nJuly 2007'], ['\n2012/0004523', '\nJanuary 2012'], ['\n2013/0206720', '\nAugust 2013'], ['\n2016/0199853', '\nJuly 2016']]",[0]," The invention claimed is:  1.  A system for interacting with a cell in a living organism, further the system communicates over a communication network, the system comprising: a controller for
releasing routing instructions, further the controller communicates through the communication network;  a frequency generator connected to the controller for providing modulated alternating electric field with variable frequency;  a first electrode
configured to receive alternating electric field charges from the controller;  one or more cell-chip circuitries, wherein at least one cell-chip circuitry is capacitvely coupled to the first electrode, the one or more cell-chip circuitries comprising: a
second electrode coupled to the first electrode and configured to receive mirror alternating charges of the first electrode's alternating electric field charges;  a harvester converts the alternating electric field into DC power and further extracts
clock signals synchronized with the e-field alternating electric field frequency;  a processor that processes external operation commands and data received from the controller;  a pulser configured to measure alternating electric field strength of the
received alternating electric field charges from the harvester and generates pulsed intervals depending upon the alternating electric field strength;  an analog digital switch matrix configured to receive routing instructions from the controller for
making conditional temporary connections under the control of the processor, further the analog digital switch matrix routes pulsed intervals;  a bi-directional communication unit is configured to modulate commands and bi-directionally communicate data
with the controller through the alternating electric e-field;  a pit having insulated walls, wherein the pit is configured to receive the cell;  a third electrode configured to provide a space for the pit and floats against ground, wherein the pit is
formed through the space;  an inherent artificial intelligence interpreter is configured to perform successive approximation to monitor the cell inside the pit based on receiving instructions from the controller;  an analyzer for measuring analog values
of the alternating electric e-field on the cell under the command of the inherent artificial intelligence interpreter, the analyzer communicates data to the bi-directional communication unit;  a charge chamber for switching polarized charges into the pit
to ionize the cell;  a nano needle is configured to be inserted through the third electrode and into the pit for bi-directionally communicating the alternating electric e-field on the cell;  and wherein, the charge chamber receives signal from the
inherent artificial intelligence interpreter to determine the amount of charge potential for treatment of the cell.
 2.  The system according to claim 1 wherein the nano needle is movable inside the pit towards the cell.
 3.  The system according to claim 2 wherein the movement of nano needle inside the living organism shreds the cell.
 4.  The system according to claim 1, wherein the cell-chip circuitry is configured to be inserted in the living organisms to examine the cell.
 5.  The system according to claim 4, wherein the charge chamber is configured to ionize the cell to prevent collagen from creating fibers towards a cancer cell inside the living organism.
 6.  The system according to claim 1 wherein the nano needle inside the pit communicates charges on the cell surface.
 7.  The system according to claim 1 wherein the pit is configured to receive at least one of markers and medicines.
 8.  The system according to claim 1, wherein the second electrode couples capacitvely to the controller to get energy, clock, and data from mirrored alternating charges.
 9.  The system according to claim 1 wherein each of the one or more cell-chip circuitries are connected via charge coupling.
 10.  The system according to claim 1 wherein the analog value is converted to a digital value by the inherent artificial intelligence interpreter.
 11.  The system according to claim 1 wherein the pulsed intervals are routed to the sub-circuits of the cell-chip circuitry.
 12.  The system according to claim 1 wherein further comprising a plurality of nano needles configured to electronically scan electrical images of a sample, further wherein the controller creates visual images from the scanned images.
 13.  The system according to claim 1 wherein the nano needle projects from the pit on the command of the inherent artificial intelligence interpreter to interact with the cell.
 14.  The system according to claim 1, wherein a second nano needle is transferable in the living organism, wherein the second nano needle further locks in the pit when the previous nano needle is released. 
"
"10,242,019","
     March 26, 2019
","User behavior segmentation using latent topic detection
"," The features relate to artificial intelligence directed compression of
     user event data based on complex analysis of user event data including
     latent feature detection and clustering. Further features are described
     for reducing the size of data transmitted during event processing data
     flows and devices such as card readers or point of sale systems. Machine
     learning features for dynamically determining an optimal compression as
     well as identifying targeted users and providing content to the targeted
     users based on the compressed data are also included.
",G06Q 20/4016 (20130101); G06F 16/23 (20190101); G06Q 30/0204 (20130101); G06Q 30/02 (20130101); G06Q 30/00 (20130101); G06F 40/216 (20200101); G06F 16/285 (20190101); G06F 16/1744 (20190101); G06F 40/30 (20200101); G06F 9/542 (20130101); G06N 7/005 (20130101); G06N 20/00 (20190101),G06F 17/27 (20060101); G06Q 20/38 (20120101),"[['\n3316395', '\nApril 1967'], ['\n4305059', '\nDecember 1981'], ['\n4371739', '\nFebruary 1983'], ['\n4398055', '\nAugust 1983'], ['\n4578530', '\nMarch 1986'], ['\n4617195', '\nOctober 1986'], ['\n4672149', '\nJune 1987'], ['\n4736294', '\nApril 1988'], ['\n4754544', '\nJuly 1988'], ['\n4774664', '\nSeptember 1988'], ['\n4775935', '\nOctober 1988'], ['\n4827508', '\nMay 1989'], ['\n4868570', '\nSeptember 1989'], ['\n4872113', '\nOctober 1989'], ['\n4876592', '\nOctober 1989'], ['\n4895518', '\nJanuary 1990'], ['\n4935870', '\nJune 1990'], ['\n4947028', '\nAugust 1990'], ['\n5025138', '\nJune 1991'], ['\n5025373', '\nJune 1991'], ['\n5034807', '\nJuly 1991'], ['\n5056019', '\nOctober 1991'], ['\n5060153', '\nOctober 1991'], ['\n5148365', '\nSeptember 1992'], ['\n5201010', '\nApril 1993'], ['\n5220501', '\nJune 1993'], ['\n5247575', '\nSeptember 1993'], ['\n5259766', '\nNovember 1993'], ['\n5262941', '\nNovember 1993'], ['\n5274547', '\nDecember 1993'], ['\n5297031', '\nMarch 1994'], ['\n5325509', '\nJune 1994'], ['\n5336870', '\nAugust 1994'], ['\n5341429', '\nAugust 1994'], ['\n5454030', '\nSeptember 1995'], ['\n5468988', '\nNovember 1995'], ['\n5504675', '\nApril 1996'], ['\n5528701', '\nJune 1996'], ['\n5555409', '\nSeptember 1996'], ['\n5563783', '\nOctober 1996'], ['\n5583763', '\nDecember 1996'], ['\n5590038', '\nDecember 1996'], ['\n5592560', '\nJanuary 1997'], ['\n5611052', '\nMarch 1997'], ['\n5615408', '\nMarch 1997'], ['\n5621201', '\nApril 1997'], ['\n5629982', '\nMay 1997'], ['\n5630127', '\nMay 1997'], ['\n5640551', '\nJune 1997'], ['\n5640577', '\nJune 1997'], ['\n5655129', '\nAugust 1997'], ['\n5659731', '\nAugust 1997'], ['\n5666528', '\nSeptember 1997'], ['\n5679176', '\nOctober 1997'], ['\n5689651', '\nNovember 1997'], ['\n5696907', '\nDecember 1997'], ['\n5704029', '\nDecember 1997'], ['\n5732400', '\nMarch 1998'], ['\n5737732', '\nApril 1998'], ['\n5739512', '\nApril 1998'], ['\n5745654', '\nApril 1998'], ['\n5748098', '\nMay 1998'], ['\n5754938', '\nMay 1998'], ['\n5768423', '\nJune 1998'], ['\n5771562', '\nJune 1998'], ['\n5774692', '\nJune 1998'], ['\n5774868', '\nJune 1998'], ['\n5774883', '\nJune 1998'], ['\n5778405', '\nJuly 1998'], ['\n5793972', '\nAugust 1998'], ['\n5797136', '\nAugust 1998'], ['\n5802142', '\nSeptember 1998'], ['\n5812840', '\nSeptember 1998'], ['\n5819226', '\nOctober 1998'], ['\n5822410', '\nOctober 1998'], ['\n5822750', '\nOctober 1998'], ['\n5822751', '\nOctober 1998'], ['\n5825884', '\nOctober 1998'], ['\n5828833', '\nOctober 1998'], ['\n5835915', '\nNovember 1998'], ['\n5844218', '\nDecember 1998'], ['\n5848396', '\nDecember 1998'], ['\n5864830', '\nFebruary 1999'], ['\n5870721', '\nFebruary 1999'], ['\n5875108', '\nFebruary 1999'], ['\n5875236', '\nFebruary 1999'], ['\n5878403', '\nMarch 1999'], ['\n5881131', '\nMarch 1999'], ['\n5884287', '\nMarch 1999'], ['\n5884289', '\nMarch 1999'], ['\n5893090', '\nApril 1999'], ['\n5905985', '\nMay 1999'], ['\n5912839', '\nJune 1999'], ['\n5915243', '\nJune 1999'], ['\n5924082', '\nJuly 1999'], ['\n5926800', '\nJuly 1999'], ['\n5930764', '\nJuly 1999'], ['\n5930774', '\nJuly 1999'], ['\n5930776', '\nJuly 1999'], ['\n5940812', '\nAugust 1999'], ['\n5950172', '\nSeptember 1999'], ['\n5950179', '\nSeptember 1999'], ['\n5956693', '\nSeptember 1999'], ['\n5963932', '\nOctober 1999'], ['\n5966695', '\nOctober 1999'], ['\n5974396', '\nOctober 1999'], ['\n5978780', '\nNovember 1999'], ['\n5995947', '\nNovember 1999'], ['\n6009415', '\nDecember 1999'], ['\n6014688', '\nJanuary 2000'], ['\n6018723', '\nJanuary 2000'], ['\n6021362', '\nFebruary 2000'], ['\n6026368', '\nFebruary 2000'], ['\n6029139', '\nFebruary 2000'], ['\n6029149', '\nFebruary 2000'], ['\n6029154', '\nFebruary 2000'], ['\n6038551', '\nMarch 2000'], ['\n6044357', '\nMarch 2000'], ['\n6058375', '\nMay 2000'], ['\n6061658', '\nMay 2000'], ['\n6061691', '\nMay 2000'], ['\n6064973', '\nMay 2000'], ['\n6064987', '\nMay 2000'], ['\n6064990', '\nMay 2000'], ['\n6070141', '\nMay 2000'], ['\n6070142', '\nMay 2000'], ['\n6073140', '\nJune 2000'], ['\n6073241', '\nJune 2000'], ['\n6088686', '\nJuly 2000'], ['\n6094643', '\nJuly 2000'], ['\n6098052', '\nAugust 2000'], ['\n6105007', '\nAugust 2000'], ['\n6115690', '\nSeptember 2000'], ['\n6115693', '\nSeptember 2000'], ['\n6119103', '\nSeptember 2000'], ['\n6121901', '\nSeptember 2000'], ['\n6128599', '\nOctober 2000'], ['\n6128602', '\nOctober 2000'], ['\n6128603', '\nOctober 2000'], ['\n6128624', '\nOctober 2000'], ['\n6134548', '\nOctober 2000'], ['\n6144957', '\nNovember 2000'], ['\n6151601', '\nNovember 2000'], ['\n6154729', '\nNovember 2000'], ['\n6178442', '\nJanuary 2001'], ['\n6182060', '\nJanuary 2001'], ['\n6198217', '\nMarch 2001'], ['\n6202053', '\nMarch 2001'], ['\n6208979', '\nMarch 2001'], ['\n6223171', '\nApril 2001'], ['\n6226408', '\nMay 2001'], ['\n6233566', '\nMay 2001'], ['\n6236977', '\nMay 2001'], ['\n6239352', '\nMay 2001'], ['\n6249770', '\nJune 2001'], ['\n6254000', '\nJuly 2001'], ['\n6256630', '\nJuly 2001'], ['\n6263334', '\nJuly 2001'], ['\n6263337', '\nJuly 2001'], ['\n6266649', '\nJuly 2001'], ['\n6278055', '\nAugust 2001'], ['\n6285983', '\nSeptember 2001'], ['\n6285987', '\nSeptember 2001'], ['\n6289252', '\nSeptember 2001'], ['\n6304860', '\nOctober 2001'], ['\n6304869', '\nOctober 2001'], ['\n6307958', '\nOctober 2001'], ['\n6311169', '\nOctober 2001'], ['\n6324524', '\nNovember 2001'], ['\n6330546', '\nDecember 2001'], ['\n6330575', '\nDecember 2001'], ['\n6334110', '\nDecember 2001'], ['\n6339769', '\nJanuary 2002'], ['\n6345300', '\nFebruary 2002'], ['\n6366903', '\nApril 2002'], ['\n6385594', '\nMay 2002'], ['\n6393406', '\nMay 2002'], ['\n6397197', '\nMay 2002'], ['\n6405173', '\nJune 2002'], ['\n6405181', '\nJune 2002'], ['\n6412012', '\nJune 2002'], ['\n6418436', '\nJuly 2002'], ['\n6424956', '\nJuly 2002'], ['\n6430539', '\nAugust 2002'], ['\n6442577', '\nAugust 2002'], ['\n6456979', '\nSeptember 2002'], ['\n6457012', '\nSeptember 2002'], ['\n6460036', '\nOctober 2002'], ['\n6496819', '\nDecember 2002'], ['\n6505176', '\nJanuary 2003'], ['\n6513018', '\nJanuary 2003'], ['\n6523022', '\nFebruary 2003'], ['\n6523041', '\nFebruary 2003'], ['\n6532450', '\nMarch 2003'], ['\n6542894', '\nApril 2003'], ['\n6543683', '\nApril 2003'], ['\n6567791', '\nMay 2003'], ['\n6574623', '\nJune 2003'], ['\n6597775', '\nJuly 2003'], ['\n6598030', '\nJuly 2003'], ['\n6601234', '\nJuly 2003'], ['\n6611816', '\nAugust 2003'], ['\n6615193', '\nSeptember 2003'], ['\n6615247', '\nSeptember 2003'], ['\n6622266', '\nSeptember 2003'], ['\n6623529', '\nSeptember 2003'], ['\n6631496', '\nOctober 2003'], ['\n6640215', '\nOctober 2003'], ['\n6651220', '\nNovember 2003'], ['\n6654727', '\nNovember 2003'], ['\n6658393', '\nDecember 2003'], ['\n6665715', '\nDecember 2003'], ['\n6687713', '\nFebruary 2004'], ['\n6708166', '\nMarch 2004'], ['\n6714918', '\nMarch 2004'], ['\n6735572', '\nMay 2004'], ['\n6748426', '\nJune 2004'], ['\n6757740', '\nJune 2004'], ['\n6766327', '\nJuly 2004'], ['\n6801909', '\nOctober 2004'], ['\n6804346', '\nOctober 2004'], ['\n6804701', '\nOctober 2004'], ['\n6807533', '\nOctober 2004'], ['\n6823319', '\nNovember 2004'], ['\n6836764', '\nDecember 2004'], ['\n6839682', '\nJanuary 2005'], ['\n6839690', '\nJanuary 2005'], ['\n6850606', '\nFebruary 2005'], ['\n6859785', '\nFebruary 2005'], ['\n6865566', '\nMarch 2005'], ['\n6873972', '\nMarch 2005'], ['\n6873979', '\nMarch 2005'], ['\n6901406', '\nMay 2005'], ['\n6910624', '\nJune 2005'], ['\n6915269', '\nJuly 2005'], ['\n6925441', '\nAugust 2005'], ['\n6959281', '\nOctober 2005'], ['\n6965889', '\nNovember 2005'], ['\n6970830', '\nNovember 2005'], ['\n6983379', '\nJanuary 2006'], ['\n6983478', '\nJanuary 2006'], ['\n6985882', '\nJanuary 2006'], ['\n6985887', '\nJanuary 2006'], ['\n6991159', '\nJanuary 2006'], ['\n6993493', '\nJanuary 2006'], ['\n6999941', '\nFebruary 2006'], ['\n7003504', '\nFebruary 2006'], ['\n7003792', '\nFebruary 2006'], ['\n7028052', '\nApril 2006'], ['\n7031945', '\nApril 2006'], ['\n7039176', '\nMay 2006'], ['\n7039607', '\nMay 2006'], ['\n7047251', '\nMay 2006'], ['\n7050982', '\nMay 2006'], ['\n7050986', '\nMay 2006'], ['\n7050989', '\nMay 2006'], ['\n7054828', '\nMay 2006'], ['\n7069240', '\nJune 2006'], ['\n7072963', '\nJuly 2006'], ['\n7076462', '\nJuly 2006'], ['\n7076475', '\nJuly 2006'], ['\n7082435', '\nJuly 2006'], ['\n7092898', '\nAugust 2006'], ['\n7117172', '\nOctober 2006'], ['\n7133935', '\nNovember 2006'], ['\n7136448', '\nNovember 2006'], ['\n7139734', '\nNovember 2006'], ['\n7143063', '\nNovember 2006'], ['\n7152053', '\nDecember 2006'], ['\n7165036', '\nJanuary 2007'], ['\n7165037', '\nJanuary 2007'], ['\n7184974', '\nFebruary 2007'], ['\n7185016', '\nFebruary 2007'], ['\n7191144', '\nMarch 2007'], ['\n7200602', '\nApril 2007'], ['\n7206768', '\nApril 2007'], ['\n7212995', '\nMay 2007'], ['\n7234156', '\nJune 2007'], ['\n7236950', '\nJune 2007'], ['\n7240059', '\nJuly 2007'], ['\n7249048', '\nJuly 2007'], ['\n7249114', '\nJuly 2007'], ['\n7263506', '\nAugust 2007'], ['\n7275083', '\nSeptember 2007'], ['\n7277869', '\nOctober 2007'], ['\n7277875', '\nOctober 2007'], ['\n7277900', '\nOctober 2007'], ['\n7283974', '\nOctober 2007'], ['\n7296734', '\nNovember 2007'], ['\n7308418', '\nDecember 2007'], ['\n7313538', '\nDecember 2007'], ['\n7313618', '\nDecember 2007'], ['\n7314166', '\nJanuary 2008'], ['\n7314167', '\nJanuary 2008'], ['\n7324962', '\nJanuary 2008'], ['\n7328169', '\nFebruary 2008'], ['\n7337133', '\nFebruary 2008'], ['\n7343149', '\nMarch 2008'], ['\n7346551', '\nMarch 2008'], ['\n7346573', '\nMarch 2008'], ['\n7360251', '\nApril 2008'], ['\n7366694', '\nApril 2008'], ['\n7367011', '\nApril 2008'], ['\n7370044', '\nMay 2008'], ['\n7373324', '\nMay 2008'], ['\n7376603', '\nMay 2008'], ['\n7376618', '\nMay 2008'], ['\n7376714', '\nMay 2008'], ['\n7379880', '\nMay 2008'], ['\n7383227', '\nJune 2008'], ['\n7386786', '\nJune 2008'], ['\n7392203', '\nJune 2008'], ['\n7392216', '\nJune 2008'], ['\n7395273', '\nJuly 2008'], ['\n7403923', '\nJuly 2008'], ['\n7403942', '\nJuly 2008'], ['\n7409362', '\nAugust 2008'], ['\n7418431', '\nAugust 2008'], ['\n7421322', '\nSeptember 2008'], ['\n7424439', '\nSeptember 2008'], ['\n7428509', '\nSeptember 2008'], ['\n7428526', '\nSeptember 2008'], ['\n7433855', '\nOctober 2008'], ['\n7444302', '\nOctober 2008'], ['\n7458508', '\nDecember 2008'], ['\n7467127', '\nDecember 2008'], ['\n7467401', '\nDecember 2008'], ['\n7472088', '\nDecember 2008'], ['\n7499868', '\nMarch 2009'], ['\n7505938', '\nMarch 2009'], ['\n7509117', '\nMarch 2009'], ['\n7512221', '\nMarch 2009'], ['\n7516149', '\nApril 2009'], ['\n7529698', '\nMay 2009'], ['\n7536329', '\nMay 2009'], ['\n7536346', '\nMay 2009'], ['\n7536348', '\nMay 2009'], ['\n7542993', '\nJune 2009'], ['\n7546266', '\nJune 2009'], ['\n7548886', '\nJune 2009'], ['\n7552089', '\nJune 2009'], ['\n7556192', '\nJuly 2009'], ['\n7562184', '\nJuly 2009'], ['\n7571139', '\nAugust 2009'], ['\n7575157', '\nAugust 2009'], ['\n7580856', '\nAugust 2009'], ['\n7581112', '\nAugust 2009'], ['\n7584126', '\nSeptember 2009'], ['\n7584146', '\nSeptember 2009'], ['\n7590589', '\nSeptember 2009'], ['\n7593893', '\nSeptember 2009'], ['\n7596512', '\nSeptember 2009'], ['\n7596716', '\nSeptember 2009'], ['\n7606778', '\nOctober 2009'], ['\n7610216', '\nOctober 2009'], ['\n7610243', '\nOctober 2009'], ['\n7610257', '\nOctober 2009'], ['\n7613671', '\nNovember 2009'], ['\n7620592', '\nNovember 2009'], ['\n7620596', '\nNovember 2009'], ['\n7623844', '\nNovember 2009'], ['\n7624068', '\nNovember 2009'], ['\n7653592', '\nJanuary 2010'], ['\n7653593', '\nJanuary 2010'], ['\n7657471', '\nFebruary 2010'], ['\n7657540', '\nFebruary 2010'], ['\n7668769', '\nFebruary 2010'], ['\n7668840', '\nFebruary 2010'], ['\n7672865', '\nMarch 2010'], ['\n7676410', '\nMarch 2010'], ['\n7676418', '\nMarch 2010'], ['\n7676751', '\nMarch 2010'], ['\n7686214', '\nMarch 2010'], ['\n7689494', '\nMarch 2010'], ['\n7689504', '\nMarch 2010'], ['\n7689505', '\nMarch 2010'], ['\n7690032', '\nMarch 2010'], ['\n7693787', '\nApril 2010'], ['\n7698163', '\nApril 2010'], ['\n7702550', '\nApril 2010'], ['\n7707059', '\nApril 2010'], ['\n7707102', '\nApril 2010'], ['\n7708190', '\nMay 2010'], ['\n7711635', '\nMay 2010'], ['\n7711636', '\nMay 2010'], ['\n7720846', '\nMay 2010'], ['\n7725300', '\nMay 2010'], ['\n7734523', '\nJune 2010'], ['\n7734539', '\nJune 2010'], ['\n7742982', '\nJune 2010'], ['\n7747480', '\nJune 2010'], ['\n7747559', '\nJune 2010'], ['\n7761379', '\nJuly 2010'], ['\n7761384', '\nJuly 2010'], ['\n7769998', '\nAugust 2010'], ['\n7778885', '\nAugust 2010'], ['\n7783515', '\nAugust 2010'], ['\n7783562', '\nAugust 2010'], ['\n7788147', '\nAugust 2010'], ['\n7788152', '\nAugust 2010'], ['\n7792732', '\nSeptember 2010'], ['\n7793835', '\nSeptember 2010'], ['\n7797252', '\nSeptember 2010'], ['\n7801811', '\nSeptember 2010'], ['\n7802104', '\nSeptember 2010'], ['\n7805345', '\nSeptember 2010'], ['\n7805362', '\nSeptember 2010'], ['\n7814004', '\nOctober 2010'], ['\n7818231', '\nOctober 2010'], ['\n7822665', '\nOctober 2010'], ['\n7827115', '\nNovember 2010'], ['\n7835983', '\nNovember 2010'], ['\n7836111', '\nNovember 2010'], ['\n7840484', '\nNovember 2010'], ['\n7844534', '\nNovember 2010'], ['\n7848972', '\nDecember 2010'], ['\n7848978', '\nDecember 2010'], ['\n7848987', '\nDecember 2010'], ['\n7849004', '\nDecember 2010'], ['\n7853518', '\nDecember 2010'], ['\n7853998', '\nDecember 2010'], ['\n7856386', '\nDecember 2010'], ['\n7856397', '\nDecember 2010'], ['\n7856494', '\nDecember 2010'], ['\n7860786', '\nDecember 2010'], ['\n7870078', '\nJanuary 2011'], ['\n7877320', '\nJanuary 2011'], ['\n7877322', '\nJanuary 2011'], ['\n7890420', '\nFebruary 2011'], ['\n7899750', '\nMarch 2011'], ['\n7904366', '\nMarch 2011'], ['\n7908242', '\nMarch 2011'], ['\n7912770', '\nMarch 2011'], ['\n7912842', '\nMarch 2011'], ['\n7912865', '\nMarch 2011'], ['\n7925549', '\nApril 2011'], ['\n7925582', '\nApril 2011'], ['\n7930242', '\nApril 2011'], ['\n7930285', '\nApril 2011'], ['\n7941363', '\nMay 2011'], ['\n7954698', '\nJune 2011'], ['\n7958126', '\nJune 2011'], ['\n7962404', '\nJune 2011'], ['\n7966255', '\nJune 2011'], ['\n7970676', '\nJune 2011'], ['\n7970701', '\nJune 2011'], ['\n7974860', '\nJuly 2011'], ['\n7983976', '\nJuly 2011'], ['\n7991666', '\nAugust 2011'], ['\n7991677', '\nAugust 2011'], ['\n7991688', '\nAugust 2011'], ['\n7991689', '\nAugust 2011'], ['\n7996521', '\nAugust 2011'], ['\n8001042', '\nAugust 2011'], ['\n8005712', '\nAugust 2011'], ['\n8005759', '\nAugust 2011'], ['\n8006261', '\nAugust 2011'], ['\n8015045', '\nSeptember 2011'], ['\n8019843', '\nSeptember 2011'], ['\n8024245', '\nSeptember 2011'], ['\n8024263', '\nSeptember 2011'], ['\n8024264', '\nSeptember 2011'], ['\n8024778', '\nSeptember 2011'], ['\n8036979', '\nOctober 2011'], ['\n8046271', '\nOctober 2011'], ['\n8060424', '\nNovember 2011'], ['\n8064586', '\nNovember 2011'], ['\n8065233', '\nNovember 2011'], ['\n8065234', '\nNovember 2011'], ['\n8073752', '\nDecember 2011'], ['\n8073768', '\nDecember 2011'], ['\n8078453', '\nDecember 2011'], ['\n8078524', '\nDecember 2011'], ['\n8078528', '\nDecember 2011'], ['\n8082202', '\nDecember 2011'], ['\n8086509', '\nDecember 2011'], ['\n8086524', '\nDecember 2011'], ['\n8095443', '\nJanuary 2012'], ['\n8099356', '\nJanuary 2012'], ['\n8103530', '\nJanuary 2012'], ['\n8104671', '\nJanuary 2012'], ['\n8104679', '\nJanuary 2012'], ['\n8121918', '\nFebruary 2012'], ['\n8126805', '\nFebruary 2012'], ['\n8127982', '\nMarch 2012'], ['\n8131614', '\nMarch 2012'], ['\n8131639', '\nMarch 2012'], ['\n8135642', '\nMarch 2012'], ['\n8145754', '\nMarch 2012'], ['\n8160960', '\nApril 2012'], ['\n8161104', '\nApril 2012'], ['\n8170938', '\nMay 2012'], ['\n8175945', '\nMay 2012'], ['\n8180654', '\nMay 2012'], ['\n8185408', '\nMay 2012'], ['\n8195550', '\nJune 2012'], ['\n8204774', '\nJune 2012'], ['\n8209250', '\nJune 2012'], ['\n8214238', '\nJuly 2012'], ['\n8214262', '\nJuly 2012'], ['\n8219464', '\nJuly 2012'], ['\n8219535', '\nJuly 2012'], ['\n8234498', '\nJuly 2012'], ['\n8239130', '\nAugust 2012'], ['\n8271378', '\nSeptember 2012'], ['\n8280805', '\nOctober 2012'], ['\n8285577', '\nOctober 2012'], ['\n8285656', '\nOctober 2012'], ['\n8290840', '\nOctober 2012'], ['\n8296213', '\nOctober 2012'], ['\n8296229', '\nOctober 2012'], ['\n8301574', '\nOctober 2012'], ['\n8306890', '\nNovember 2012'], ['\n8315933', '\nNovember 2012'], ['\n8315942', '\nNovember 2012'], ['\n8315943', '\nNovember 2012'], ['\n8321335', '\nNovember 2012'], ['\n8326671', '\nDecember 2012'], ['\n8326672', '\nDecember 2012'], ['\n8326760', '\nDecember 2012'], ['\n8340685', '\nDecember 2012'], ['\n8341073', '\nDecember 2012'], ['\n8352343', '\nJanuary 2013'], ['\n8364518', '\nJanuary 2013'], ['\n8364582', '\nJanuary 2013'], ['\n8364588', '\nJanuary 2013'], ['\n8386377', '\nFebruary 2013'], ['\n8392334', '\nMarch 2013'], ['\n8401889', '\nMarch 2013'], ['\n8417587', '\nApril 2013'], ['\n8417612', '\nApril 2013'], ['\n8433512', '\nApril 2013'], ['\n8438105', '\nMay 2013'], ['\n8458074', '\nJune 2013'], ['\n8468198', '\nJune 2013'], ['\n8473354', '\nJune 2013'], ['\n8478673', '\nJuly 2013'], ['\n8489482', '\nJuly 2013'], ['\n8510184', '\nAugust 2013'], ['\n8515828', '\nAugust 2013'], ['\n8515862', '\nAugust 2013'], ['\n8533322', '\nSeptember 2013'], ['\n8560434', '\nOctober 2013'], ['\n8566029', '\nOctober 2013'], ['\n8566167', '\nOctober 2013'], ['\n8589069', '\nNovember 2013'], ['\n8589208', '\nNovember 2013'], ['\n8595101', '\nNovember 2013'], ['\n8600854', '\nDecember 2013'], ['\n8606626', '\nDecember 2013'], ['\n8606666', '\nDecember 2013'], ['\n8620579', '\nDecember 2013'], ['\n8626560', '\nJanuary 2014'], ['\n8626618', '\nJanuary 2014'], ['\n8626646', '\nJanuary 2014'], ['\n8630929', '\nJanuary 2014'], ['\n8639920', '\nJanuary 2014'], ['\n8666885', '\nMarch 2014'], ['\n8682770', '\nMarch 2014'], ['\n8694390', '\nApril 2014'], ['\n8694403', '\nApril 2014'], ['\n8706596', '\nApril 2014'], ['\n8732004', '\nMay 2014'], ['\n8738515', '\nMay 2014'], ['\n8744944', '\nJune 2014'], ['\n8751461', '\nJune 2014'], ['\n8762053', '\nJune 2014'], ['\n8775290', '\nJuly 2014'], ['\n8775299', '\nJuly 2014'], ['\n8775301', '\nJuly 2014'], ['\n8781877', '\nJuly 2014'], ['\n8781933', '\nJuly 2014'], ['\n8781951', '\nJuly 2014'], ['\n8781953', '\nJuly 2014'], ['\n8781975', '\nJuly 2014'], ['\n8788388', '\nJuly 2014'], ['\n8805805', '\nAugust 2014'], ['\n8825544', '\nSeptember 2014'], ['\n8930251', '\nJanuary 2015'], ['\n8938432', '\nJanuary 2015'], ['\n8966649', '\nFebruary 2015'], ['\n9026088', '\nMay 2015'], ['\n9057616', '\nJune 2015'], ['\n9057617', '\nJune 2015'], ['\n9058340', '\nJune 2015'], ['\n9063226', '\nJune 2015'], ['\n9143541', '\nSeptember 2015'], ['\n9147042', '\nSeptember 2015'], ['\n9152727', '\nOctober 2015'], ['\n9251541', '\nFebruary 2016'], ['\n9378500', '\nJune 2016'], ['\n9483606', '\nNovember 2016'], ['\n9508092', '\nNovember 2016'], ['\n9509711', '\nNovember 2016'], ['\n9553936', '\nJanuary 2017'], ['\n9563916', '\nFebruary 2017'], ['\n9576030', '\nFebruary 2017'], ['\n9595051', '\nMarch 2017'], ['\n9619579', '\nApril 2017'], ['\n9652802', '\nMay 2017'], ['\n9870589', '\nJanuary 2018'], ['\n9916596', '\nMarch 2018'], ['\n9916621', '\nMarch 2018'], ['\n10019508', '\nJuly 2018'], ['\n10078868', '\nSeptember 2018'], ['\n2001/0013011', '\nAugust 2001'], ['\n2001/0014868', '\nAugust 2001'], ['\n2001/0014878', '\nAugust 2001'], ['\n2001/0016833', '\nAugust 2001'], ['\n2001/0027413', '\nOctober 2001'], ['\n2001/0029470', '\nOctober 2001'], ['\n2001/0034631', '\nOctober 2001'], ['\n2001/0037332', '\nNovember 2001'], ['\n2001/0039523', '\nNovember 2001'], ['\n2001/0049620', '\nDecember 2001'], ['\n2002/0019804', '\nFebruary 2002'], ['\n2002/0023051', '\nFebruary 2002'], ['\n2002/0023143', '\nFebruary 2002'], ['\n2002/0026411', '\nFebruary 2002'], ['\n2002/0029162', '\nMarch 2002'], ['\n2002/0035511', '\nMarch 2002'], ['\n2002/0046096', '\nApril 2002'], ['\n2002/0049626', '\nApril 2002'], ['\n2002/0049701', '\nApril 2002'], ['\n2002/0049738', '\nApril 2002'], ['\n2002/0052836', '\nMay 2002'], ['\n2002/0052841', '\nMay 2002'], ['\n2002/0055869', '\nMay 2002'], ['\n2002/0069122', '\nJune 2002'], ['\n2002/0072927', '\nJune 2002'], ['\n2002/0077964', '\nJune 2002'], ['\n2002/0082892', '\nJune 2002'], ['\n2002/0087460', '\nJuly 2002'], ['\n2002/0091706', '\nJuly 2002'], ['\n2002/0095360', '\nJuly 2002'], ['\n2002/0099628', '\nJuly 2002'], ['\n2002/0099641', '\nJuly 2002'], ['\n2002/0099649', '\nJuly 2002'], ['\n2002/0099824', '\nJuly 2002'], ['\n2002/0099936', '\nJuly 2002'], ['\n2002/0111845', '\nAugust 2002'], ['\n2002/0119824', '\nAugust 2002'], ['\n2002/0120504', '\nAugust 2002'], ['\n2002/0123928', '\nSeptember 2002'], ['\n2002/0128960', '\nSeptember 2002'], ['\n2002/0128962', '\nSeptember 2002'], ['\n2002/0129368', '\nSeptember 2002'], ['\n2002/0133444', '\nSeptember 2002'], ['\n2002/0138297', '\nSeptember 2002'], ['\n2002/0138331', '\nSeptember 2002'], ['\n2002/0138333', '\nSeptember 2002'], ['\n2002/0138334', '\nSeptember 2002'], ['\n2002/0138417', '\nSeptember 2002'], ['\n2002/0143661', '\nOctober 2002'], ['\n2002/0147623', '\nOctober 2002'], ['\n2002/0147669', '\nOctober 2002'], ['\n2002/0147695', '\nOctober 2002'], ['\n2002/0156676', '\nOctober 2002'], ['\n2002/0161496', '\nOctober 2002'], ['\n2002/0161664', '\nOctober 2002'], ['\n2002/0161711', '\nOctober 2002'], ['\n2002/0165757', '\nNovember 2002'], ['\n2002/0169747', '\nNovember 2002'], ['\n2002/0173984', '\nNovember 2002'], ['\n2002/0173994', '\nNovember 2002'], ['\n2002/0184255', '\nDecember 2002'], ['\n2002/0188544', '\nDecember 2002'], ['\n2002/0194099', '\nDecember 2002'], ['\n2002/0194103', '\nDecember 2002'], ['\n2002/0194140', '\nDecember 2002'], ['\n2002/0198824', '\nDecember 2002'], ['\n2003/0000568', '\nJanuary 2003'], ['\n2003/0002639', '\nJanuary 2003'], ['\n2003/0004787', '\nJanuary 2003'], ['\n2003/0004855', '\nJanuary 2003'], ['\n2003/0004865', '\nJanuary 2003'], ['\n2003/0009368', '\nJanuary 2003'], ['\n2003/0009393', '\nJanuary 2003'], ['\n2003/0009418', '\nJanuary 2003'], ['\n2003/0009426', '\nJanuary 2003'], ['\n2003/0018549', '\nJanuary 2003'], ['\n2003/0018578', '\nJanuary 2003'], ['\n2003/0018769', '\nJanuary 2003'], ['\n2003/0023489', '\nJanuary 2003'], ['\n2003/0033242', '\nFebruary 2003'], ['\n2003/0033261', '\nFebruary 2003'], ['\n2003/0041031', '\nFebruary 2003'], ['\n2003/0046222', '\nMarch 2003'], ['\n2003/0060284', '\nMarch 2003'], ['\n2003/0061132', '\nMarch 2003'], ['\n2003/0061163', '\nMarch 2003'], ['\n2003/0065563', '\nApril 2003'], ['\n2003/0069839', '\nApril 2003'], ['\n2003/0078877', '\nApril 2003'], ['\n2003/0093289', '\nMay 2003'], ['\n2003/0093311', '\nMay 2003'], ['\n2003/0093366', '\nMay 2003'], ['\n2003/0097320', '\nMay 2003'], ['\n2003/0097342', '\nMay 2003'], ['\n2003/0097380', '\nMay 2003'], ['\n2003/0101111', '\nMay 2003'], ['\n2003/0105696', '\nJune 2003'], ['\n2003/0105728', '\nJune 2003'], ['\n2003/0110111', '\nJune 2003'], ['\n2003/0110293', '\nJune 2003'], ['\n2003/0113727', '\nJune 2003'], ['\n2003/0115080', '\nJune 2003'], ['\n2003/0115133', '\nJune 2003'], ['\n2003/0120591', '\nJune 2003'], ['\n2003/0135451', '\nJuly 2003'], ['\n2003/0139986', '\nJuly 2003'], ['\n2003/0144950', '\nJuly 2003'], ['\n2003/0149610', '\nAugust 2003'], ['\n2003/0158751', '\nAugust 2003'], ['\n2003/0158776', '\nAugust 2003'], ['\n2003/0163708', '\nAugust 2003'], ['\n2003/0164497', '\nSeptember 2003'], ['\n2003/0167218', '\nSeptember 2003'], ['\n2003/0171942', '\nSeptember 2003'], ['\n2003/0182214', '\nSeptember 2003'], ['\n2003/0195830', '\nOctober 2003'], ['\n2003/0195859', '\nOctober 2003'], ['\n2003/0200151', '\nOctober 2003'], ['\n2003/0205845', '\nNovember 2003'], ['\n2003/0208362', '\nNovember 2003'], ['\n2003/0208428', '\nNovember 2003'], ['\n2003/0212618', '\nNovember 2003'], ['\n2003/0212654', '\nNovember 2003'], ['\n2003/0216965', '\nNovember 2003'], ['\n2003/0219709', '\nNovember 2003'], ['\n2003/0225656', '\nDecember 2003'], ['\n2003/0225692', '\nDecember 2003'], ['\n2003/0225742', '\nDecember 2003'], ['\n2003/0229892', '\nDecember 2003'], ['\n2003/0233278', '\nDecember 2003'], ['\n2003/0233323', '\nDecember 2003'], ['\n2003/0233655', '\nDecember 2003'], ['\n2003/0236738', '\nDecember 2003'], ['\n2004/0002916', '\nJanuary 2004'], ['\n2004/0006536', '\nJanuary 2004'], ['\n2004/0010443', '\nJanuary 2004'], ['\n2004/0019518', '\nJanuary 2004'], ['\n2004/0023637', '\nFebruary 2004'], ['\n2004/0024692', '\nFebruary 2004'], ['\n2004/0029311', '\nFebruary 2004'], ['\n2004/0030649', '\nFebruary 2004'], ['\n2004/0030667', '\nFebruary 2004'], ['\n2004/0033375', '\nFebruary 2004'], ['\n2004/0034570', '\nFebruary 2004'], ['\n2004/0039681', '\nFebruary 2004'], ['\n2004/0039688', '\nFebruary 2004'], ['\n2004/0044615', '\nMarch 2004'], ['\n2004/0044617', '\nMarch 2004'], ['\n2004/0046497', '\nMarch 2004'], ['\n2004/0049452', '\nMarch 2004'], ['\n2004/0054619', '\nMarch 2004'], ['\n2004/0059626', '\nMarch 2004'], ['\n2004/0059653', '\nMarch 2004'], ['\n2004/0062213', '\nApril 2004'], ['\n2004/0078248', '\nApril 2004'], ['\n2004/0078324', '\nApril 2004'], ['\n2004/0083215', '\nApril 2004'], ['\n2004/0088221', '\nMay 2004'], ['\n2004/0093278', '\nMay 2004'], ['\n2004/0098625', '\nMay 2004'], ['\n2004/0102197', '\nMay 2004'], ['\n2004/0103147', '\nMay 2004'], ['\n2004/0107123', '\nJune 2004'], ['\n2004/0107125', '\nJune 2004'], ['\n2004/0111305', '\nJune 2004'], ['\n2004/0111358', '\nJune 2004'], ['\n2004/0111363', '\nJune 2004'], ['\n2004/0117235', '\nJune 2004'], ['\n2004/0117358', '\nJune 2004'], ['\n2004/0122730', '\nJune 2004'], ['\n2004/0122735', '\nJune 2004'], ['\n2004/0128150', '\nJuly 2004'], ['\n2004/0128227', '\nJuly 2004'], ['\n2004/0128230', '\nJuly 2004'], ['\n2004/0128232', '\nJuly 2004'], ['\n2004/0128236', '\nJuly 2004'], ['\n2004/0139035', '\nJuly 2004'], ['\n2004/0143526', '\nJuly 2004'], ['\n2004/0143546', '\nJuly 2004'], ['\n2004/0153330', '\nAugust 2004'], ['\n2004/0153448', '\nAugust 2004'], ['\n2004/0158520', '\nAugust 2004'], ['\n2004/0158523', '\nAugust 2004'], ['\n2004/0163101', '\nAugust 2004'], ['\n2004/0167793', '\nAugust 2004'], ['\n2004/0176995', '\nSeptember 2004'], ['\n2004/0177046', '\nSeptember 2004'], ['\n2004/0186807', '\nSeptember 2004'], ['\n2004/0193535', '\nSeptember 2004'], ['\n2004/0193538', '\nSeptember 2004'], ['\n2004/0199456', '\nOctober 2004'], ['\n2004/0199458', '\nOctober 2004'], ['\n2004/0199462', '\nOctober 2004'], ['\n2004/0199789', '\nOctober 2004'], ['\n2004/0205157', '\nOctober 2004'], ['\n2004/0212299', '\nOctober 2004'], ['\n2004/0220896', '\nNovember 2004'], ['\n2004/0225545', '\nNovember 2004'], ['\n2004/0225586', '\nNovember 2004'], ['\n2004/0225594', '\nNovember 2004'], ['\n2004/0225596', '\nNovember 2004'], ['\n2004/0230448', '\nNovember 2004'], ['\n2004/0230459', '\nNovember 2004'], ['\n2004/0230527', '\nNovember 2004'], ['\n2004/0230534', '\nNovember 2004'], ['\n2004/0230820', '\nNovember 2004'], ['\n2004/0243450', '\nDecember 2004'], ['\n2004/0243518', '\nDecember 2004'], ['\n2004/0243588', '\nDecember 2004'], ['\n2004/0261116', '\nDecember 2004'], ['\n2005/0004805', '\nJanuary 2005'], ['\n2005/0015330', '\nJanuary 2005'], ['\n2005/0021397', '\nJanuary 2005'], ['\n2005/0021476', '\nJanuary 2005'], ['\n2005/0027633', '\nFebruary 2005'], ['\n2005/0027983', '\nFebruary 2005'], ['\n2005/0033734', '\nFebruary 2005'], ['\n2005/0038726', '\nFebruary 2005'], ['\n2005/0050027', '\nMarch 2005'], ['\n2005/0058262', '\nMarch 2005'], ['\n2005/0065874', '\nMarch 2005'], ['\n2005/0086261', '\nApril 2005'], ['\n2005/0091164', '\nApril 2005'], ['\n2005/0097039', '\nMay 2005'], ['\n2005/0102206', '\nMay 2005'], ['\n2005/0102226', '\nMay 2005'], ['\n2005/0113991', '\nMay 2005'], ['\n2005/0125350', '\nJune 2005'], ['\n2005/0130704', '\nJune 2005'], ['\n2005/0137899', '\nJune 2005'], ['\n2005/0137963', '\nJune 2005'], ['\n2005/0144452', '\nJune 2005'], ['\n2005/0144641', '\nJune 2005'], ['\n2005/0154664', '\nJuly 2005'], ['\n2005/0154665', '\nJuly 2005'], ['\n2005/0159996', '\nJuly 2005'], ['\n2005/0177489', '\nAugust 2005'], ['\n2005/0192008', '\nSeptember 2005'], ['\n2005/0197953', '\nSeptember 2005'], ['\n2005/0197954', '\nSeptember 2005'], ['\n2005/0201272', '\nSeptember 2005'], ['\n2005/0209892', '\nSeptember 2005'], ['\n2005/0209922', '\nSeptember 2005'], ['\n2005/0222900', '\nOctober 2005'], ['\n2005/0228692', '\nOctober 2005'], ['\n2005/0246256', '\nNovember 2005'], ['\n2005/0251408', '\nNovember 2005'], ['\n2005/0251474', '\nNovember 2005'], ['\n2005/0251820', '\nNovember 2005'], ['\n2005/0256780', '\nNovember 2005'], ['\n2005/0256809', '\nNovember 2005'], ['\n2005/0257250', '\nNovember 2005'], ['\n2005/0262014', '\nNovember 2005'], ['\n2005/0262158', '\nNovember 2005'], ['\n2005/0267774', '\nDecember 2005'], ['\n2005/0273442', '\nDecember 2005'], ['\n2005/0273849', '\nDecember 2005'], ['\n2005/0278246', '\nDecember 2005'], ['\n2005/0278542', '\nDecember 2005'], ['\n2005/0279824', '\nDecember 2005'], ['\n2005/0279827', '\nDecember 2005'], ['\n2005/0288954', '\nDecember 2005'], ['\n2005/0288998', '\nDecember 2005'], ['\n2005/0289003', '\nDecember 2005'], ['\n2006/0004626', '\nJanuary 2006'], ['\n2006/0004731', '\nJanuary 2006'], ['\n2006/0010055', '\nJanuary 2006'], ['\n2006/0014129', '\nJanuary 2006'], ['\n2006/0015425', '\nJanuary 2006'], ['\n2006/0020611', '\nJanuary 2006'], ['\n2006/0031158', '\nFebruary 2006'], ['\n2006/0031747', '\nFebruary 2006'], ['\n2006/0032909', '\nFebruary 2006'], ['\n2006/0041443', '\nFebruary 2006'], ['\n2006/0041464', '\nFebruary 2006'], ['\n2006/0041840', '\nFebruary 2006'], ['\n2006/0059073', '\nMarch 2006'], ['\n2006/0059110', '\nMarch 2006'], ['\n2006/0074986', '\nApril 2006'], ['\n2006/0080126', '\nApril 2006'], ['\n2006/0080230', '\nApril 2006'], ['\n2006/0080233', '\nApril 2006'], ['\n2006/0080251', '\nApril 2006'], ['\n2006/0080263', '\nApril 2006'], ['\n2006/0085334', '\nApril 2006'], ['\n2006/0089842', '\nApril 2006'], ['\n2006/0095363', '\nMay 2006'], ['\n2006/0095923', '\nMay 2006'], ['\n2006/0100954', '\nMay 2006'], ['\n2006/0122921', '\nJune 2006'], ['\n2006/0129428', '\nJune 2006'], ['\n2006/0129481', '\nJune 2006'], ['\n2006/0131390', '\nJune 2006'], ['\n2006/0136330', '\nJune 2006'], ['\n2006/0149674', '\nJuly 2006'], ['\n2006/0155624', '\nJuly 2006'], ['\n2006/0155639', '\nJuly 2006'], ['\n2006/0161435', '\nJuly 2006'], ['\n2006/0173726', '\nAugust 2006'], ['\n2006/0173772', '\nAugust 2006'], ['\n2006/0173776', '\nAugust 2006'], ['\n2006/0177226', '\nAugust 2006'], ['\n2006/0178189', '\nAugust 2006'], ['\n2006/0178957', '\nAugust 2006'], ['\n2006/0178971', '\nAugust 2006'], ['\n2006/0178983', '\nAugust 2006'], ['\n2006/0184440', '\nAugust 2006'], ['\n2006/0195390', '\nAugust 2006'], ['\n2006/0202012', '\nSeptember 2006'], ['\n2006/0204051', '\nSeptember 2006'], ['\n2006/0206416', '\nSeptember 2006'], ['\n2006/0212350', '\nSeptember 2006'], ['\n2006/0218069', '\nSeptember 2006'], ['\n2006/0218079', '\nSeptember 2006'], ['\n2006/0229943', '\nOctober 2006'], ['\n2006/0229961', '\nOctober 2006'], ['\n2006/0229996', '\nOctober 2006'], ['\n2006/0239512', '\nOctober 2006'], ['\n2006/0241923', '\nOctober 2006'], ['\n2006/0242046', '\nOctober 2006'], ['\n2006/0242047', '\nOctober 2006'], ['\n2006/0242048', '\nOctober 2006'], ['\n2006/0242050', '\nOctober 2006'], ['\n2006/0253328', '\nNovember 2006'], ['\n2006/0253358', '\nNovember 2006'], ['\n2006/0259364', '\nNovember 2006'], ['\n2006/0262929', '\nNovember 2006'], ['\n2006/0265243', '\nNovember 2006'], ['\n2006/0265323', '\nNovember 2006'], ['\n2006/0267999', '\nNovember 2006'], ['\n2006/0271456', '\nNovember 2006'], ['\n2006/0271457', '\nNovember 2006'], ['\n2006/0271552', '\nNovember 2006'], ['\n2006/0276171', '\nDecember 2006'], ['\n2006/0277102', '\nDecember 2006'], ['\n2006/0277141', '\nDecember 2006'], ['\n2006/0282328', '\nDecember 2006'], ['\n2006/0282359', '\nDecember 2006'], ['\n2006/0293921', '\nDecember 2006'], ['\n2006/0293932', '\nDecember 2006'], ['\n2006/0293979', '\nDecember 2006'], ['\n2006/0294199', '\nDecember 2006'], ['\n2007/0005508', '\nJanuary 2007'], ['\n2007/0011026', '\nJanuary 2007'], ['\n2007/0011039', '\nJanuary 2007'], ['\n2007/0011083', '\nJanuary 2007'], ['\n2007/0016500', '\nJanuary 2007'], ['\n2007/0016501', '\nJanuary 2007'], ['\n2007/0016518', '\nJanuary 2007'], ['\n2007/0016522', '\nJanuary 2007'], ['\n2007/0022141', '\nJanuary 2007'], ['\n2007/0022297', '\nJanuary 2007'], ['\n2007/0027778', '\nFebruary 2007'], ['\n2007/0027791', '\nFebruary 2007'], ['\n2007/0030282', '\nFebruary 2007'], ['\n2007/0033227', '\nFebruary 2007'], ['\n2007/0038483', '\nFebruary 2007'], ['\n2007/0038497', '\nFebruary 2007'], ['\n2007/0043654', '\nFebruary 2007'], ['\n2007/0055598', '\nMarch 2007'], ['\n2007/0055599', '\nMarch 2007'], ['\n2007/0055618', '\nMarch 2007'], ['\n2007/0055621', '\nMarch 2007'], ['\n2007/0061195', '\nMarch 2007'], ['\n2007/0061243', '\nMarch 2007'], ['\n2007/0067207', '\nMarch 2007'], ['\n2007/0067208', '\nMarch 2007'], ['\n2007/0067209', '\nMarch 2007'], ['\n2007/0067235', '\nMarch 2007'], ['\n2007/0067285', '\nMarch 2007'], ['\n2007/0067297', '\nMarch 2007'], ['\n2007/0067437', '\nMarch 2007'], ['\n2007/0072190', '\nMarch 2007'], ['\n2007/0078741', '\nApril 2007'], ['\n2007/0078985', '\nApril 2007'], ['\n2007/0083460', '\nApril 2007'], ['\n2007/0093234', '\nApril 2007'], ['\n2007/0094137', '\nApril 2007'], ['\n2007/0106582', '\nMay 2007'], ['\n2007/0112667', '\nMay 2007'], ['\n2007/0112668', '\nMay 2007'], ['\n2007/0118393', '\nMay 2007'], ['\n2007/0121843', '\nMay 2007'], ['\n2007/0124235', '\nMay 2007'], ['\n2007/0156515', '\nJuly 2007'], ['\n2007/0156589', '\nJuly 2007'], ['\n2007/0156718', '\nJuly 2007'], ['\n2007/0168246', '\nJuly 2007'], ['\n2007/0168267', '\nJuly 2007'], ['\n2007/0179860', '\nAugust 2007'], ['\n2007/0192165', '\nAugust 2007'], ['\n2007/0192248', '\nAugust 2007'], ['\n2007/0205266', '\nSeptember 2007'], ['\n2007/0208653', '\nSeptember 2007'], ['\n2007/0208729', '\nSeptember 2007'], ['\n2007/0220611', '\nSeptember 2007'], ['\n2007/0226093', '\nSeptember 2007'], ['\n2007/0226114', '\nSeptember 2007'], ['\n2007/0244732', '\nOctober 2007'], ['\n2007/0244807', '\nOctober 2007'], ['\n2007/0250327', '\nOctober 2007'], ['\n2007/0271178', '\nNovember 2007'], ['\n2007/0282684', '\nDecember 2007'], ['\n2007/0282730', '\nDecember 2007'], ['\n2007/0282736', '\nDecember 2007'], ['\n2007/0288271', '\nDecember 2007'], ['\n2007/0288355', '\nDecember 2007'], ['\n2007/0288360', '\nDecember 2007'], ['\n2007/0288559', '\nDecember 2007'], ['\n2007/0294163', '\nDecember 2007'], ['\n2007/0299759', '\nDecember 2007'], ['\n2007/0299771', '\nDecember 2007'], ['\n2008/0004957', '\nJanuary 2008'], ['\n2008/0005313', '\nJanuary 2008'], ['\n2008/0010687', '\nJanuary 2008'], ['\n2008/0015887', '\nJanuary 2008'], ['\n2008/0015938', '\nJanuary 2008'], ['\n2008/0021804', '\nJanuary 2008'], ['\n2008/0027859', '\nJanuary 2008'], ['\n2008/0028067', '\nJanuary 2008'], ['\n2008/0033852', '\nFebruary 2008'], ['\n2008/0052182', '\nFebruary 2008'], ['\n2008/0059224', '\nMarch 2008'], ['\n2008/0059317', '\nMarch 2008'], ['\n2008/0059364', '\nMarch 2008'], ['\n2008/0059449', '\nMarch 2008'], ['\n2008/0065774', '\nMarch 2008'], ['\n2008/0066188', '\nMarch 2008'], ['\n2008/0071882', '\nMarch 2008'], ['\n2008/0077526', '\nMarch 2008'], ['\n2008/0086368', '\nApril 2008'], ['\n2008/0091463', '\nApril 2008'], ['\n2008/0097768', '\nApril 2008'], ['\n2008/0097928', '\nApril 2008'], ['\n2008/0103800', '\nMay 2008'], ['\n2008/0103972', '\nMay 2008'], ['\n2008/0110973', '\nMay 2008'], ['\n2008/0120155', '\nMay 2008'], ['\n2008/0120569', '\nMay 2008'], ['\n2008/0126233', '\nMay 2008'], ['\n2008/0133273', '\nJune 2008'], ['\n2008/0133322', '\nJune 2008'], ['\n2008/0133325', '\nJune 2008'], ['\n2008/0133531', '\nJune 2008'], ['\n2008/0134042', '\nJune 2008'], ['\n2008/0140476', '\nJune 2008'], ['\n2008/0140507', '\nJune 2008'], ['\n2008/0140549', '\nJune 2008'], ['\n2008/0140576', '\nJune 2008'], ['\n2008/0147454', '\nJune 2008'], ['\n2008/0147523', '\nJune 2008'], ['\n2008/0154766', '\nJune 2008'], ['\n2008/0167883', '\nJuly 2008'], ['\n2008/0167936', '\nJuly 2008'], ['\n2008/0167956', '\nJuly 2008'], ['\n2008/0172324', '\nJuly 2008'], ['\n2008/0175360', '\nJuly 2008'], ['\n2008/0177655', '\nJuly 2008'], ['\n2008/0177836', '\nJuly 2008'], ['\n2008/0183564', '\nJuly 2008'], ['\n2008/0195425', '\nAugust 2008'], ['\n2008/0195600', '\nAugust 2008'], ['\n2008/0208548', '\nAugust 2008'], ['\n2008/0208610', '\nAugust 2008'], ['\n2008/0208631', '\nAugust 2008'], ['\n2008/0208788', '\nAugust 2008'], ['\n2008/0215470', '\nSeptember 2008'], ['\n2008/0221934', '\nSeptember 2008'], ['\n2008/0221947', '\nSeptember 2008'], ['\n2008/0221970', '\nSeptember 2008'], ['\n2008/0221971', '\nSeptember 2008'], ['\n2008/0221972', '\nSeptember 2008'], ['\n2008/0221973', '\nSeptember 2008'], ['\n2008/0221990', '\nSeptember 2008'], ['\n2008/0222016', '\nSeptember 2008'], ['\n2008/0222027', '\nSeptember 2008'], ['\n2008/0228538', '\nSeptember 2008'], ['\n2008/0228539', '\nSeptember 2008'], ['\n2008/0228540', '\nSeptember 2008'], ['\n2008/0228541', '\nSeptember 2008'], ['\n2008/0228556', '\nSeptember 2008'], ['\n2008/0228606', '\nSeptember 2008'], ['\n2008/0228635', '\nSeptember 2008'], ['\n2008/0243680', '\nOctober 2008'], ['\n2008/0244008', '\nOctober 2008'], ['\n2008/0255897', '\nOctober 2008'], ['\n2008/0255992', '\nOctober 2008'], ['\n2008/0262925', '\nOctober 2008'], ['\n2008/0263638', '\nOctober 2008'], ['\n2008/0281737', '\nNovember 2008'], ['\n2008/0288382', '\nNovember 2008'], ['\n2008/0294540', '\nNovember 2008'], ['\n2008/0294546', '\nNovember 2008'], ['\n2008/0301016', '\nDecember 2008'], ['\n2008/0301188', '\nDecember 2008'], ['\n2008/0312963', '\nDecember 2008'], ['\n2008/0312969', '\nDecember 2008'], ['\n2009/0006185', '\nJanuary 2009'], ['\n2009/0006475', '\nJanuary 2009'], ['\n2009/0012889', '\nJanuary 2009'], ['\n2009/0018996', '\nJanuary 2009'], ['\n2009/0019027', '\nJanuary 2009'], ['\n2009/0024505', '\nJanuary 2009'], ['\n2009/0030776', '\nJanuary 2009'], ['\n2009/0037247', '\nFebruary 2009'], ['\n2009/0037323', '\nFebruary 2009'], ['\n2009/0043637', '\nFebruary 2009'], ['\n2009/0044279', '\nFebruary 2009'], ['\n2009/0048877', '\nFebruary 2009'], ['\n2009/0076883', '\nMarch 2009'], ['\n2009/0089190', '\nApril 2009'], ['\n2009/0089205', '\nApril 2009'], ['\n2009/0094675', '\nApril 2009'], ['\n2009/0106150', '\nApril 2009'], ['\n2009/0112650', '\nApril 2009'], ['\n2009/0113532', '\nApril 2009'], ['\n2009/0119169', '\nMay 2009'], ['\n2009/0119199', '\nMay 2009'], ['\n2009/0125369', '\nMay 2009'], ['\n2009/0132347', '\nMay 2009'], ['\n2009/0132559', '\nMay 2009'], ['\n2009/0144102', '\nJune 2009'], ['\n2009/0144160', '\nJune 2009'], ['\n2009/0144201', '\nJune 2009'], ['\n2009/0172815', '\nJuly 2009'], ['\n2009/0182653', '\nJuly 2009'], ['\n2009/0198557', '\nAugust 2009'], ['\n2009/0198602', '\nAugust 2009'], ['\n2009/0198612', '\nAugust 2009'], ['\n2009/0199264', '\nAugust 2009'], ['\n2009/0210886', '\nAugust 2009'], ['\n2009/0215479', '\nAugust 2009'], ['\n2009/0216591', '\nAugust 2009'], ['\n2009/0222308', '\nSeptember 2009'], ['\n2009/0222373', '\nSeptember 2009'], ['\n2009/0222374', '\nSeptember 2009'], ['\n2009/0222375', '\nSeptember 2009'], ['\n2009/0222376', '\nSeptember 2009'], ['\n2009/0222377', '\nSeptember 2009'], ['\n2009/0222378', '\nSeptember 2009'], ['\n2009/0222379', '\nSeptember 2009'], ['\n2009/0222380', '\nSeptember 2009'], ['\n2009/0228918', '\nSeptember 2009'], ['\n2009/0234665', '\nSeptember 2009'], ['\n2009/0234775', '\nSeptember 2009'], ['\n2009/0240609', '\nSeptember 2009'], ['\n2009/0248567', '\nOctober 2009'], ['\n2009/0248568', '\nOctober 2009'], ['\n2009/0248569', '\nOctober 2009'], ['\n2009/0248570', '\nOctober 2009'], ['\n2009/0248571', '\nOctober 2009'], ['\n2009/0248572', '\nOctober 2009'], ['\n2009/0248573', '\nOctober 2009'], ['\n2009/0249440', '\nOctober 2009'], ['\n2009/0254476', '\nOctober 2009'], ['\n2009/0254971', '\nOctober 2009'], ['\n2009/0265326', '\nOctober 2009'], ['\n2009/0271248', '\nOctober 2009'], ['\n2009/0271265', '\nOctober 2009'], ['\n2009/0276233', '\nNovember 2009'], ['\n2009/0276368', '\nNovember 2009'], ['\n2009/0300066', '\nDecember 2009'], ['\n2009/0313163', '\nDecember 2009'], ['\n2009/0319648', '\nDecember 2009'], ['\n2009/0327120', '\nDecember 2009'], ['\n2010/0009320', '\nJanuary 2010'], ['\n2010/0010935', '\nJanuary 2010'], ['\n2010/0030649', '\nFebruary 2010'], ['\n2010/0043055', '\nFebruary 2010'], ['\n2010/0094704', '\nApril 2010'], ['\n2010/0094758', '\nApril 2010'], ['\n2010/0094768', '\nApril 2010'], ['\n2010/0094774', '\nApril 2010'], ['\n2010/0100945', '\nApril 2010'], ['\n2010/0107225', '\nApril 2010'], ['\n2010/0114724', '\nMay 2010'], ['\n2010/0114744', '\nMay 2010'], ['\n2010/0121767', '\nMay 2010'], ['\n2010/0130172', '\nMay 2010'], ['\n2010/0142698', '\nJune 2010'], ['\n2010/0145836', '\nJune 2010'], ['\n2010/0145847', '\nJune 2010'], ['\n2010/0169159', '\nJuly 2010'], ['\n2010/0169264', '\nJuly 2010'], ['\n2010/0185453', '\nJuly 2010'], ['\n2010/0198629', '\nAugust 2010'], ['\n2010/0205662', '\nAugust 2010'], ['\n2010/0211445', '\nAugust 2010'], ['\n2010/0217837', '\nAugust 2010'], ['\n2010/0223168', '\nSeptember 2010'], ['\n2010/0228657', '\nSeptember 2010'], ['\n2010/0229245', '\nSeptember 2010'], ['\n2010/0248681', '\nSeptember 2010'], ['\n2010/0250364', '\nSeptember 2010'], ['\n2010/0250434', '\nSeptember 2010'], ['\n2010/0250469', '\nSeptember 2010'], ['\n2010/0268557', '\nOctober 2010'], ['\n2010/0274739', '\nOctober 2010'], ['\n2010/0293114', '\nNovember 2010'], ['\n2010/0312717', '\nDecember 2010'], ['\n2010/0332292', '\nDecember 2010'], ['\n2011/0004498', '\nJanuary 2011'], ['\n2011/0016042', '\nJanuary 2011'], ['\n2011/0023115', '\nJanuary 2011'], ['\n2011/0029388', '\nFebruary 2011'], ['\n2011/0035333', '\nFebruary 2011'], ['\n2011/0047071', '\nFebruary 2011'], ['\n2011/0054981', '\nMarch 2011'], ['\n2011/0066495', '\nMarch 2011'], ['\n2011/0071950', '\nMarch 2011'], ['\n2011/0076663', '\nMarch 2011'], ['\n2011/0078073', '\nMarch 2011'], ['\n2011/0093383', '\nApril 2011'], ['\n2011/0112958', '\nMay 2011'], ['\n2011/0125595', '\nMay 2011'], ['\n2011/0126275', '\nMay 2011'], ['\n2011/0137789', '\nJune 2011'], ['\n2011/0145122', '\nJune 2011'], ['\n2011/0161323', '\nJune 2011'], ['\n2011/0164746', '\nJuly 2011'], ['\n2011/0173116', '\nJuly 2011'], ['\n2011/0184838', '\nJuly 2011'], ['\n2011/0184851', '\nJuly 2011'], ['\n2011/0196791', '\nAugust 2011'], ['\n2011/0211445', '\nSeptember 2011'], ['\n2011/0213641', '\nSeptember 2011'], ['\n2011/0218826', '\nSeptember 2011'], ['\n2011/0219421', '\nSeptember 2011'], ['\n2011/0238566', '\nSeptember 2011'], ['\n2011/0251946', '\nOctober 2011'], ['\n2011/0258050', '\nOctober 2011'], ['\n2011/0258142', '\nOctober 2011'], ['\n2011/0264581', '\nOctober 2011'], ['\n2011/0270779', '\nNovember 2011'], ['\n2011/0276396', '\nNovember 2011'], ['\n2011/0282779', '\nNovember 2011'], ['\n2011/0307397', '\nDecember 2011'], ['\n2011/0320307', '\nDecember 2011'], ['\n2012/0005070', '\nJanuary 2012'], ['\n2012/0011056', '\nJanuary 2012'], ['\n2012/0011158', '\nJanuary 2012'], ['\n2012/0016948', '\nJanuary 2012'], ['\n2012/0029956', '\nFebruary 2012'], ['\n2012/0035980', '\nFebruary 2012'], ['\n2012/0047219', '\nFebruary 2012'], ['\n2012/0054592', '\nMarch 2012'], ['\n2012/0066065', '\nMarch 2012'], ['\n2012/0066106', '\nMarch 2012'], ['\n2012/0084230', '\nApril 2012'], ['\n2012/0089605', '\nApril 2012'], ['\n2012/0101938', '\nApril 2012'], ['\n2012/0101939', '\nApril 2012'], ['\n2012/0106801', '\nMay 2012'], ['\n2012/0110677', '\nMay 2012'], ['\n2012/0116807', '\nMay 2012'], ['\n2012/0123968', '\nMay 2012'], ['\n2012/0124498', '\nMay 2012'], ['\n2012/0136763', '\nMay 2012'], ['\n2012/0143637', '\nJune 2012'], ['\n2012/0143921', '\nJune 2012'], ['\n2012/0158460', '\nJune 2012'], ['\n2012/0158574', '\nJune 2012'], ['\n2012/0158654', '\nJune 2012'], ['\n2012/0173339', '\nJuly 2012'], ['\n2012/0179536', '\nJuly 2012'], ['\n2012/0191479', '\nJuly 2012'], ['\n2012/0209586', '\nAugust 2012'], ['\n2012/0215682', '\nAugust 2012'], ['\n2012/0216125', '\nAugust 2012'], ['\n2012/0232958', '\nSeptember 2012'], ['\n2012/0239497', '\nSeptember 2012'], ['\n2012/0239515', '\nSeptember 2012'], ['\n2012/0265661', '\nOctober 2012'], ['\n2012/0284118', '\nNovember 2012'], ['\n2012/0290660', '\nNovember 2012'], ['\n2012/0317016', '\nDecember 2012'], ['\n2012/0323954', '\nDecember 2012'], ['\n2013/0080242', '\nMarch 2013'], ['\n2013/0080467', '\nMarch 2013'], ['\n2013/0085804', '\nApril 2013'], ['\n2013/0085902', '\nApril 2013'], ['\n2013/0103571', '\nApril 2013'], ['\n2013/0117832', '\nMay 2013'], ['\n2013/0124263', '\nMay 2013'], ['\n2013/0132151', '\nMay 2013'], ['\n2013/0137464', '\nMay 2013'], ['\n2013/0151388', '\nJune 2013'], ['\n2013/0159168', '\nJune 2013'], ['\n2013/0159411', '\nJune 2013'], ['\n2013/0173359', '\nJuly 2013'], ['\n2013/0173450', '\nJuly 2013'], ['\n2013/0173481', '\nJuly 2013'], ['\n2013/0218638', '\nAugust 2013'], ['\n2013/0226787', '\nAugust 2013'], ['\n2013/0226820', '\nAugust 2013'], ['\n2013/0238413', '\nSeptember 2013'], ['\n2013/0268324', '\nOctober 2013'], ['\n2013/0275331', '\nOctober 2013'], ['\n2013/0293363', '\nNovember 2013'], ['\n2013/0347059', '\nDecember 2013'], ['\n2014/0012633', '\nJanuary 2014'], ['\n2014/0019331', '\nJanuary 2014'], ['\n2014/0032265', '\nJanuary 2014'], ['\n2014/0032384', '\nJanuary 2014'], ['\n2014/0046887', '\nFebruary 2014'], ['\n2014/0095251', '\nApril 2014'], ['\n2014/0096249', '\nApril 2014'], ['\n2014/0149179', '\nMay 2014'], ['\n2014/0156501', '\nJune 2014'], ['\n2014/0164112', '\nJune 2014'], ['\n2014/0164398', '\nJune 2014'], ['\n2014/0172686', '\nJune 2014'], ['\n2014/0181285', '\nJune 2014'], ['\n2014/0244353', '\nAugust 2014'], ['\n2014/0278774', '\nSeptember 2014'], ['\n2014/0310157', '\nOctober 2014'], ['\n2014/0316852', '\nOctober 2014'], ['\n2014/0316855', '\nOctober 2014'], ['\n2014/0317022', '\nOctober 2014'], ['\n2014/0324538', '\nOctober 2014'], ['\n2014/0344069', '\nNovember 2014'], ['\n2015/0026039', '\nJanuary 2015'], ['\n2015/0051948', '\nFebruary 2015'], ['\n2015/0120391', '\nApril 2015'], ['\n2015/0120755', '\nApril 2015'], ['\n2015/0295906', '\nOctober 2015'], ['\n2015/0310543', '\nOctober 2015'], ['\n2015/0332414', '\nNovember 2015'], ['\n2015/0363328', '\nDecember 2015'], ['\n2016/0055487', '\nFebruary 2016'], ['\n2016/0071175', '\nMarch 2016'], ['\n2016/0092997', '\nMarch 2016'], ['\n2016/0210224', '\nJuly 2016'], ['\n2016/0246581', '\nAugust 2016'], ['\n2017/0278182', '\nSeptember 2017'], ['\n2018/0060546', '\nMarch 2018'], ['\n2018/0189871', '\nJuly 2018']]","[26, '11,328,205', '11,321,362', '11,250,517', '11,226,991', '11,201,964', '11,176,570', '11,144,938', '11,138,631', '11,126,609', '11,107,158', '11,106,868', '11,010,345', '10,991,231', '10,963,961', '10,956,684', '10,936,629', '10,930,139', '10,929,878', '10,891,691', '10,783,457', '10,692,105', '10,678,894', '10,650,449', '10,606,954', '10,580,025', '10,579,655']"," What is claimed is:  1.  A method of artificial intelligence guided segmentation of event data, the method comprising: accessing, from a data store, a plurality of event records associated with
respective users of a plurality of users, wherein a first plurality of event records associated with a first user are stored using a first quantity of storage;  accessing an event categories data structure indicating a plurality of event categories and,
for each event category, attribute criteria usable to identify events associated with respective event categories;  for the event records, identifying one or more attributes of the event record, comparing the identified one or more attributes of the
event record to the attribute criteria of respective event categories, and based on said comparing, assigning, to the event record, an event category having attribute criteria matching the identified one or more attributes of the event record; 
generating, for the first user, first compressed event data using the event records associated with the first user and a latent feature identification model, wherein the latent feature identification model takes the event records for the first user and
the event categories assigned thereto as an input, and provides association values for the first user for respective event topics identified by the first compressed event data, wherein first compressed event data associated with the first user is stored
using a second quantity of storage, the second quantity of storage being less than the first quantity of storage for storing the event records of the first user;  assigning the first user to one of a plurality of data clusters included in a clustering
model using the first compressed event data for the first user;  and generating, for the first user, second compressed event data using a comparison between the first compressed event data for the first user and an average latent feature identification
value for a latent feature included in the data cluster to which the first user has been assigned, wherein the second compressed event data associated with the first user is stored using a third quantity of storage, the third quantity of storage being
less than the second quantity of storage.
 2.  The method of claim 1, wherein assigning the first user to one of the data clusters comprises: identifying center points for each data cluster included in the clustering model;  generating an association strength for each latent feature
included in the first compressed event data for the first user for each data cluster, the association strength indicating a degree of association between the first compressed event data for a user and respective data cluster center points;  and
identifying the one of the data clusters as having the highest association strength for the first user from amongst the data clusters included in the clustering model.
 3.  The method of claim 2, wherein generating the association strength for the first user comprises comparing a latent feature identification value included in the first compressed event record for a latent feature for the first user to the
center point.
 4.  The method of claim 2, wherein generating the second compressed event data further comprises: generating a secondary association strength for each latent feature included in the first compressed event data for a user assigned to the data
cluster, the secondary association strength indicating a secondary degree of association between the first compressed event data for the user assigned to the data cluster and the secondary center point of the secondary data cluster to which the user is
not assigned, wherein the second compressed event data comprises an identifier for the secondary data cluster and the generated secondary association strengths.
 5.  The method of claim 2, further comprising: accessing content data including a content identifier and an indication of a target data cluster of the data clusters;  identifying a plurality of users assigned to the target data cluster; 
selecting a target set of users having second compressed event data including generated association strengths indicating a threshold degree of association to the center point of the target data cluster;  and generating an electronic communication to
provide to the target set of user profiles, the electronic communication including content indicated by the content identifier.
 6.  The method of claim 1, further comprising: training the latent feature identification model through probabilistic analysis of a plurality of historical event records to identify a target number of topics;  and training the clustering model
using a desired compression level indicating a number of data clusters for the clustering model, wherein training the clustering model includes generating a center point for each data cluster using topically compressed historical event data.
 7.  The method of claim 1, wherein the latent feature identification model comprises a latent dirichlet allocation model.
 8.  A method of compressing transaction data, the method comprising: receiving a plurality of transaction records each identifying a transaction by one of a plurality of users;  assigning a category to each of the plurality of transaction
records;  generating first compressed transaction records using a latent feature identification model, wherein the latent feature identification model takes the transaction records for the one of the plurality of users and categories assigned thereto as
an input, and provides association values for the one of the plurality of users for respective topics identified in the first compressed event data;  identifying a clustering compression model for the one of the plurality of users;  and generating second
compressed transaction records using the first compressed transaction records and the clustering compression model.
 9.  The method of claim 8, wherein generating a first compressed transaction record for the one of the plurality of users comprises receiving association strengths for each topic identified by the latent feature identification model for a set of
transactions for the one of the plurality of users.
 10.  The method of claim 8, further comprising: receiving a compression configuration indicating a target number of features to identify for an end user;  and training a latent dirichlet allocation model to identify the target number of features
using the received plurality of transaction records, wherein the latent feature identification model comprises the latent dirichlet allocation model.
 11.  The method of claim 8, wherein each data cluster included in the clustering compression model is associated with at least one latent feature identifiable by the latent feature identification model, and wherein generating the second
compressed transaction records comprises: assigning each user to one of the data clusters using the first compressed transaction records;  and generating the second compressed transaction records for each user using a comparison between the first
compressed transaction data for a user and the center point for the cluster to which the user is assigned.
 12.  The method of claim 11, where generating the second compressed transaction records further comprises: calculating a secondary center point for a secondary data cluster using first compressed transaction data for each user assigned to the
secondary data cluster;  and generating a secondary association strength for each latent feature included in the first compressed transaction data for a user assigned to the data cluster, the secondary association strength indicating a secondary degree
of association between the first compressed transaction data for the user assigned to the data cluster and the secondary center point of the secondary data cluster to which the user is not assigned, wherein the second compressed transaction data
comprises an identifier for the secondary data cluster and the generated secondary association strengths.
 13.  The method of claim 8, further comprising training a clustering model using the desired compression level and at least a portion of the plurality of transaction records.
 14.  The method of claim 8, further comprising: receiving, from a transaction terminal, a pending transaction record for a user included in the plurality of users, wherein the pending transaction record is not included in the plurality of
transaction records;  retrieving a second compressed transaction record for the user using an identifier of the user included in the pending transaction record;  and transmitting the second compressed transaction record to the transaction terminal.
 15.  The method of claim 14, further comprising: selecting a content element for presentation to the user during or after the pending transaction using the second compressed transaction record;  and providing the content element to a content
delivery system configured to transmit the content element to the user.
 16.  A transaction data compression system comprising: one or more computer processors configured to execute software instructions;  a non-transitory tangible storage device storing the software instructions executable by the one or more
processors to at least: access transaction data associated with a plurality of users;  for a plurality of transactions in the transaction data: assign a transaction category based on one or more attributes of the transaction;  and assign a transaction
category level for the transaction category based at least in part on on spend levels of individual users associated with the plurality of transactions;  and generate, for each user, first compressed transaction data using the transaction categories
assigned to the transaction records for a respective user and a latent feature identification model, wherein the latent feature identification model takes the event records for the first user and the event categories assigned thereto as an input, and
provides association values for the first user for respective event topics identified by the first compressed event data, wherein the first compressed transaction data associated with the one of the respective users is stored using a second quantity of
storage, the second quantity of storage being less than the first quantity of storage;  identify a clustering compression model for users included in the plurality of users;  assign each of the users to one of a plurality of data clusters included in the
respective clustering compression model using respective first compressed transaction data for the user;  and generate, for each user, second compressed transaction data using a comparison between the first compressed transaction data for a user and an
average for the data cluster to which the user has been assigned, wherein the second compressed transaction data is stored using a third quantity of storage, the third quantity of storage being less than the second quantity of storage.
 17.  The transaction data compression system of claim 16, wherein the software instructions are further executable by the one or more processors to at least: access content data including a content identifier and an indication of a target data
cluster of the data clusters;  identify a plurality of users assigned to the target data cluster;  select a target set of users having second compressed transaction data including generated association strengths indicating a threshold degree of
association to the center point of the target data cluster;  and generate an electronic communication to provide to the target set of user profiles, the electronic communication including content indicated by the content identifier.
 18.  The transaction data compression system of claim 17, wherein the software instructions are further executable by the one or more processors to at least: access the target set of users;  identify a target device for a user included in the
target set of users;  and provide the electronic communication to the target device.
 19.  The transaction data compression system of claim 16, further comprising a card reader including: one or more computer processors configured to execute software instructions;  a non-transitory tangible storage device storing the software
instructions executable by the one or more computer processors to cause the card reader to at least: detect payment information for a transaction for a user;  receive compressed transaction data during the transaction for the user;  and identify content
stored by the card reader using a comparison between a content selection rule and the compressed transaction data, said content for presentation via the card reader;  and a display configured to present the content to the user.
 20.  The transaction data compression system of claim 16, wherein the software instructions are further executable by the one or more processors to at least generate at least one of the latent feature identification model and a clustering model
identifying the plurality of data clusters for the plurality of transaction records.  "
"10,242,324","
     March 26, 2019
","Optical communication system
"," The present disclosure describes an automated, cognitive based computing
     system using artificial intelligence (AI) and machine learning algorithms
     to sense, predict, and infer network conditions, configured to
     dynamically manage transmission of information between communication
     nodes. The communication nodes comprise orbital nodes positioned in orbit
     above earth and terrestrial nodes coupled with earth interconnected via a
     hybrid mesh network topology. One or more automated, cognitive based
     physical computing processors, using artificial intelligence (AI) and
     machine learning algorithms to sense, predict, and infer network
     conditions, determine a target terrestrial node to receive information
     initially stored on a first orbital node; determine transmission
     conditions between the target terrestrial node and the first orbital node
     based on output signals from sensors; dynamically determine whether
     transmission conditions between the first orbital node and the target
     terrestrial node prevent optical transmission of the information directly
     from the first orbital node to the target terrestrial node; and,
     responsive to a determination that transmission conditions prevent
     optical transmission of the information to the target terrestrial node
     from the first orbital node, automatically transmit the information along
     an alternate route between the first orbital node and the target
     terrestrial node, wherein the alternate route includes transmission
     between some orbital node and an alternative target terrestrial node
     other than the target terrestrial node.
",H04B 10/27 (20130101); G06N 5/022 (20130101); G06N 20/00 (20190101); H04B 10/118 (20130101),H04B 10/00 (20130101); H04B 10/27 (20130101); G06N 99/00 (20190101); H04B 10/118 (20130101); G06N 5/02 (20060101),"[['\n9438341', '\nSeptember 2016'], ['\n2010/0085916', '\nApril 2010'], ['\n2010/0085948', '\nApril 2010'], ['\n2011/0286325', '\nNovember 2011'], ['\n2015/0207562', '\nJuly 2015'], ['\n2016/0065308', '\nMarch 2016'], ['\n2016/0350676', '\nDecember 2016']]",[0]," What is claimed is:  1.  An automated, cognitive based computing system using artificial intelligence (AI) and machine learning algorithms to sense, predict, and infer network conditions,
configured to dynamically manage transmission of information via optical communication signals between and among orbital and terrestrial communication nodes interconnected via a hybrid mesh network topology, the communication nodes comprising orbital
nodes positioned in orbit above earth and terrestrial nodes coupled with earth, the system comprising: one or more sensors configured to dynamically generate output signals conveying information related to transmission conditions between nodes;  and one
or more physical computer processors configured by computer readable instructions to: automatically determine an orbital node to receive information initially sourced from, stored on, or transmitted via, a source terrestrial node;  automatically
determine a target terrestrial node to receive information initially sourced from, stored on, or transmitted via, one or more orbital nodes;  dynamically determine transmission conditions between the source node and target terrestrial node(s) via one or
more orbital nodes based on the output signals;  determine whether transmission conditions between the orbital nodes, source node and target terrestrial nodes prevent optical transmission of the information;  responsive to a determination that
transmission conditions prevent optical transmission of the information to the target terrestrial node via the planned orbital node(s), automatically transmit the information along an alternate route between the orbital node(s) and the target terrestrial
node, wherein the alternate route includes transmission between one or more orbital nodes and an alternative target terrestrial node other than the target terrestrial node interconnected via a hybrid mesh network topology;  and responsive to a
determination that transmission conditions prevent optical transmission of the information from the source orbital or terrestrial node to the planned orbital node(s), automatically transmit the information along an alternate route from the source orbital
or terrestrial node to orbital node(s), wherein the alternate route includes transmission between an alternate source orbital or terrestrial node and one or more alternate orbital nodes interconnected via a hybrid mesh network topology.
 2.  The system of claim 1, wherein the one or more automated, cognitive based physical computing processors, using artificial intelligence (AI) and machine learning algorithms to sense, predict, and infer network conditions are configured such
that transmission between some orbital node and the alternative target terrestrial node includes transmission from the original orbital node to the alternative terrestrial node interconnected via a hybrid mesh network topology.
 3.  The system of claim 1, wherein the one or more automated, cognitive based physical computing processors, using artificial intelligence (AI) and machine learning algorithms to sense, predict, and infer network conditions, are configured such
that transmission between some orbital node and the alternative target terrestrial node includes transmission from one or more additional orbital nodes to the alternative terrestrial node interconnected via a hybrid mesh network topology.
 4.  The system of claim 1, wherein the one or more automated, cognitive based physical computing processors, using artificial intelligence (AI) and machine learning algorithms to sense, predict, and infer network conditions, dynamically
determine transmission conditions between the transmitting orbital node(s) and the alternative terrestrial node interconnected via a hybrid mesh network topology.
 5.  The system of claim 1, wherein the one or more automated, cognitive based physical computing processors, using artificial intelligence (AI) and machine learning algorithms to sense, predict, and infer network conditions, are configured such
that the information initially sourced from, stored on, or transmitted via, an initial orbital node is dynamically communicated to other orbital nodes and/or terrestrial nodes via optical communication signals interconnected via a hybrid mesh network
topology.
 6.  The system of claim 1, wherein the one or more automated, cognitive based physical computing processors, using artificial intelligence (AI) and machine learning algorithms to sense, predict, and infer network conditions, are configured such
that the information initially sourced from, stored on, or transmitted via, an orbital node or terrestrial node is not communicated to other orbital nodes and/or terrestrial nodes via radio frequency (RF) signals.
 7.  The system of claim 1, wherein the one or more automated, cognitive based physical computing processors, using artificial intelligence (AI) and machine learning algorithms to sense, predict, and infer network conditions-, are configured such
that the transmission conditions include one or more of forecast atmospheric conditions, ambient atmospheric conditions or other operational conditions of individual nodes interconnected via a hybrid mesh network topology.
 8.  An optical communication system composed of automated, cognitive based physical computing processors, using artificial intelligence (AI) and machine learning algorithms to sense, predict, and infer network conditions, configured to
automatically manage transmission of information between communication nodes interconnected via a hybrid mesh network topology, the system comprising: orbital nodes positioned in orbit above earth configured to communicate with each other and terrestrial
nodes by transmitting and receiving optical communication signals;  terrestrial nodes coupled with earth configured to dynamically communicate with each other and the orbital nodes by transmitting and receiving optical communication signals, the
terrestrial nodes including a source node, target terrestrial node and one or more alternative source and target terrestrial nodes;  and one or more automated, cognitive based physical computing processors, using artificial intelligence (AI) and machine
learning algorithms to sense, predict, and infer network conditions configured by computer readable instructions to facilitate transmission and receipt of the optical communication signals by the orbital nodes and the terrestrial nodes such that,
responsive to transmission conditions preventing optical communication signals from communicating information initially stored on, or transmitted via, a source orbital or terrestrial node to the target terrestrial node via one or more orbital nodes, the
one or more physical computer processors dynamically facilitate transmission of the information along an alternate route between the source orbital or terrestrial node, one or more orbital nodes and the target terrestrial node, wherein the alternate
route includes transmission of optical signals between one or more orbital nodes and an alternative source and/or alternative target terrestrial nodes interconnected via a hybrid mesh network topology.
 9.  The system of claim 8, wherein the orbital nodes and the terrestrial nodes are configured such that the optical communication signals are laser light beams.
 10.  The system of claim 8, wherein the orbital nodes and the terrestrial nodes are configured such that the optical communication signals have a frequency between about 186 THz and about 197 THz.
 11.  The system of claim 8, wherein the orbital nodes and the terrestrial nodes are configured such that the optical communication signals have a wavelength of between about 1500 nm and about 1650 nm.
 12.  The system of claim 8, wherein the orbital nodes are satellites or stratospheric vehicles.
 13.  The system of claim 8, wherein the one or more automated, cognitive based physical computing processors, using artificial intelligence (AI) and machine learning algorithms to sense, predict, and infer network conditions are configured such
that the transmission conditions include forecast and ambient atmospheric conditions.
 14.  A method for dynamically managing transmission of information between communication nodes based on automated, cognitive based physical computing processors, using artificial intelligence (AI) and machine learning algorithms to sense,
predict, and infer network conditions, the communication nodes comprising orbital nodes positioned in orbit above earth and terrestrial nodes coupled with earth interconnected via a hybrid mesh network topology, the method comprising: dynamically
generating output signals conveying information related to transmission conditions between nodes;  automatically determine an orbital node to receive information initially sourced from, stored on, or transmitted via, a source terrestrial node; 
determining a target terrestrial node to receive information initially sourced from, stored on, or transmitted via, one or more orbital nodes;  determining transmission conditions between the source node and target terrestrial node(s) via one or more
orbital nodes based on the output signals;  determining whether transmission conditions between the orbital nodes, source node, and target terrestrial node prevent optical transmission of the information;  responsive to a determination that transmission
conditions prevent optical transmission of the information to the target terrestrial node via the planned orbital node(s), transmitting the information via optical communication signals dynamically along an alternate route between the orbital node(s) and
the target terrestrial node, wherein the alternate route includes transmission of optical communication signals between one or more orbital nodes and an alternative target terrestrial node other than the target terrestrial node;  and responsive to a
determination that transmission conditions prevent optical transmission of the information from the source orbital or terrestrial node to the planned orbital node(s), automatically transmit the information along an alternate route from the source node to
orbital node(s), wherein the alternate route includes transmission between an alternate source orbital or terrestrial node and one or more alternate orbital nodes interconnected via a hybrid mesh network topology.
 15.  The system of claim 1, wherein the one or more automated, cognitive based physical computing processors, using artificial intelligence (AI) and machine learning algorithms to sense, predict, and infer network conditions, where such
AI/machine learning is configured centrally for all terrestrial nodes and/or dynamically distributed among each terrestrial node.
 16.  The system of claim 1, wherein the one or more automated, cognitive based physical computing processors, using artificial intelligence (AI) and machine learning algorithms to sense, predict, and infer network conditions, dynamically and
automatically provision, test and deliver optical routes between and among orbital nodes, terrestrial nodes and customer origin and destinations.
 17.  The system of claim 1, wherein the one or more automated, cognitive based physical computing processors, using artificial intelligence (AI) and machine learning algorithms to sense, predict, and infer network conditions, automatically
monitor, detect and isolate circuit faults along optical routes between and among orbital nodes, terrestrial nodes and customer origin and destinations.
 18.  The system of claim 1, wherein the one or more automated, cognitive based physical computing processors, using artificial intelligence (AI) and machine learning algorithms to sense, predict, and infer network conditions, use detected
weather conditions to predict and plan route switching of data sourced from, or destined to, a terrestrial node.
 19.  The system of claim 8, wherein the terrestrial nodes are fixed or mobile.  "
"10,242,425","
     March 26, 2019
","Translation of artificial intelligence representations
"," Techniques for translating graphical representations of domain knowledge
     are provided. In one example, a computer-implemented method comprises
     receiving, by a device operatively coupled to a processor, a graphical
     representation of domain knowledge. The graphical representation
     comprises information indicative of a central concept and at least one
     chain of events associated with the central concept. The
     computer-implemented method further comprises translating, by the device,
     the graphical representation into an artificial intelligence planning
     problem. The artificial intelligence planning problem is expressed in an
     artificial intelligence description language. The translating comprises
     parsing the graphical representation into groupings of terms. A first
     grouping of terms of the grouping of terms comprises an event from the at
     least one chain of events and a second grouping of terms of the grouping
     of terms comprises the information indicative of the central concept. The
     computer-implemented method also comprises validating, by the device, the
     artificial intelligence planning problem.
",G06N 5/02 (20130101); G06N 20/00 (20190101); G06T 1/20 (20130101); G06F 16/532 (20190101); G06N 5/022 (20130101),G06T 1/20 (20060101); G06N 5/02 (20060101),"[['\n6012152', '\nJanuary 2000'], ['\n7006992', '\nFebruary 2006'], ['\n8082220', '\nDecember 2011'], ['\n9177060', '\nNovember 2015'], ['\n2007/0271263', '\nNovember 2007'], ['\n2013/0041712', '\nFebruary 2013'], ['\n2013/0144917', '\nJune 2013'], ['\n2014/0052494', '\nFebruary 2014'], ['\n2015/0339580', '\nNovember 2015'], ['\n2016/0321544', '\nNovember 2016']]",[0],
"10,249,207","
     April 2, 2019
","Educational teaching system and method utilizing interactive avatars with
     learning manager and authoring manager functions
"," An educational system presents an interactive avatar representing a
     subject matter expert in a particular field on a student's device, where
     the avatar can respond to queries posed by the student, and accompany the
     response with additional supporting information. The avatar's responses
     are based on artificial intelligence comparisons between the student
     queries and a knowledge base of anticipated questions, responses and
     learning goals. An authoring manager system employs natural language
     processing to identify an underlying meaning in the student's query and
     to add semantically equivalent questions in the knowledge base and
     internet searches to aid in compiling the list of anticipated questions.
     A student profile is stored containing student information and a history
     of the student's interaction with the avatar. Learning manager software,
     using the learning goals in the knowledge base and a record of the
     conversation, can compare and assess the student's progress to the
     learning goals.
",G09B 5/065 (20130101); G06N 20/00 (20190101); G09B 7/04 (20130101); G06N 3/006 (20130101); G09B 5/08 (20130101); G06N 5/022 (20130101); G09B 7/08 (20130101),G09B 7/00 (20060101); G09B 7/08 (20060101); G09B 7/04 (20060101); G09B 5/06 (20060101); G09B 5/08 (20060101); G06N 3/00 (20060101),"[['\n5870755', '\nFebruary 1999'], ['\n8346563', '\nJanuary 2013'], ['\n9158841', '\nOctober 2015'], ['\n2008/0120558', '\nMay 2008'], ['\n2008/0221892', '\nSeptember 2008'], ['\n2012/0329029', '\nDecember 2012'], ['\n2013/0029308', '\nJanuary 2013'], ['\n2014/0310614', '\nOctober 2014'], ['\n2015/0269176', '\nSeptember 2015'], ['\n2015/0278820', '\nOctober 2015'], ['\n2016/0071302', '\nMarch 2016'], ['\n2016/0098936', '\nApril 2016']]","[2, '11,328,726', '11,189,290']"," We claim:  1.  A method for on-line teaching and assessing a student in a subject matter, utilizing an interactive avatar, comprising the steps: (a) storing on electronic data-storage means: (i)
a first database, said first database storing (A) an anticipated student query pertaining the subject matter, (B) a response to the anticipated query, (C) instructions for the delivery of said response by the interactive avatar, (D) an entry to be
employed by learning manager software, said entry identifying a learning goal for the subject matter, (E) ranking of the anticipated query to indicate a relative importance to the learning goal, and (F) a prompt or suggestion to help guide the student to
achieve the learning goal;  (ii) a second database comprising student profile information including at least one of demographic information, credential information, and prior experience, a log documenting a communication between student and avatar, and
an assessment of the student's progress in relation to the learning goal;  (iii) a third database comprising information necessary to create a graphical user interface of the interactive avatar;  (b) providing a first data entry means for entering data
to create and update the first, second and third databases, the first data entry means comprising at least one of voice, keyboard, text, touch screen, gesture interface, display screen and mouse, or other form for entering data into said first, second
and third databases and for entering a student query;  (c) receiving a query from the student pertaining to the subject matter;  (d) providing data-processing means for at least one of the following: (i) comparing a student-provided query with the
anticipated query, (ii) selecting a response to the student-provided query, (iii) providing a suggestion of a topic for the student to explore, (iv) providing a question to the student, and (v) searching the first database for a first match response
corresponding to an anticipated query stored therein;  (e) providing first data outputting means for transmitting control information comprising the selected response to at least one of: (i) a display, (ii) a mechanical apparatus, (iii) audio output, and
(iv) video output.
 2.  The method of claim 1, where the first database further comprises a test question to assess the student's understanding of the response with an expected answer to the test question.
 3.  The method of claim 1, where the data-processing means further comprises searching the first database for an alternative response corresponding to the anticipated query stored therein.
 4.  The method of claim 1 where the first primary database comprises a set of resources for each response, the set of resources comprise of at least one of: (a) a gesture, (b) a facial expression, (c) a movement, (d) a behavior, (e) a graphic,
(f) a video, (g) a simulation, or (h) another interaction to be presented by the interactive avatar, and in which the third graphical user interface database contains instructions to cause the avatar to present said set of resources.
 5.  The method of claim 1 where the first data entry means is capable of at least one of: (a) converting a voice input to text form using automatic speech recognition software;  (b) converting a non-verbal input to lexical form;  and (c)
converting at least one of a touch screen input, a gesture interface, a display screen interface, a mouse movement or other electronic input to lexical form.
 6.  The method of claim 1 where the first data entry means employs natural language processing software to do at least one of the following functions: correct spelling errors, translate slang, translate common shortcuts, correct common automatic
speech recognition translation errors, and extract underlying meaning of a query.
 7.  The method of claim 1 where the first database comprises a facility for designating key words, topics, question forms, or other forms of a query in the anticipated query.
 8.  The method of claim 1 where the first database comprises a facility for designating alternative equivalent forms for the anticipated query.
 9.  The method of claim 1 further comprising providing a query processor to match an unanticipated query to an entry in the first database for delivery of a response to the unanticipated query.
 10.  The method of claim 9 in which a secondary query matching system is optimized to improve precision and recall of the delivery of the response through at least one of (a) testing systems driven by author-managed testing and validation
content, and (b) system performance with live users, providing a feedback loop for system improvement.
 11.  The method of claim 10, in which a tertiary system is employed to find a response to a query using a secondary general knowledge base in the event that the primary and secondary query matching systems fail to find a match to the student
query.
 12.  The method of claim 1 further comprising employing an author manager system to create at least one category of anticipated query and response in the first database, the author manager system comprising: (a) second data entry means to create
or edit at least one record in the first database comprising a category of anticipated query and response;  (b) second data processing means to convert the category of anticipated query and response to a more general form of the anticipated query that
correspond to a response, said second data processing means comprising at least one of: (i) natural language processing to convert a query in the first database to a basic query, (ii) means to identify key words and assign appropriate weighting to
emphasize important parts of the query and de-emphasize unimportant parts of the query, (iii) means to improve query matching using a dictionary of synonyms, (iv) means to create an equivalent query based on a type of the query, (v) means to create an
equivalent query using a list of equivalent words or phrases particular to the interactive avatar and/or subject matter, (vi) natural language processing to create an equivalent query to improve a likelihood of a correct match, and (vii) means to search
internet search engines for a variant of an anticipated query and retrieve such variant;  (c) second data processing output means to convert a category of query, equivalent query and response into a format required by the second data processing means for
selecting a response corresponding to the student query.
 13.  The method of claim 12 in which at least one anticipated query is obtained from an external data set of queries.
 14.  The method of claim 12 in which at least one anticipated query is obtained from an external search engine data set.
 15.  The method of claim 12 in which the interactive avatar is employed to solicit both an anticipated query and a response from an author to create the category in the first database.
 16.  The method of claim 1 in which a scripting or markup language is employed to select a response corresponding to the student query.
 17.  The method of claim 1 where the first data processing means further comprises using learning manager software to analyze the student's progress in making a query, receiving a response, and answering a test question, and providing an
assessment of the student's achievement of the learning goal.
 18.  The method of claim 1 in which the learning manager software employs adaptive guidance to the student's progress toward the learning goal, encouraging learning of a lesser explored content by the student based on the student's progress in
relation to the learning goal.
 19.  The method of claim 1, where the third database further comprises a set of accessories, objects, architecture, and other elements of an environment for the interactive avatar.
 20.  A system for teaching subject matter by allowing students to ask questions of a subject matter expert interactive avatar, the system comprising: a first database storing: (a) an anticipated query pertaining to the subject matter, (b) a
response corresponding to the anticipated query, and (c) an instruction for the delivery by the interactive avatar of the response to the anticipated query;  a second database storing student profile information, comprising a student's progress
information;  a third database storing user interface information, the user information comprising information for presenting the interactive avatar;  a first data input means for receiving an input query and for creating and augmenting at least one of:
(a) the first database, (b) the second database, and (c) the third database;  data output means for transmitting at least one of: (a) the response, (b) the input query, and (c) student progress information;  and data processing means for (a) searching
the first database for a first match response of the input query corresponding to an anticipated query stored in the first database;  and capable of at least one of: (b) delivering the first match response through the data output means;  and (c)
analyzing a student's progress in understanding the educational subject matter.
 21.  The system of claim 20, further comprising: a fourth database storing general information pertaining to at least one of the subject matter and the interactive avatar;  and means for searching the fourth database to provide a response to a
query in the event no match is found in the first database.
 22.  The system of claim 20, further comprising: second data processing means capable of analyzing a response or a query which are not found in the first database or do not result in a first match response, and identifying an appropriate entry
for addition to the first database to produce a first match response for a future response or future query.
 23.  The system of claim 20, where the data output means further comprises transmission of at least one of: (a) a two dimensional image, (b) a three dimensional image, (c) audio output, (d) video output, and (e) control of a mechanical device.
 24.  The system of claim 20, where the first data input means further comprises receiving at least one of: (a) text entry, (b) video capture, (c) audio input, and (d) touch screen input.
 25.  The system of claim 20, further comprising an authoring manager, the authoring manager comprising: (a) second data entry means to create or edit at least one record in the first database comprising a category of anticipated query and
response;  (b) second data processing means to convert the category of anticipated query and response to a more general form of the anticipated query that correspond to a paired response, said second data processing means comprising at least one of: (i)
natural language processing to convert a query in the first database to a basic query, (ii) means to identify key words in the basic query and designate flexible matching patterns and variable word weighting within the basic query, (iii) means to improve
query matching using a dictionary of synonyms, (iv) means to create an equivalent query based on a type of the query, (v) means to create an equivalent query using a list of equivalent words or phrases particular to the interactive avatar or subject
matter, (vi) natural language processing to create an equivalent query to improve a likelihood of correct match, and (vii) means to search internet search engines for a variant of one or more anticipated queries and retrieve such variant;  (c) electronic
data processing output means to convert a category of query, an equivalent query and response into a format required by the second data processing means for selecting a response corresponding to the student query.
 26.  The system of claim 20 in which the interactive avatar is employed to solicit both an anticipated query and a response from an author to create an entry in the first database.
 27.  The system of claim 20, where the first database further comprises at least one of: (d) a learning goal pertaining to the subject matter, (e) a follow up question to assess the student's understanding of the response, (f) guidance to help
the student achieve the learning goal, and (g) a supplemental query and response thereto for enhancing the first database.  "
"10,250,574","
     April 2, 2019
","Systems and methods for encoded communications
"," Systems and methods for encoded communications are disclosed. In some
     embodiments, a server system may be configured to receive a communication
     from a user interface at an encoded communication module that includes an
     artificial intelligence based natural language processing module,
     determine whether the received communication is an encoded communication,
     decode the encoded communication to generate a financial query when it is
     determined that the received communication is an encoded communication,
     retrieve financial data associated with the user, determine an answer to
     the financial query based on the retrieved financial data, encode the
     determined answer to generate an encoded responsive communication, and
     transmit the generated encoded responsive communication to the user
     interface for providing to a user of the user interface.
",H04L 63/0876 (20130101); G06N 20/00 (20190101); G06N 5/02 (20130101); G06Q 10/067 (20130101); H04L 63/0428 (20130101); H04L 63/083 (20130101); G06F 16/9535 (20190101); G06F 21/6245 (20130101); G06F 16/248 (20190101),G06F 21/60 (20130101); H04L 29/06 (20060101); G06F 21/44 (20130101); G06N 5/02 (20060101); G06Q 10/06 (20120101); G06F 21/62 (20130101),"[['\n2009/0083548', '\nMarch 2009'], ['\n2017/0024742', '\nJanuary 2017']]",[0]," The invention claimed is:  1.  A method comprising: receiving, by an encoded communication module of a server system, a communication from a user interface, wherein the encoded communication
module further comprises an artificial intelligence based natural language processing module;  receiving, by the server system, user preferences from the user interface, and storing the user preferences on a database communicatively coupled to the server
system;  determining, by the encoded communication module, whether the received communication is an encoded communication;  generating, by the encoded communication module, a financial query when it is determined that the received communication is an
encoded communication and providing the financial query to a query response module;  determining, by the query response module, a response to the financial query and providing the determined response to the encoded communication module;  encoding, by the
encoded communication module, the response to the financial query to generate an encoded responsive communication;  and transmitting, by the server system, the generated encoded responsive communication to the user interface for presentation to a user of
the user interface, wherein at least one of determining whether the received communication is an encoded communication, generating the financial query, and encoding the response to the financial query is based at least in part on the stored user
preferences.
 2.  The method of claim 1, wherein determining whether the received communication is an encoded communication further comprises: determining a semantic representation of the received communication;  and matching the semantic representation to at
least one user designated query stored in a database coupled to the server system.
 3.  The method of claim 2, wherein generating the financial query further comprises matching the at least one user designated query to an associated financial query stored in the database.
 4.  The method of claim 1, wherein determining the response to the financial query further comprises: retrieving user financial data;  and generating the response to the financial query based on the retrieved user financial data.
 5.  The method of claim 4, wherein retrieving user financial data further comprises: retrieving user financial data from one or more financial services databases remote from the server system via a financial information application interface.
 6.  The method of claim 1, further comprising: generating, by the encoded communication module, a non-responsive encoded communication in response to the received communication from the user interface when it is determined that the received
communication is not an encoded communication.
 7.  The method of claim 1, further comprising: authenticating at least one of the user device and the user of the user device by verification of data provided to the server system against user authentication data stored in a database, wherein
the user authentication data further comprises at least one of a password, an encoded password, a phrase, an encoded phrase, a question, an encoded question, a series of communications, an encoded series of communications, a pin, a phone number
corresponding to where the communication is received from, an email address corresponding to where the communication is received from, a social media account corresponding to where the communication is received from, and a phone line at which the user
communication is received.
 8.  The method of claim 1, wherein the financial query further comprises at least one of a balance query, a transaction status query, a credit score query, a card status query, a credit limit query, a loan status query, and a rewards query.
 9.  A system for encoded communication comprising: a processor;  and non-volatile memory storing computer program code that when executed on the processor causes the processor to execute a process operable to: receive a communication from a user
interface at an encoded communication module that further comprises an artificial intelligence based natural language processing module;  receive user preferences transmitted from the user interface and store the received user preferences on a database
communicatively coupled to the processor;  determine whether the received communication is an encoded communication;  decode the encoded communication to generate a financial query when it is determined that the received communication is an encoded
communication;  retrieve financial data associated with the user;  determine an answer to the financial query based on the retrieved financial data;  encode the determined answer to generate an encoded responsive communication;  and transmit the
generated encoded responsive communication to the user interface for providing to a user of the user interface, wherein at least one of determines whether the received communication is an encoded communication, generates the financial query, and encodes
the response to the financial query is based at least in part on the stored user preferences.
 10.  The system of claim 9, wherein when determining whether the received communication is an encoded communication, the processor is further configured to: determine a semantic representation of the received communication;  and match the
semantic representation to at least one user designated query stored in a database coupled to the server system.
 11.  The system of claim 9, wherein when generating the financial query, the processor is further configured to match the at least one user designated query to an associated financial query stored in the database.
 12.  The system of claim 9, wherein when determining a response to the financial query, the processor is further configured to: retrieve user financial data;  and generate a response to the financial query based on the retrieved user financial
data.
 13.  The system of claim 9, wherein the processor is further configured to: generate, by the encoded communication module, a non-responsive encoded communication in response to the received communication from the user interface when it is
determined that the received communication is not an encoded communication.
 14.  The system of claim 9, wherein the processor is further configured to: authenticate at least one of the user device and the user of the user device by verification of data provided to the processor against user authentication data stored in
a database, wherein the user authentication data further comprises at least one of a password, an encoded password, a phrase, an encoded phrase, a question, an encoded question, a series of communications, an encoded series of communications, a pin, a
phone number corresponding to where the communication is received from, an email address that the communication is received from, a social media account that the communication is received from, and a phone line at which the communication is received.
 15.  The system of claim 9, wherein the user interface is configured to receive from and transmit to the processor, at least one of text based and audio based communications.
 16.  The system of claim 9, wherein the financial query further comprises at least one of a balance query, a transaction status query, a credit score query, a card status query, a credit limit query, a loan status query, and a rewards query.
 17.  A user device comprising: a display;  an input device;  a processor in communication with the display and the input device;  and a non-transitory memory in communication with the processor, the memory storing instructions that, when
executed by the processor, cause the processor to perform processing comprising: displaying, on the display of the user device, one or more user preference prompts related to user preferences;  receiving user preferences from the input device responsive
to the displayed one or more user preference prompts;  sending a communication from a user of the user device to a server system communicatively coupled with the user device, wherein the communication is associated with a financial query;  transmitting
the received user preferences to a database communicatively coupled to the server system;  receiving, from the server system, a responsive communication from the user interface responsive to the communication, wherein the responsive communication is
associated with an answer to the financial query and generated using an artificial intelligence based natural language processing module;  and displaying on the display of the user device, the responsive communication, wherein the responsive
communication is generated at least in part based on the user preferences.  "
"10,251,115","
     April 2, 2019
","System and method for intelligent assistant service
"," Artificial intelligence-based, intelligent agent (IA) services may
     include an IA server assisting users of a wireless network in various
     communication scenarios (e.g., calls, texts, chats, etc.). A user may
     specify rules for managing communications directed to a User Equipment
     (UE) of the user. Examples of such rules may include intercepting an
     incoming call to the user, managing the call based on whether the user is
     available, determining a reason for an incoming call and notifying the
     user about the reason, inviting the user to join the call, adding other
     users to the call, recording portions of the call, providing requested
     information, taking notes, scheduling meetings, and providing other
     assistant-type services, etc. The IA services may also include
     monitoring, interpreting, and responding to information that is sent to,
     or by, the user, during a communication, in addition to implementing
     Machine Learning procedures for self-improvement.
",H04W 4/16 (20130101); H04W 40/02 (20130101); H04L 65/1073 (20130101); H04W 76/10 (20180201); H04L 65/1016 (20130101); H04W 4/08 (20130101),H04W 40/00 (20090101); H04W 40/02 (20090101); H04W 76/10 (20180101),"[['\n7653191', '\nJanuary 2010'], ['\n8837704', '\nSeptember 2014'], ['\n9736308', '\nAugust 2017'], ['\n2005/0070282', '\nMarch 2005'], ['\n2005/0195802', '\nSeptember 2005'], ['\n2006/0072726', '\nApril 2006'], ['\n2007/0047522', '\nMarch 2007'], ['\n2010/0310062', '\nDecember 2010'], ['\n2011/0044442', '\nFebruary 2011'], ['\n2013/0094637', '\nApril 2013']]","[4, '11,349,798', '11,206,190', '11,025,687', '10,594,748']"," What is claimed is:  1.  A server device, comprising: a non-transitory memory device storing a plurality of processor-executable instructions;  and one or more processors configured to execute
the processor-executable instructions, wherein executing the processor-executable instructions causes the one or more processors to: determine that a first UE, to which a call request from a second UE is directed, is registered for intelligent assistant
(IA) services that are configured to assist the first UE during communication sessions with other UEs;  identify, based on the call request, rules associated with the first UE for providing the IA services;  determine, by applying the rules to the call
request, that the call request is to be routed to the server device instead of to the first UE;  cause the call request to be rerouted to the server device by establishing a communication session between the server device and the second UE;  request,
based on the rerouting, and via the established communication session with the UE, information regarding a reason for the call from the second UE;  provide, to the first UE: the information regarding the reason for the call request, from the second UE,
and in conjunction with the information regarding the reason for the call request, an option to join the established communication session;  receive, from the first UE, a selection of the option to join the established communication session;  and add the
first UE to the communication session based on receiving the selection of the option to join the communication session.
 2.  The server device of claim 1, wherein executing the processor-executable instructions causes the one or more processors further to: receive, from a Telephony Application Server (TAS) of an Internet Protocol (IP) Multimedia Subsystem (IMS), a
request to determine whether the first UE is registered for the IA services, the request including an identifier of the first UE;  and use the identifier to query a database of user accounts registered for IA services;  and determine, based on a user
account resulting from the query, that the first UE is registered for IA services.
 3.  The server device of claim 2, wherein the identifier is a Mobile Device Number (MDN) of the first UE.
 4.  The server device of claim 1, wherein executing the processor-executable instructions causes the one or more processors to: cause the call request to be rerouted to the server device by: notifying a Telephony Application Server (TAS) of an
Internet Protocol (IP) Multimedia Subsystem (IMS) that the first UE is registered for the IA services.
 5.  The server device of claim 1, wherein executing the processor-executable instructions, to request the information regarding the reason for the call request from the second UE, further causes the one or more processors to: receive, from the
second UE and in response to the request for the information, a message from the second UE;  and use an artificial intelligence (AI) technique to interpret a meaning of the message, wherein the reason for the call request from the UE is determined based
on the interpreted meaning.
 6.  A method, comprising: determining, by a server device, that a first UE, to which a call request from a second UE is directed, is registered for intelligent assistant (IA) services that are configured to assist the first UE during
communication sessions with other UEs;  identifying, by the server device and based on the call request, rules associated with the first UE for providing the IA services;  determining, by the server device and by applying the rules to the call request,
that the call request is to be routed to the server device instead of to the first UE;  causing, by the server device, the call request to be rerouted to the server device by establishing a communication session between the server device and the second
UE;  requesting, by the server device and based on the rerouting, and via the established communication session with the UE, information regarding a reason for the call from the second UE;  providing, by the server device and to the first UE: the
information regarding the reason for the call request, from the second UE, and in conjunction with the information regarding the reason for the call request, an option to join the established communication session;  receiving, by the server device and
from the first UE, a selection of the option to join the established communication session;  and add the first UE to the communication session based on receiving the selection of the option to join the communication session.
 7.  The method of claim 6, further comprising: receiving, from a Telephony Application Server (TAS) of an Internet Protocol (IP) Multimedia Subsystem (IMS), a request to determine whether the first UE is registered for the IA services, the
request including an identifier of the first UE;  and using the identifier to query a database of user accounts registered for IA services;  and determining, based on a user account resulting from the query, that the first UE is registered for IA
services.
 8.  The method of claim 7, wherein the identifier is a Mobile Device Number (MDN) of the first UE.
 9.  The method of claim 6, wherein causing the call request to be rerouted to the server device includes: notifying a Telephony Application Server (TAS) of an Internet Protocol (IP) Multimedia Subsystem (IMS) that the first UE is registered for
the IA services.
 10.  The method of claim 6, wherein requesting the information regarding the reason for the call request from the second UE further includes: receiving, from the second UE and in response to the request for the information, a message from the
second UE;  and using an artificial intelligence (AI) technique to interpret a meaning of the message, wherein the reason for the call request from the UE is determined based on the interpreted meaning.
 11.  A non-transitory computer readable medium storing a plurality of processor-executable instructions, wherein executing the processor-executable instructions causes one or more processors to: determine that a first UE, to which a call request
from a second UE is directed, is registered for intelligent assistant (IA) services that are configured to assist the first UE during communication sessions with other UEs;  identify, based on the call request, rules associated with the first UE for
providing the IA services;  determine, by applying the rules to the call request, that the call request is to be routed to the server device instead of to the first UE;  cause the call request to be rerouted to the server device by establishing a
communication session between the server device and the second UE;  provide, to the first UE: the information regarding the reason for the call request, from the second UE, and in conjunction with the information regarding the reason for the call
request, an option to join the established communication session;  receive, from the first UE, a selection of the option to join the established communication session;  and add the first UE to the communication session based on receiving the selection of
the option to join the communication session.
 12.  The non-transitory computer readable medium of claim 11, wherein executing the processor-executable instructions causes the one or more processors further to: receive, from a Telephony Application Server (TAS) of an Internet Protocol (IP)
Multimedia Subsystem (IMS), a request to determine whether the first UE is registered for the IA services, the request including an identifier of the first UE;  and use the identifier to query a database of user accounts registered for IA services;  and
determine, based on a user account resulting from the query, that the first UE is registered for IA services.
 13.  The non-transitory computer readable medium of claim 12, wherein the identifier is a Mobile Device Number (MDN) of the first UE.
 14.  The non-transitory computer readable medium of claim 11, wherein executing the processor-executable instructions causes the one or more processors further to: cause the call request to be rerouted to the server device by: notifying a
Telephony Application Server (TAS) of an Internet Protocol (IP) Multimedia Subsystem (IMS) that the first UE is registered for the IA services.
 15.  The non-transitory computer-readable medium of claim 11, wherein the processor-executable instructions, to request the information regarding the reason for the call request from the second UE, further include processor-executable
instructions to: receive, from the second UE and in response to the request for the information, a message from the second UE;  and use an artificial intelligence (AI) technique to interpret a meaning of the message, wherein the reason for the call from
the UE is determined based on the interpreted meaning.
 16.  The non-transitory computer-readable medium of claim 15, wherein the meaning of the message indicates that at least a third UE should be added to the communication session, wherein executing the processor-executable instructions further
causes the one or more processors to: contact the third UE based on determining that the meaning of the message indicates that at least the third UE should be added to the communication session;  and add, based on contacting the third UE, the third UE to
the call session, such that the first, second, and third UEs are simultaneously engaged in the communication session after the third UE has been added.
 17.  The non-transitory computer-readable medium of claim 15, wherein the message from the second UE includes a Short Messaging Service (""SMS"") message.
 18.  The server device of claim 5, wherein the meaning of the message indicates that at least a third UE should be added to the communication session, wherein executing the processor-executable instructions further causes the one or more
processors to: contact the third UE based on determining that the meaning of the message indicates that at least the third UE should be added to the communication session;  and add, based on contacting the third UE, the third UE to the call session, such
that the first, second, and third UEs are simultaneously engaged in the communication session after the third UE has been added.
 19.  The server device of claim 5, wherein the message from the second UE includes a Short Messaging Service (""SMS"") message.
 20.  The method of claim 10, wherein the meaning of the message indicates that at least a third UE should be added to the communication session, the method further comprising: contacting the third UE based on determining that the meaning of the
message indicates that at least the third UE should be added to the communication session;  and adding, based on contacting the third UE, the third UE to the call session, such that the first, second, and third UEs are simultaneously engaged in the
communication session after the third UE has been added.  "
"10,252,160","
     April 9, 2019
","System for personalizing content presented in an avatar wait state
"," Customized wait state experiences for an avatar in a virtual universe are
     provided by determining user alternative wait context preferences,
     selecting alternative wait state content objects accordingly, creating a
     wait state context in the virtual universe system using the alternative
     wait state content objects, and transporting the waiting avatar to the
     newly created wait state context for the duration of the wait. The custom
     context can include scenery, sounds, and even information streams and
     feeds. The avatar is optionally returned to the original context upon an
     event, such as a customer service representative, being available and
     ready to interact with the user. Artificial intelligence processes are
     optionally used to determine probable wait state context preferences for
     the user. Rewards may be given to the user for waiting, and for
     completing games or challenges.
",A63F 13/12 (20130101); A63F 13/45 (20140902); A63F 13/79 (20140902); A63F 13/88 (20140902); A63F 2300/5553 (20130101); A63F 2300/6036 (20130101); A63F 2300/636 (20130101); A63F 2300/5593 (20130101),A63F 13/45 (20140101); A63F 13/88 (20140101); A63F 13/30 (20140101); A63F 13/79 (20140101),"[['\n5718632', '\nFebruary 1998'], ['\n5768122', '\nJune 1998'], ['\n5848396', '\nDecember 1998'], ['\n5880731', '\nMarch 1999'], ['\n6271843', '\nAugust 2001'], ['\n6608636', '\nAugust 2003'], ['\n6772195', '\nAugust 2004'], ['\n6774885', '\nAugust 2004'], ['\n7225464', '\nMay 2007'], ['\n2001/0029506', '\nOctober 2001'], ['\n2002/0113820', '\nAugust 2002'], ['\n2003/0156134', '\nAugust 2003'], ['\n2005/0266925', '\nDecember 2005'], ['\n2007/0118420', '\nMay 2007'], ['\n2007/0130001', '\nJune 2007'], ['\n2009/0124349', '\nMay 2009']]","[2, '11,291,919', '10,617,961']"," What is claimed is:  1.  A method for customizing a wait state experience for an attendee user represented by an attendee avatar in a virtual universe, the method comprising: during a default
context of a virtual universe, detecting, by a server computer having a virtual world engine, that an attendee avatar is entering a wait state for a virtual world event, wherein the default context comprises a virtual meeting room or virtual class room
having a virtual presentation screen and a presenter avatar;  inferring, by the virtual world engine of the server computer, one or more wait state context preferences for an attendee user which are not explicitly established by the attendee user, the
inferring including artificial intelligence analysis of one or more items selected from the group consisting of the content of an attendee user inventory, one or more past conversations in the virtual universe default context by the attendee user, an
appearance factor of the avatar representing the attendee user, and a friend list for the attendee user in the default context of the virtual universe;  selecting, by the virtual world engine of the server computer, at least one game content object
according to the one or more wait state context preferences;  creating, by the virtual world engine of the server computer, a wait state context in the virtual universe containing the at least one game content object;  transporting, by the virtual world
engine of the server computer, the attendee avatar to the wait state context and allowing the attendee user to interact with the at least one game content object;  and returning, by the virtual world engine of the server computer, the attendee avatar to
the default context responsive to the virtual world event starting or occurring.
 2.  The method as set forth in claim 1 wherein the inferring one or more wait state context preferences further comprises accessing, by the virtual world engine of the server computer, a stored preference profile.
 3.  The method as set forth in claim 1 wherein the inferring further comprises artificial intelligence analysis of past waiting room preferences for the attendee user.
 4.  The method as set forth in claim 1 wherein the event comprises reaching an end of a queue for a service by the attendee avatar.
 5.  The method as set forth in claim 4 wherein the service comprises conferencing the attendee avatar to at least one avatar representing one or more facilitator users selected from the group consisting of a customer service representative, an
educator, a trainer, and a business person.
 6.  The method as set forth in claim 5 further comprising providing, by the virtual world engine of the server computer, to the customer service representative, one or more information objects regarding activities of the attendee user during the
wait state context.
 7.  The method as set forth in claim 1 further comprising depositing, by the virtual world engine of the server computer, a reward object into a attendee user inventory responsive to returning the attendee avatar to the default context.
 8.  The method as set forth in claim 1 further comprising advancing, by the virtual world engine of the server computer, a position of the attendee avatar within a queue responsive to completion of a game or challenge, wherein the event
comprises reaching an end of the queue.
 9.  The method as set forth in claim 1 wherein the selecting further comprises selecting at least one environment and scenery object, and wherein the creating includes the selected environment or scenery object.
 10.  The method as set forth in claim 1 wherein the selecting further comprises selecting an information stream object, and wherein the creating includes the information stream object.
 11.  The method as set forth in claim 1 wherein the selecting further comprises selecting a challenge object, and wherein the creating includes the selected challenge object.
 12.  The method as set forth in claim 1 wherein the selecting further comprises selecting a reward object deposited to an attendee user inventory.
 13.  The method as set forth in claim 12 wherein the event comprises the attendee avatar being in the wait state context for a predetermined period of time.
 14.  The method as set forth in claim 13 wherein the event comprises completion of a game or challenge object.
 15.  A computer program product for customizing a wait state experience for an attendee user represented by an attendee avatar in a virtual universe, comprising: a tangible, computer-readable computer memory which is not a propagating signal per
se;  one or more program instructions encoded by the computer memory, for causing a virtual world engine of a computer server to, when executed, perform steps comprising: during a default context of a virtual universe, detecting that an attendee avatar
is entering a wait state for a virtual world event, wherein the default context comprises a virtual meeting room or virtual class room having a virtual presentation screen and a presenter avatar;  inferring one or more wait state context preferences for
an attendee user which are not explicitly established by the attendee user, the inferring including artificial intelligence analysis of one or more items selected from the group consisting of the content of an attendee user inventory, one or more past
conversations in the virtual universe default context by the attendee user, an appearance factor of the avatar representing the attendee user, and a friend list for the attendee user in the default context of the virtual universe;  selecting at least one
game content object according to the one or more wait state context preferences;  creating a wait state context in the virtual universe containing the at least one game content object;  transporting the attendee avatar to the wait state context and
allowing the attendee user to interact with the at least one game content object;  and returning the attendee avatar to the default context responsive to the virtual world event starting or occurring.
 16.  The computer program product as set forth in claim 15 wherein the event comprises reaching an end of a queue for a service by the attendee avatar, and wherein the service comprises conferencing the attendee avatar to at least one avatar
representing one or more facilitator users selected from the group consisting of a customer service representative, an educator, a trainer, and a business representative person.
 17.  A system for customizing a wait state experience for an attendee user represented by an attendee avatar in a virtual universe, comprising: a computer server having a processor and a virtual world engine;  a tangible, computer-readable
computer memory which is not a propagating signal per se;  one or more program instructions encoded by the computer memory, for causing the virtual world engine of the computer server to, when executed, perform steps comprising: during a default context
of a virtual universe, detecting that an attendee avatar is entering a wait state for a virtual world event, wherein the default context comprises a virtual meeting room or virtual class room having a virtual presentation screen and a presenter avatar; 
inferring one or more wait state context preferences for an attendee user which are not explicitly established by the attendee user, the inferring including artificial intelligence analysis of one or more items selected from the group consisting of the
content of an attendee user inventory, one or more past conversations in the virtual universe default context by the attendee user, an appearance factor of the avatar representing the attendee user, and a friend list for the attendee user in the default
context of the virtual universe;  selecting at least one game content object according to the one or more wait state context preferences;  creating a wait state context in the virtual universe containing the at least one game content object; 
transporting the attendee avatar to the wait state context and allowing the attendee user to interact with the at least one game content object;  and returning the attendee avatar to the default context responsive to the virtual world event starting or
occurring.
 18.  The system as set forth in claim 17 wherein the event comprises reaching an end of a queue for a service by the attendee avatar, and wherein the service comprises conferencing the attendee avatar to at least one avatar representing one or
more facilitator users selected from the group consisting of a customer service representative, an educator, a trainer, and a business representative person.  "
"10,252,419","
     April 9, 2019
","System and method for robotic delivery between moving targets
"," A robot for delivering items within a building or within a prescribed
     radius of a building are provided. A method comprises receiving a task
     indicating a non-stationary origin and a destination; identifying a
     current location of the non-stationary origin by interrogating a remote
     computer associated with the non-stationary origin for the current
     location of the non-stationary origin; moving towards the current
     location of the non-stationary origin; determining that the
     non-stationary origin has changed location by interrogating the remote
     computer associated with the non-stationary origin for an updated current
     location of the non-stationary origin; predicting a next location of the
     non-stationary origin using an artificial intelligence prediction
     algorithm; determining that the robot has arrived at the origin;
     detecting an interaction with the robot that is associated with
     introducing an item to or removing an item from a storage compartment in
     the robot; moving towards the destination inside of the building.
",G05D 1/0088 (20130101); B25J 9/1664 (20130101); G05D 1/12 (20130101); B25J 9/161 (20130101); G05D 1/0282 (20130101); G05D 1/0274 (20130101); G05B 2219/39473 (20130101); G05D 2201/0206 (20130101),B25J 9/16 (20060101); G05D 1/00 (20060101); G05D 1/02 (20060101),"[['\n5959574', '\nSeptember 1999'], ['\n6382554', '\nMay 2002'], ['\n9741010', '\nAugust 2017'], ['\n2011/0231016', '\nSeptember 2011'], ['\n2012/0123668', '\nMay 2012'], ['\n2013/0110281', '\nMay 2013'], ['\n2014/0136414', '\nMay 2014'], ['\n2014/0254896', '\nSeptember 2014'], ['\n2015/0006005', '\nJanuary 2015'], ['\n2015/0227882', '\nAugust 2015'], ['\n2016/0187886', '\nJune 2016'], ['\n2016/0250933', '\nSeptember 2016'], ['\n2017/0242438', '\nAugust 2017'], ['\n2017/0286892', '\nOctober 2017'], ['\n2017/0300855', '\nOctober 2017']]","[3, '11,327,503', '11,325,250', '11,209,832']"," What is claimed is:  1.  A robot to navigate from a non-stationary origin to a destination within a building, the robot comprising: one or more sensors that collect sensor data;  one or more
processors;  a non-transitory computer-readable medium having instructions embodied thereon, the instructions, when executed by the one or more processors, perform operations comprising: receiving a task comprising navigating from a non-stationary origin
to a destination;  identifying a current location of the non-stationary origin by interrogating a remote computer having information about the non-stationary origin for the current location of the non-stationary origin;  moving towards the current
location of the non-stationary origin;  while moving towards the current location of the non-stationary origin: determining that the non-stationary origin has changed location by interrogating the remote computer associated with the non-stationary origin
for an updated current location of the non-stationary origin;  predicting a next location of the non-stationary origin using an artificial intelligence prediction algorithm;  calculating an updated route to the updated current location of the
non-stationary origin;  moving towards the predicted next location of the non-stationary origin via the updated route;  determining whether the robot has arrived at the non-stationary origin;  in response to determining that the robot has not arrived at
the non-stationary origin, repeating the determining, predicting, calculating, and moving until the robot has arrived at the non-stationary origin;  detecting an interaction with the robot that is associated with introducing an item to or removing an
item from a storage compartment in the robot;  moving towards the destination within the building.
 2.  The robot of claim 1, further comprising determining that the robot is within a radius of the non-stationary origin and, sending to the remote computer, an electronic message indicating that the robot is within the radius.
 3.  The robot of claim 1, wherein the destination is stationary.
 4.  The robot of claim 1, wherein the destination is non-stationary.
 5.  The robot of claim 1, wherein the remote computer is a non-stationary remote computer.
 6.  The robot of claim 1, wherein the remote computer is a smartphone or vehicle navigation system.
 7.  The robot of claim 1, wherein the current location of the non-stationary origin is identified using a global positioning system (GPS) or WIFI receiver of the remote computer.
 8.  The robot of claim 1, wherein the non-stationary origin is a vehicle outside of the building.
 9.  The robot of claim 1, wherein the non-stationary origin is a person inside or within a distance from the building.
 10.  The robot of claim 1, further comprising: using the current location of the non-stationary origin, determining a time of arrival of the non-stationary origin at an anticipated location, and delaying moving towards the non-stationary origin
based on the time of arrival.
 11.  A robot to navigate from an origin within a building to a non-stationary destination, the robot comprising: one or more sensors that collect sensor data;  one or more processors;  a non-transitory computer-readable medium having
instructions embodied thereon, the instructions, when executed by the one or more processors, perform operations comprising: receiving a task comprising navigation from an origin to a non-stationary destination;  navigating to the origin;  at the origin,
detecting an interaction with the robot that is associated with introducing an item to a storage compartment of the robot;  locking the storage compartment;  identifying a current location of the non-stationary destination by interrogating a remote
computer having information about the non-stationary destination for the current location of the non-stationary destination;  moving towards the current location of the non-stationary destination;  while moving towards the current location of the
non-stationary destination: determining that the non-stationary destination has changed location by interrogating the remote computer for an updated current location of the non-stationary destination;  predicting a next location of the non-stationary
destination using an artificial intelligence prediction algorithm;  calculating an updated route to the updated current location of the non-stationary destination;  moving towards the predicted next location of the non-stationary destination via the
updated route;  determining whether the robot has arrived at the non-stationary destination;  in response to determining that the robot has not arrived at the non-stationary destination, repeating the determining, predicting, calculating, and moving
until the robot has arrived at the non-stationary destination;  unlocking the storage compartment.
 12.  The robot of claim 11, further comprising determining that the robot is within a radius of the non-stationary destination and, sending to the remote computer, an electronic message indicating that the robot is within the radius.
 13.  The robot of claim 11, wherein the origin is stationary.
 14.  The robot of claim 11, wherein the origin is non-stationary.
 15.  The robot of claim 11, wherein the remote computer is a non-stationary remote computer.
 16.  The robot of claim 11, wherein the remote computer is a smartphone or vehicle navigation system.
 17.  The robot of claim 11, wherein the current location of the non-stationary destination is identified using a global positioning system (GPS) or WIFI receiver of the remote computer.
 18.  The robot of claim 11, wherein the non-stationary destination is a vehicle outside of the building.
 19.  The robot of claim 11, wherein the non-stationary destination is a person inside or within a distance from the building.
 20.  The robot of claim 11, further comprising authenticating the destination.
 21.  The robot of claim 20, wherein authenticating the destination comprises at least one of: requesting and receiving a PIN from a person, matching a voice print of the person, performing a retinal scan of the person, using facial recognition
of the person, authenticating a fingerprint of the person, confirming the presence of a smartphone using RFID, requesting that the person swipe a credit card associated with the task, by receiving a signature from the person via a touchscreen, by
scanning a QR code displayed by the smartphone, or by scanning an identification card associated with the person.
 22.  A method for navigating a robot from a non-stationary origin to a destination within a building, the method comprising: receiving a task comprising navigating from a non-stationary origin to a destination;  identifying a current location of
the non-stationary origin by interrogating a remote computer having information about the non-stationary origin for the current location of the non-stationary origin;  moving towards the current location of the non-stationary origin;  while moving
towards the current location of the non-stationary origin: determining that the non-stationary origin has changed location by interrogating the remote computer for an updated current location of the non-stationary origin;  predicting a next location of
the non-stationary origin using an artificial intelligence prediction algorithm;  calculating an updated route to the updated current location of the non-stationary origin;  moving towards the predicted next location of the non-stationary origin via the
updated route;  determining whether the robot has arrived at the non-stationary origin;  in response to determining that the robot has not arrived at the non-stationary origin, repeating the determining, predicting, calculating, and moving until the
robot has arrived at the non-stationary origin;  detecting an interaction with the robot that is associated with introducing an item to or removing an item from a storage compartment in the robot;  moving towards the destination within the building. 
"
"10,255,387","
     April 9, 2019
","Modeling of crop growth for desired moisture content of bovine feedstuff
     and determination of harvest windows for high-moisture corn using
     field-level diagnosis and forecasting of weather conditions and
     observations and user input of harvest condition states
"," A modeling framework for evaluating the impact of weather conditions on
     farming and harvest operations applies real-time, field-level weather
     data and forecasts of meteorological and climatological conditions
     together with user-provided and/or observed feedback of a present state
     of a harvest-related condition to agronomic models and to generate a
     plurality of harvest advisory outputs for precision agriculture. A
     harvest advisory model simulates and predicts the impacts of this weather
     information and user-provided and/or observed feedback in one or more
     physical, empirical, or artificial intelligence models of precision
     agriculture to analyze crops, plants, soils, and resulting agricultural
     commodities, and provides harvest advisory outputs to a diagnostic
     support tool for users to enhance farming and harvest decision-making,
     whether by providing pre-, post-, or in situ-harvest operations and crop
     analyzes.
",G06F 30/20 (20200101); G06N 20/00 (20190101); Y02A 40/10 (20180101),G06F 17/50 (20060101); G06N 99/00 (20100101),"[['\n5884225', '\nMarch 1999'], ['\n7167797', '\nJanuary 2007'], ['\n2012/0109614', '\nMay 2012'], ['\n2013/0173321', '\nJuly 2013'], ['\n2014/0012732', '\nJanuary 2014']]",[0]," The invention claimed is:  1.  A method comprising: ingesting, as input data, weather information and crop-specific information for a corn crop to be harvested, the weather information including
at least one of current field-level weather data and forecasted weather data, and the crop-specific information at least including planting data and bovine feedstuff data that includes a desired moisture content of between 25% and 35% moisture for a
bovine feedstuff from a corn crop to be stored in one or more post-harvest storage structures, the one or more post-harvest storage structures including an upright tower silo, a bunker silo, a bag silo, and an oxygen-limiting silo;  modeling the input
data in a plurality of data processing modules within a computing environment in which the plurality of data processing modules are executed in conjunction with, and performed on, at least one computer processor, the data processing modules configured to
profile crop growth for a harvest of the corn crop at the desired moisture content, by developing an agricultural model of one or more physical and empirical characteristics impacting harvest operations of the corn crop, to enable a harvest operation to
obtain the bovine feedstuff comprised of the corn crop at the desired moisture content, including: 1) predicting expected weather conditions at a specific location impacting a crop moisture content of the corn crop during crop growth, 2) aggregating the
expected weather conditions with the crop-specific information into the agricultural model to simulate the moisture content of the corn crop at one or more crop growth stages, 3) identifying differences between the simulated moisture content additional
data representing in-field observations of the timing of the crop growth as a growing season progresses and sampled observations of the crop moisture content taken at the specific location at times corresponding to the one or more crop growth stages to
determine a variance between the crop moisture content and the simulated moisture content expected from in-field dry-down of the crop, and 4) modifying the simulation of the moisture content based on the differences between the simulated moisture content
and the additional data to predict a harvest condition to produce bovine feedstuff having the desired moisture content from the corn crop;  generating, as output data, a harvest condition output profile representing the predicted harvest condition;  and
initiating a harvest operation from the harvest condition output profile representing the predicted harvest condition to harvest the corn crop at the desired moisture content for the bovine feedstuff, wherein a user performs the harvest operation in the
particular field based on the predicted harvest condition for the corn crop, or an automated harvest operation is controlled based on the predicted harvest condition for the corn crop.
 2.  The method of claim 1, further comprising applying the harvest output condition profile to a decision support tool configured to provide one or more advisories of the predicted harvest condition, wherein the decision support tool generates
an advisory of one or more harvest opportunity windows to achieve the desired moisture content for the corn crop to a user.
 3.  The method of claim 2, wherein the decision support tool generates a risk advisory of a risk from harvesting corn at a moisture content that exceeds the desired moisture content, the risk advisory including one or more of damage to a storage
structure, loss of feedstuff nutrient levels due to runoff and seepage in the storage structure, and water supply contamination.
 4.  The method of claim 2, wherein the decision support tool generates a risk advisory of a risk from harvesting corn at a lesser moisture content than the desired moisture content, the risk advisory including one or more of a packing factor, a
digestibility, and a sugar content of the bovine feedstuff.
 5.  The method of claim 1, wherein the modeling further comprises applying the weather information to one or more predictive numerical weather models to generate the prediction of expected weather conditions.
 6.  The method of claim 1, further comprising ingesting the additional data representing sampled observations at the specific location at corresponding times, the additional data including physical, empirical or observed agricultural
information, wherein the physical, empirical or observed agricultural information and field and soil information includes at least one of sampled crop moisture content, sampled soil temperature, sampled soil moisture content, and current and/or
forecasted soil conditions.
 7.  The method of claim 1, wherein the agricultural model of one or more physical and empirical characteristics impacting harvest operations of the agricultural commodity includes at least one of a crop-specific growth model, a soil model, and a
land surface model.
 8.  The method of claim 1, further comprising applying the expected weather conditions and the additional data representing sampled observations at the specific location at corresponding times to develop an artificial intelligence model
configured to analyze a specific harvest condition model by building a comprehensive harvest condition dataset for the agricultural model of one or more physical and empirical characteristics impacting harvest operations of the corn crop, and applying
the artificial intelligence model to predict the one or more harvest opportunity windows to produce bovine feedstuff having the desired moisture content from the corn crop.
 9.  The method of claim 1, wherein the desired moisture content is between 24% and 35% moisture for storage of the corn crop in a bunker silo, an upright tower silo, or a bag silo.
 10.  The method of claim 1, wherein the desired moisture content is between 24% and 32% moisture for storage of the corn crop in an oxygen-limiting silo.
 11.  A system comprising: a computing environment including at least one computer-readable storage medium having program instructions stored therein and a computer processor operable to execute the program instructions to profile crop growth for
a harvest of a corn crop at a desired moisture content in a harvest advisory model within a plurality of data processing modules, the plurality of data processing modules including: a weather modeling module configured to diagnose and predict expected
weather conditions at a specific location impacting a moisture content of the corn crop during crop growth by applying weather information including current field-level weather data and forecasted weather data to one or more predictive numerical weather
models;  one or more modules configured to develop an agricultural model of one or more physical and empirical characteristics impacting harvest operations of the corn crop, to enable a harvest operation to obtain a bovine feedstuff comprised of the corn
crop at the desired moisture content, by 1) aggregating the expected weather conditions with crop-specific information comprised at least of planting data and bovine feedstuff data that includes a desired moisture content of between 24% and 35% moisture
for the bovine feedstuff comprised of the corn to be stored in one or more post-harvest storage structures, into the agricultural model, 2) simulating the moisture content of the corn crop at one or more crop growth stages, and 3) comparing additional
data representing sampled one or more observations of a crop moisture content taken at the specific location at times corresponding to the one or more crop growth stage to determine a variance between the crop moisture content and the simulated moisture
content expected from in-field dry-down of the crop to modify the simulation based on identified differences between the otherwise simulated moisture content and the one or more sampled observations of the crop moisture content;  a harvest condition
prediction module configured to predict one or more harvest opportunity windows to produce bovine feedstuff having the desired moisture content from the corn crop in a harvest output condition profile for planning a timing of harvest operations and
storage of the corn crop;  and an output module configured to initiate a harvest operation from the harvest condition output profile representing the predicted harvest condition to harvest the corn crop at the desired moisture content for the bovine
feedstuff, wherein a user performs the harvest operation in the particular field based on the predicted harvest condition for the corn crop, or an automated harvest operation is controlled based on the predicted harvest condition for the corn crop.
 12.  The system of claim 11, wherein the harvest output condition profile is applied to a diagnostic support tool configured to provide one or more advisories to a user.
 13.  The system of claim 12, wherein the decision support tool generates a risk advisory of a risk from harvesting corn at a moisture content that exceeds the desired moisture content, the risk advisory including one or more of damage to a
storage structure, loss of feedstuff nutrient levels due to runoff and seepage in the storage structure, and water supply contamination.
 14.  The system of claim 12, wherein the decision support tool generates a risk advisory of a risk from harvesting corn at a lesser moisture content than the desired moisture content, the risk advisory including one or more of a packing factor,
a digestibility, and a sugar content of the bovine feedstuff.
 15.  The system of claim 11, wherein the one or more modules are further configured to apply the expected weather conditions and the additional data representing sampled observations of actual moisture content at the specific location at
corresponding times to develop an artificial intelligence model configured to analyze a specific harvest condition model by building a comprehensive harvest condition dataset for the agricultural model of one or more physical and empirical
characteristics impacting harvest operations of the corn crop, and applying the artificial intelligence model to predict the one or more harvest opportunity windows to produce bovine feedstuff having the desired moisture content from the corn crop.
 16.  The system of claim 11, wherein the one or more post-harvest storage structures include an upright tower silo, a bunker silo, a bag silo, and an oxygen-limiting silo.
 17.  The system of claim 11, wherein the desired moisture content is between 24% and 35% moisture for storage of the corn crop in a bunker silo, an upright tower silo, or a bag silo.
 18.  The system of claim 11, wherein the desired moisture content is between 24% and 32% moisture for storage of the corn crop in an oxygen-limiting silo.
 19.  A method of evaluating agricultural conditions to support harvest operations and storage of a bovine feedstuff, comprising: within a computing environment comprised of a computer processor and at least one computer-readable storage medium
operably coupled to the computer processor and having program instructions stored therein, the computer processor being operable to execute the program instructions to profile crop growth for a harvest of a corn crop at a desired moisture content in a
harvest advisory model configured to perform the steps of: developing an agricultural model of one or more physical and empirical characteristics impacting harvest operations of the corn crop, to enable a harvest operation to obtain bovine feedstuff
comprised of the corn crop at the desired moisture content, by forecasting weather conditions impacting moisture content during crop growth of the corn crop to be harvested in a specific location, by applying weather information comprised of current
field-level weather data and forecasted weather data to one or more predictive numerical weather models;  simulating the moisture content of the corn crop at one or more crop growth stages by applying the forecasted weather conditions and crop-specific
information comprised at least of planting data and bovine feedstuff data that includes a desired moisture content of between 24% and 35% moisture for a bovine feedstuff comprised of corn to be stored in one or more post-harvest storage structures, to
the agricultural model, the agricultural model comprising at least one of a crop-specific growth model, a soil model, and a land surface model;  and adjusting the simulation of the moisture content of the corn crop at one or more crop growth stages using
at least one of 1) differences between the simulated moisture content and additional data representing in-field observations of the timing of the crop growth as a growing season progresses and sampled observations of a crop moisture content taken at the
specific location at times corresponding to the one or more crop growth stages, determined from a variance between the crop moisture content and the simulated moisture content expected from in-field dry-down of the crop, and 2) one or more artificial
intelligence models developed to analyze the moisture content of the corn crop at one or more crop growth stages from a comprehensive harvest condition dataset for the agricultural model of the one or more physical and empirical characteristics impacting
harvest operations of the corn crop, to generate a prediction of one or more harvest opportunity windows to achieve the desired moisture content of the corn crop for storage in one or more of an upright tower silo, a bunker silo, a bag silo, and an
oxygen-limiting silo;  and initiating a harvest operation from the harvest condition output profile representing the predicted harvest condition to harvest the corn crop at the desired moisture content for the bovine feedstuff, wherein a user performs
the harvest operation in the particular field based on the predicted harvest condition for the corn crop, or an automated harvest operation is controlled based on the predicted harvest condition for the corn crop.
 20.  The method of claim 19, further comprising generating a harvest output condition profile representing the prediction, and applying the harvest output condition profile to a diagnostic support tool configured to provide one or more
advisories based on the prediction to a user performing harvest operations.
 21.  The method of claim 20, wherein the decision support tool generates a risk advisory of a risk from harvesting corn at a moisture content that is higher than the desired moisture content, the risk advisory including one or more of damage to
a storage structure, loss of feedstuff nutrient levels due to runoff and seepage in the storage structure, and water supply contamination.
 22.  The method of claim 20, wherein the decision support tool generates a risk advisory of a risk from harvesting corn at a moisture content that is lower than the desired moisture content, the risk advisory including one or more of a packing
factor, a digestability, and a sugar content of the bovine feedstuff.
 23.  The method of claim 19, wherein the desired moisture content is between 24% and 35% moisture for storage of the corn crop in a bunker silo, an upright tower silo, or a bag silo.
 24.  The method of claim 19, wherein the desired moisture content is between 24% and 32% moisture for storage of the corn crop in an oxygen-limiting silo.  "
"10,255,390","
     April 9, 2019
","Prediction of in-field dry-down of a mature small grain, coarse grain, or
     oilseed crop using field-level analysis and forecasting of weather
     conditions and crop characteristics including sampled moisture content
"," A modeling framework for evaluating the impact of weather conditions on
     farming and harvest operations applies real-time, field-level weather
     data and forecasts of meteorological and climatological conditions
     together with user-provided and/or observed feedback of a present state
     of a harvest-related condition to agronomic models and to generate a
     plurality of harvest advisory outputs for precision agriculture. A
     harvest advisory model simulates and predicts the impacts of this weather
     information and user-provided and/or observed feedback in one or more
     physical, empirical, or artificial intelligence models of precision
     agriculture to analyze crops, plants, soils, and resulting agricultural
     commodities, and provides harvest advisory outputs to a diagnostic
     support tool for users to enhance farming and harvest decision-making,
     whether by providing pre-, post-, or in situ-harvest operations and crop
     analyzes.
",G06N 5/04 (20130101); G06N 20/00 (20190101); A01D 91/00 (20130101); A01F 12/58 (20130101); G05B 13/048 (20130101); A01B 79/005 (20130101); G06N 99/00 (20130101); G06Q 50/02 (20130101); G06F 30/20 (20200101); A01G 25/167 (20130101); G01W 1/10 (20130101); A01F 25/22 (20130101); Y02A 90/10 (20180101); Y02A 40/10 (20180101),G06N 20/00 (20190101); A01G 22/00 (20180101); G01W 1/10 (20060101); G06N 5/04 (20060101); A01F 12/58 (20060101); G06F 17/50 (20060101); A01B 79/00 (20060101); A01D 91/00 (20060101); A01G 25/16 (20060101); G06N 99/00 (20190101); G05B 13/04 (20060101); G06Q 50/02 (20120101); A01F 25/22 (20060101),"[['\n2010/0306012', '\nDecember 2010'], ['\n2012/0109614', '\nMay 2012']]",[0]," The invention claimed is:  1.  A method comprising: ingesting, as input data, weather information and crop-specific information for a grain crop to be harvested, the weather information including
recent and current field-level weather data and extended-range weather forecast data, and the crop-specific information at least including a sampled moisture content of the grain crop collected by sensors associated with agricultural equipment in a
particular field hosting the grain crop to be harvested, the sensors configured to gather data related to in-field dry down directly from the particular field, the sampled moisture content representing an amount of water present in the grain crop reaped
as a percentage of weight;  modeling the input data in a plurality of data processing modules within a computing environment in which the plurality of data processing modules are executed in conjunction with at least one processor, the data processing
modules configured to project an in-field dry-down of the grain crop over time for a planning of harvest operations by 1) predicting expected weather conditions impacting a rate of drying of the grain crop in the particular field, 2) developing an
agricultural model that applies the expected weather conditions and one or more physical and empirical characteristics of a grain crop from the crop-specific information to perform one or more simulations to forecast changes in an in-field moisture
content of the grain crop over time in the particular field, and 3) continually augmenting the forecasted changes in the in-field moisture content over time by performing additional simulations based on the sampled moisture content of the grain crop
collected by the sensors associated with the agricultural equipment in the particular field as a growing season progresses, updated field-level weather observations, and updated weather forecast data for a geographical area including the particular
field, to predict expected times for the grain crop to reach one or more in-field moisture content thresholds;  generating, as output data, a harvest condition profile representing one or more predictions of the in-field dry-down of the grain crop in the
particular field based on the forecasted changes in the in-field moisture content over time;  and developing a harvest schedule for performing one or more agricultural activities from the harvest condition profile in the particular field, wherein a user
performs the one or more agricultural activities based on the harvest schedule, or an automated performance of the one or more agricultural activities by an agricultural vehicle is controlled based on the harvest schedule.
 2.  The method of claim 1, further comprising generating one or more advisories for at least one of a harvesting machine in the particular field, a user onboard the harvesting machine, and a user involved in harvest operations in the particular
field, based on the harvest condition profile.
 3.  The method of claim 2, further comprising applying the harvest condition profile to a harvest advisory tool configured to provide the one or more advisories to the at least one of a harvesting machine in the particular field, a user onboard
the harvesting machine, and a user involved in harvest operations in the particular field.
 4.  The method of claim 2, wherein the one or more advisories include an advisory that the grain crop will experience further in-field drying over a specified time period, an advisory that the grain crop will not experience further in-field
drying over a specified time period, and an advisory that the grain crop will reach a desired moisture content in an anticipated harvest window.
 5.  The method of claim 1, wherein the crop-specific information further includes one or more of anticipated temporal harvest windows, crop growth stage data, crop relative maturity data, crop planting depth and row spacing data, crop planting
date data, crop post-maturity dry-down characteristics, and targeted harvest crop moisture or temperature thresholds for the grain crop.
 6.  The method of claim 1, wherein the modeling further comprises applying the weather information to one or more predictive numerical weather models to generate the prediction of expected weather conditions.
 7.  The method of claim 1, wherein the applying the expected weather conditions and the crop-specific information to an agricultural model that applies one or more physical and empirical characteristics of the grain crop further comprises
simulating the rate of drying of the grain crop in a particular field, identifying differences in one or more simulations of the rate of drying with the sampled moisture content at corresponding times, and modifying the one or more simulations based on
the differences between the otherwise simulated rate of drying and the sampled moisture content at the corresponding times.
 8.  The method of claim 1, further comprising applying the expected weather conditions, the sampled moisture content, and crop metadata representing actual and/or realized performance of the grain crop over a specified time period to
automatically develop an artificial intelligence model configured to analyze a specific in-field dry-down of grain crop by building a comprehensive harvest condition dataset for the agricultural model that applies one or more physical and empirical
characteristics of a grain crop to forecast changes in an in-field moisture content of a grain crop with similar characteristics in any field at any selected time.
 9.  The method of claim 1, wherein the grain crop is at least one of a mature small grain, coarse grain, or oilseed crop.
 10.  A method of evaluating moisture content of a grain crop to support harvest operations, comprising: within a computing environment comprised of a computer processor and at least one computer-readable storage medium operably coupled to the
computer processor and having program instructions stored therein, the computer processor being operable to execute the program instructions to project an in-field dry-down of the grain crop over time for a planning of harvest operations in a harvest
advisory model configured to perform the steps of: predicting expected weather conditions impacting a rate of drying of the grain crop in a particular field hosting the grain crop at one or more temporal harvest windows by applying weather information
comprised of recent and current field-level weather data and extended-range weather forecast data to one or more predictive numerical weather models;  developing an agricultural model that applies the expected weather conditions and one or more physical
and empirical characteristics of a grain crop from crop-specific information that at least includes a sampled moisture content of the grain crop collected by sensors associated with agricultural equipment in the particular field, the sensors configured
to gather data related to an in-field dry-down directly from the particular field, to model the rate of drying of the grain crop over time in the particular field by performing one or more simulations, the sampled moisture content representing an amount
of water present in the grain crop reaped as a percentage of weight;  forecasting changes in an in-field moisture content of the grain crop over time in the particular field as the one or more temporal harvest windows approach from the rate of drying; 
and continually augmenting the forecasted changes in the in-field moisture content over time by performing additional simulations based on the sampled moisture content of the grain crop collected by the sensors associated with the agricultural equipment
in the particular field as a growing season progresses, updated field-level weather observations, and updated weather forecast data for a geographical area including the particular field, to predict expected times for the grain crop to reach one or more
in-field moisture content thresholds;  and developing a harvest schedule for performing one or more agricultural activities from the harvest condition profile in the particular field, wherein a user performs the one or more agricultural activities based
on the harvest schedule, or an automated performance of the one or more agricultural activities by an agricultural vehicle is controlled based on the harvest schedule.
 11.  The method of claim 10, further comprising generating a harvest condition profile representing one or more predictions of the in-field dry-down of the grain crop in the particular field based on the forecasted changes in the in-field
moisture content over time.
 12.  The method of claim 11, further comprising generating one or more advisories for at least one of a harvesting machine in the particular field, a user onboard the harvesting machine, and a user involved in harvest operations in the
particular field, based on the harvest condition profile.
 13.  The method of claim 12, wherein the one or more advisories include an advisory that the grain crop will experience further in-field drying over a specified time period, an advisory that the grain crop will not experience further in-field
drying over a specified time period, and an advisory that the grain crop will reach a desired moisture content in an anticipated harvest window.
 14.  The method of claim 12, further comprising applying the harvest condition profile to a diagnostic support tool configured to provide the one or more advisories to the at least one of a harvesting machine in the particular field, a user
onboard the harvesting machine, and a user involved in harvest operations in the particular field, based on the harvest condition profile.
 15.  The method of claim 10, further comprising simulating the rate of drying in the agricultural model that applies one or more physical and empirical characteristics of a grain crop, and modifying one or more simulations of the rate of drying
using differences between the otherwise simulated rate of drying and the sampled moisture content of the grain crop collected by agricultural equipment in the particular field at corresponding times.
 16.  The method of claim 10, wherein the crop-specific information further includes one or more of crop relative maturity data, crop planting depth and row spacing data, crop planting date data, crop post-maturity dry-down characteristics, and
targeted harvest crop moisture or temperature thresholds.
 17.  The method of claim 10, further comprising automatically developing or more artificial intelligence artificial models configured to analyze a specific in-field dry-down of a grain crop by building a comprehensive harvest condition dataset
for the agricultural model of one or more physical and empirical characteristics that applies one or more physical and empirical characteristics of a grain crop to forecast changes in an in-field moisture content of a grain crop with similar
characteristics in any field at any selected time.
 18.  The method of claim 10, wherein the grain crop is at least one of a mature small grain, coarse grain, or oilseed crop.
 19.  A system comprising: a computing environment including at least one computer-readable storage medium having program instructions stored therein and a computer processor operable to execute the program instructions to project an in-field
dry-down of the grain crop over time for a planning of harvest operations by performing a harvest advisory model within a plurality of data processing modules, the plurality of data processing modules including: a weather modeling module configured to
predict expected weather conditions impacting a rate of drying of the grain crop in a particular field hosting the grain crop to be harvested by applying weather information including recent and current field-level weather data and extended-range weather
forecast data to one or more predictive numerical weather models;  one or more modules configured to develop an agricultural model that aggregates the expected weather conditions and one or more physical and empirical characteristics of a grain crop with
crop-specific information at least including a sampled moisture content of the grain crop collected by sensors associated with agricultural equipment in the particular field, the sensors configured to gather data related to an in-field dry-down directly
from the particular field, and the sampled moisture content representing an amount of water present in the grain crop reaped as a percentage of weight, the agricultural model configured to perform one or more simulations to forecast changes in an
in-field moisture content of the grain crop over time in the particular field, and continually augment the forecasted changes in the in-field moisture content over time by performing additional simulations based on the sampled moisture content of the
grain crop collected by the sensors associated with the agricultural equipment in the particular field as a growing season progresses, updated field-level weather observations, and updated weather forecast data for a geographical area including the
particular field, to predict expected times for the grain crop to reach one or more in-field moisture content thresholds;  a harvest condition module configured to generate a harvest condition profile representing one or more predictions of the in-field
dry-down of the grain crop in the particular field based on the forecasted changes in the in-field moisture content of the grain crop over time, and one or more advisories to at least one of a harvesting machine in the particular field, a user onboard
the harvesting machine, and a user involved in harvest operations in the particular field based on the harvest condition profile;  and an output module configured to develop a harvest schedule for performing one or more agricultural activities from the
harvest condition profile in the particular field, wherein a user performs the one or more agricultural activities based on the harvest schedule, or an automated performance of the one or more agricultural activities by an agricultural vehicle is
controlled based on the harvest schedule.
 20.  The system of claim 19, wherein the harvest condition profile is applied to a diagnostic support tool configured to provide the one or more advisories to one or more of a user performing harvest operations, and agricultural equipment
performing harvest operations.
 21.  The system of claim 19, wherein the one or more advisories include an advisory that the grain crop will experience further in-field drying over a specified time period, an advisory that the grain crop will not experience further in-field
drying over a specified time period, and an advisory that the grain crop will reach a desired moisture content in an anticipated harvest window.
 22.  The system of claim 19, wherein the crop-specific information further includes one or more of anticipated temporal harvest windows, crop growth stage data, crop relative maturity data, crop planting depth and row spacing data, crop planting
date data, crop post-maturity dry-down characteristics, and targeted harvest crop moisture or temperature thresholds for the grain crop.
 23.  The system of claim 19, wherein the one or more modules are further configured to simulate the rate of drying in the agricultural model that applies one or more physical and empirical characteristics of a grain crop, and modify one or more
simulations of the rate of drying using differences between the otherwise simulated rate of drying and the sampled moisture content of the grain crop collected by agricultural equipment in the particular field at corresponding times.
 24.  The system of claim 19, wherein the one or more modules are further configured to automatically develop an artificial intelligence model configured to analyze a specific in-field dry-down of a grain crop by building a comprehensive dataset
for the agricultural model that applies one or more physical and empirical characteristics of a grain crop, and predict in-field dry-down of a crop with similar characteristics in any field at any selected time.
 25.  The system of claim 19, wherein the grain crop is at least one of a mature small grain, coarse grain, or oilseed crop.  "
"10,255,391","
     April 9, 2019
","Modeling of time-variant threshability due to interactions between a crop
     in a field and atmospheric and soil conditions for prediction of daily
     opportunity windows for harvest operations using field-level diagnosis
     and prediction of weather conditions and observations and recent input of
     harvest condition states
"," A modeling framework for evaluating the impact of weather conditions on
     farming and harvest operations applies real-time, field-level weather
     data and forecasts of meteorological and climatological conditions
     together with user-provided and/or observed feedback of a present state
     of a harvest-related condition to agronomic models and to generate a
     plurality of harvest advisory outputs for precision agriculture. A
     harvest advisory model simulates and predicts the impacts of this weather
     information and user-provided and/or observed feedback in one or more
     physical, empirical, or artificial intelligence models of precision
     agriculture to analyze crops, plants, soils, and resulting agricultural
     commodities, and provides harvest advisory outputs to a diagnostic
     support tool for users to enhance farming and harvest decision-making,
     whether by providing pre-, post-, or in situ-harvest operations and crop
     analyzes.
",A01D 91/00 (20130101); G01W 1/10 (20130101); G06Q 50/02 (20130101); G05B 13/048 (20130101); G06N 20/00 (20190101); G06N 99/00 (20130101); A01G 25/167 (20130101); G06N 5/04 (20130101); A01F 12/58 (20130101); G06F 30/20 (20200101); A01B 79/005 (20130101); Y02A 90/10 (20180101); Y02A 40/10 (20180101); A01F 25/22 (20130101),G06N 20/00 (20190101); A01G 22/00 (20180101); G06N 5/04 (20060101); A01F 12/58 (20060101); G01W 1/10 (20060101); A01D 91/00 (20060101); G06F 17/50 (20060101); A01B 79/00 (20060101); A01G 25/16 (20060101); G06N 99/00 (20190101); G05B 13/04 (20060101); G06Q 50/02 (20120101); A01F 25/22 (20060101),"[['\n2012/0109614', '\nMay 2012']]",[0]," The invention claimed is:  1.  A method, comprising: ingesting, as input data, weather information, crop-specific information and harvest data for a crop to be harvested, the weather information
including recent and current field-level weather data and extended-range weather forecast data, the crop-specific information including at least one of crop type data and crop state data representing at least a standing or windrowed status of the crop,
and the harvest data including data related to a threshability of the crop measured by one or more sensors installed upon agricultural equipment associated with harvesting operations of the crop, the one or more sensors configured to gather the data
related to the threshability of the crop directly from the particular field;  modeling the input data in a plurality of data processing modules within a computing environment in which the plurality of data processing modules are executed in conjunction
with at least one processor, the data processing modules configured to project time-varying threshability of the crop to be harvested in one or more forthcoming temporal harvest windows, by 1) predicting expected weather conditions relative to crop
wetness over at least one time period relative to the one or more forthcoming temporal harvest windows, 2) developing an agricultural model that applies the expected weather conditions and one or more physical and empirical characteristics of the crop to
be harvested from the crop-specific information and the harvest data to perform one or more simulations to forecast changes in one or more threshability characteristics of the crop over the one or more forthcoming temporal harvest windows, and 3)
continually augmenting the forecasted changes in the one or more threshability characteristics of the crop by performing additional simulations based on the threshability of the crop measured by the one or more sensors as a growing season progresses,
updated field-level weather observations, and updated weather forecast data for a geographical area including the particular field, to predict expected times for the crop to reach one or more threshing thresholds;  generating, as output data, a harvest
condition profile representing the forecast of the changes in one or more threshability characteristics of the crop on both a current day and future days within the at least one time period relative to the one or more forthcoming temporal harvest
windows, and generating one or more advisories based on the harvest condition profile to at least one of a user performing harvest operations, a user managing harvest operations, and agricultural equipment performing harvest operations;  and developing a
harvest schedule for performing one or more agricultural activities from the harvest condition profile in the particular field, wherein a user performs the one or more agricultural activities based on the harvest schedule, or an automated performance of
the one or more agricultural activities by an agricultural vehicle is controlled based on the harvest schedule.
 2.  The method of claim 1, further comprising applying the harvest condition profile to a harvest advisory tool configured to provide the one or more advisories to the at least one of a user performing harvest operations, and agricultural
equipment performing harvest operations.
 3.  The method of claim 2, wherein the one or more advisories include an advisory that the crop will experience an increase in in-field wetness over a specified time period, an advisory that the crop will experience a decrease in in-field
wetness over a specified time period, and an advisory that the crop will reach a desired in-field wetness in the one or more forthcoming temporal harvest windows.
 4.  The method of claim 1, wherein the crop-specific information further includes one or more of crop growth stage data, crop relative maturity data, crop planting depth and row spacing data, crop planting date, crop post-maturity dry-down
characteristics, and targeted harvest crop moisture or temperature thresholds for the crop.
 5.  The method of claim 1, wherein the modeling further comprises applying the weather information to one or more predictive numerical weather models to generate the prediction of expected weather conditions.
 6.  The method of claim 1, wherein the applying the expected weather conditions, the crop-specific information, and the harvest data to an agricultural model that applies one or more physical and empirical characteristics of the crop to be
harvested further comprises simulating the one or more threshability characteristics of the crop, identifying differences in simulations of the one or more threshability characteristics of the crop with the threshability data reported by agricultural
equipment associated with harvesting operations of the crop at corresponding times, and modifying the simulations based on the differences between the otherwise simulated one or more threshability characteristics of the crop and the data representing the
threshability of the crop measured by agricultural equipment associated with harvesting operations of the crop at the corresponding times.
 7.  The method of claim 1, further comprising applying the expected weather conditions, the crop-specific information, and the harvest data to automatically develop an artificial intelligence model configured to analyze crop threshability by
building a comprehensive harvest condition dataset for the agricultural model that applies one or more physical and empirical characteristics of a crop to forecast changes in one or more threshability characteristics of the crop with similar
characteristics in any field at any selected time.
 8.  The method of claim 1, wherein the input data further includes field-specific information for a location of a particular field where the crop to be harvested is planted.
 9.  The method of claim 1, wherein the input data further includes imagery data that includes information indicative of at least one of crop quality and seed quality.
 10.  A method of evaluating threshability of a crop to support harvest operations, comprising: within a computing environment comprised of a computer processor and at least one computer-readable storage medium operably coupled to the computer
processor and having program instructions stored therein, the computer processor being operable to execute the program instructions to project time-varying threshability of a crop to be harvested in one or more forthcoming temporal harvest windows for a
planning of harvest operations in a harvest advisory model configured to perform the steps of: predicting expected weather conditions relative to crop wetness over at least one time period relative to the one or more forthcoming temporal harvest windows
by applying weather information comprised of recent and current field-level weather data and extended-range weather forecast data to one or more predictive numerical weather models;  developing an agricultural model that applies the expected weather
conditions and one or more physical and empirical characteristics of the crop to be harvested from crop-specific information that includes at least one of crop type data and crop state data representing at least a standing or windrowed status of the
crop, and harvest data that includes data related to a threshability of the crop measured by one or more sensors installed upon agricultural equipment associated with harvesting operations of the crop to model a moisture condition of the crop over the
one or more forthcoming temporal harvest windows by performing or more simulations, the one or more sensors configured to gather the data related to the threshability of the crop directly from the particular field;  forecasting changes in one or more
threshability characteristics of the crop as the one or more forthcoming temporal harvest windows from the modeled moisture condition of the crop;  and continually augmenting the forecasted changes in the one or more threshability characteristics of the
crop by performing additional simulations based on the threshability of the crop measured by the one or more sensors as a growing season progresses, updated field-level weather observations, and updated weather forecast data for a geographical area
including the particular field, to predict expected times for the crop to reach one or more threshing thresholds;  and developing a harvest schedule for performing one or more agricultural activities from the harvest condition profile in the particular
field, wherein a user performs the one or more agricultural activities based on the harvest schedule, or an automated performance of the one or more agricultural activities by an agricultural vehicle is controlled based on the harvest schedule.
 11.  The method of claim 10, further comprising generating a harvest condition profile representing one or more predictions of crop threshability over the one or more forthcoming temporal harvest windows based on the forecasted changes in the
one or more threshability characteristics.
 12.  The method of claim 11, further comprising generating one or more advisories based on the harvest condition profile, and providing the one or more advisories to at least one of a user performing harvest operations, a user managing harvest
operations, and agricultural equipment performing harvest operations.
 13.  The method of claim 12, wherein the one or more advisories include an advisory that the crop will experience an increase in in-field wetness over a specified time period, an advisory that the crop will experience a decrease in in-field
wetness over a specified time period, and an advisory that the crop will reach a desired in-field wetness in the one or more forthcoming temporal harvest windows.
 14.  The method of claim 11, further comprising applying the harvest condition profile to a diagnostic support tool configured to provide the one or more advisories to the at least one of a user performing harvest operations, a user managing
harvest operations, and agricultural equipment performing harvest operations.
 15.  The method of claim 11, further comprising simulating the one or more threshability characteristics of the crop in the agricultural model that applies one or more physical and empirical characteristics of a crop to be harvested, and
modifying one or more simulations of the one or more threshability characteristics of the crop using differences identified between the otherwise one or more threshability characteristics and the data representing the threshability of the crop measured
by agricultural equipment associated with harvesting operations of the crop at corresponding times.
 16.  The method of claim 11, wherein the crop-specific information further includes one or more of crop relative maturity data, crop planting depth and row spacing data, crop planting date data, crop post-maturity dry-down characteristics, and
targeted harvest crop moisture or temperature thresholds.
 17.  The method of claim 11, further comprising automatically developing or more artificial intelligence artificial models configured to analyze crop threshability by building a comprehensive harvest condition dataset for the agricultural model
of one or more physical and empirical characteristics that applies one or more physical and empirical characteristics of the crop to be harvested to forecast changes in one or more threshability characteristics of the crop with similar characteristics in
any field at any selected time.
 18.  The method of claim 10, further comprising applying field-specific information for a location of a particular field where the crop to be harvested is planted.
 19.  The method of claim 10, further comprising applying imagery data to the agricultural model that applies one or more physical and empirical characteristics of the crop to be harvested, the imagery data including information indicative of at
least one of crop quality and seed quality.
 20.  A system comprising: a computing environment including at least one computer-readable storage medium having program instructions stored therein and a computer processor operable to execute the program instructions to project time-varying
threshability of a crop to be harvested in one or more forthcoming temporal harvest windows for a planning of harvest operations by performing a harvest advisory model within a plurality of data processing modules, the plurality of data processing
modules including: a weather modeling module configured to predict expected weather conditions relative to crop wetness over at least one time period relative to the one or more forthcoming temporal harvest windows by applying weather information
including recent and current field-level weather data and extended-range weather forecast data to one or more predictive numerical weather models;  one or more modules configured to develop an agricultural model that aggregates the expected weather
conditions and one or more physical and empirical characteristics of the crop to be harvested with crop-specific information including at least one of crop type data and crop state data representing at least a standing or windrowed status of the crop,
and with harvest data that includes data related to a threshability of the crop measured one or more sensors installed upon by agricultural equipment associated with harvesting operations of the crop, the one or more sensors configured to gather the data
related to the threshability of the crop directly from the particular field, the agricultural model configured to perform one or more simulations to forecast changes in one or more threshability characteristics of the crop over the one or more
forthcoming temporal harvest windows, and continually augment the forecasted changes in the one or more threshability characteristics of the crop by performing additional simulations based on the threshability of the crop measured by the one or more
sensors as a growing season progresses, updated field-level weather observations, and updated weather forecast data for a geographical area including the particular field, to predict expected times for the crop to reach one or more threshing thresholds; 
a harvest condition module configured to generate a harvest condition profile representing the forecast of the changes in one or more threshability characteristics of the crop on both a current day and future days within the at least one time period
relative to the one or more forthcoming temporal harvest windows, and generating one or more advisories based on the harvest condition profile;  and an output module configured to develop a harvest schedule for performing one or more agricultural
activities from the harvest condition profile in the particular field, wherein a user performs the one or more agricultural activities based on the harvest schedule, or an automated performance of the one or more agricultural activities by an
agricultural vehicle is controlled based on the harvest schedule.
 21.  The system of claim 20, wherein the harvest condition profile is applied to a diagnostic support tool configured to provide the one or more advisories to one or more of a user performing harvest operations, a user managing harvest
operations, and agricultural equipment performing harvest operations.
 22.  The system of claim 20, wherein the one or more advisories include an advisory that the crop will experience an increase in in-field wetness over a specified time period, an advisory that the crop will experience a decrease in in-field
wetness over a specified time period, and an advisory that the crop will reach a desired in-field wetness in the one or more forthcoming temporal harvest windows.
 23.  The system of claim 20, wherein the crop-specific information further includes crop growth stage data, crop relative maturity data, crop planting depth and row spacing data, crop planting date data, crop post-maturity dry-down
characteristics, and targeted harvest crop moisture or temperature thresholds for the crop.
 24.  The system of claim 20, wherein the one or more modules are further configured to simulate the one or more threshability characteristics of the crop, identify differences in simulations of the one or more threshability characteristics of
the crop with the threshability data representing an in-field crop wetness condition reported by agricultural equipment associated with harvesting operations of the crop at corresponding times, and modify the simulations based on the differences between
the otherwise simulated one or more threshability characteristics of the crop and the data representing the threshability of the crop measured by agricultural equipment associated with harvesting operations of the crop at the corresponding times.
 25.  The system of claim 20, wherein the one or more modules are further configured to automatically develop an artificial intelligence model configured to analyze crop threshability by building a comprehensive harvest condition dataset for the
agricultural model that applies one or more physical and empirical characteristics of a crop to forecast changes in one or more threshability characteristics of the crop with similar characteristics in any field at any selected time.
 26.  The system of claim 20, wherein the one or more modules are further configured to aggregate field-specific information for a location of a particular field where the crop to be harvested is planted.
 27.  The system of claim 20, wherein the one or more modules are further configured to aggregate imagery data in the agricultural model that applies one or more physical and empirical characteristics of a crop to be harvested, the imagery data
including information indicative of at least one of crop quality and seed quality.  "
"10,261,496","
     April 16, 2019
","Production system
"," A sensor monitors a treatment status of a predetermined manufacturing
     device, and an abnormality detection device detects an abnormality of a
     sensor signal that is a monitoring result of the sensor. The sensor
     signal is a digital data group obtained by sampling an analog waveform at
     a predetermined sampling period. A management apparatus learns
     characteristics of a plurality of digital data groups accumulated in past
     times through use of artificial intelligence to generate a learned model.
     An abnormality detection device holds the learned model and determines
     whether an abnormality is present in the digital data group of a current
     processing target by using the learned model.
",G05B 19/406 (20130101); G05B 19/4184 (20130101); G05B 2219/33322 (20130101); G06N 20/00 (20190101); Y02P 90/02 (20151101); G05B 2219/45031 (20130101),G06N 99/00 (20100101); G05B 19/406 (20060101); G05B 19/418 (20060101),"[['\n2017/0220008', '\nAugust 2017'], ['\n2018/0321672', '\nNovember 2018']]",[0]," What is claimed is:  1.  A production system for manufacturing a product by using a plurality of manufacturing processes, the production system comprising: a plurality of edge devices performing
treatment associated with the plurality of manufacturing processes;  a master device managing the entire production system;  a sensor provided corresponding to a predetermined edge device among the plurality of edge devices and monitoring a treatment
status of the predetermined edge device;  an abnormality detection device provided corresponding to the predetermined edge device and detecting an abnormality of a sensor signal that is a monitoring result of the sensor;  and a communication network
connecting the plurality of edge devices, the master device, and the abnormality detection device, wherein the sensor signal is a digital data group obtained by sampling an analog waveform at a predetermined sampling period, wherein the master device
learns characteristics of a plurality of the digital data groups accumulated in past times through use of artificial intelligence to generate a learned model for determining whether an abnormality is present in the digital data group, and wherein the
abnormality detection device holds the generated learned model and determines whether an abnormality is present in the digital data group which is a current processing target by using the held learned model.
 2.  The production system according to claim 1, wherein the master device classifies the plurality of digital data groups into a plurality of categories and learns to generate a plurality of the learned models corresponding to the plurality of
categories, respectively, and wherein the abnormality detection device recognizes a category of the digital data group which is the current processing target and determines whether an abnormality is present by using the learned model corresponding to the
recognized category.
 3.  The production system according to claim 2, wherein the master device determines a classification method of the plurality of categories based on an instruction of an engineer, and registers the determined classification method in the
abnormality detection device, and wherein the abnormality detection device recognizes the category of the digital data group which is the current processing target based on the registered classification method.
 4.  The production system according to claim 1, wherein the abnormality detection device includes a memory circuit holding the learned model, and a processor circuit determining whether an abnormality is present by using the held learned model.
 5.  The production system according to claim 1, wherein the plurality of manufacturing processes are preceding processes of a semiconductor product.
 6.  A production system for manufacturing a plurality of types of semiconductor products by using a plurality of manufacturing processes, the production system comprising: a plurality of manufacturing devices performing processing treatment
associated with the plurality of manufacturing processes by using a predetermined process recipe depending on a type of the semiconductor product;  a management apparatus managing the entire production system;  a first sensor and a second sensor each
provided corresponding to a predetermined manufacturing device among the plurality of manufacturing devices and each monitoring a status of the processing treatment of the predetermined manufacturing device;  an abnormality detection device provided
corresponding to the predetermined manufacturing device and detecting an abnormality of a first sensor signal that is a monitoring result of the first sensor and an abnormality of a second sensor signal that is a monitoring result of the second sensor; 
and a communication network connecting the plurality of manufacturing devices, the management apparatus, and the abnormality detection device, wherein the first sensor signal is a first digital data group obtained by sampling an analog waveform at a
first sampling period, wherein the second sensor signal is a second digital data group obtained by sampling an analog waveform at a second sampling period, wherein the management apparatus learns characteristics of a plurality of the first digital data
groups accumulated in past times through use of artificial intelligence to generate a first learned model for determining whether an abnormality is present in the first digital data group, and learns characteristics of a plurality of the second digital
data groups accumulated in past times through use of artificial intelligence to generate a second learned model for determining whether an abnormality is present in the second digital data group, and wherein the abnormality detection device holds the
generated first learned model and the generated second learned model and determines whether an abnormality is present in the first digital data group which is a current processing target by using the held first learned model and determines whether an
abnormality is present in the second digital data group which is a current processing target by using the held second learned model.
 7.  The production system according to claim 6, wherein the management apparatus classifies the plurality of first digital data groups into a plurality of categories based on a first classification method and learns to generate a plurality of
the first learned models corresponding to the plurality of categories, respectively, and classifies the plurality of second digital data groups into a plurality of categories based on a second classification method and learns to generate a plurality of
the second learned models corresponding to the plurality of categories, respectively, and wherein the abnormality detection device recognizes a category of the first digital data group which is the current processing target and determines whether an
abnormality is present by using the first learned model corresponding to the recognized category, and recognizes a category of the second digital data group which is the current processing target and determines whether an abnormality is present by using
the second learned model corresponding to the recognized category.
 8.  The production system according to claim 7, wherein the management apparatus determines the first classification method and the second classification method based on an instruction of an engineer, and registers the determined first
classification method and the determined second classification method in the abnormality detection device, and wherein the abnormality detection device recognizes the category of the first digital data group which is the current processing target based
on the registered first classification method and recognizes the category of the second digital data group which is the current processing target based on the registered second classification method.
 9.  The production system according to claim 7, wherein each of the first classification method and the second classification method is determined by using a single identifier or a plurality of identifiers among a plurality of production
identifiers narrowing down types of the semiconductor products in a stepwise manner, a process identifier identifying the plurality of manufacturing processes, a plurality of device identifiers narrowing down types of the plurality of manufacturing
devices in a stepwise manner, and a recipe identifier identifying the predetermined process recipe.
 10.  The production system according to claim 9, wherein the single identifier or the plurality of identifiers to be used in the first classification method differs from the single identifier or the plurality of identifiers to be used in the
second classification method.
 11.  The production system according to claim 6, wherein the abnormality detection device includes a memory circuit holding the first learned model and the second learned model, and a processor circuit determining whether an abnormality is
present by using the held first learned model and the held second learned model.
 12.  The production system according to claim 6, wherein at least one of the first sampling period and the second sampling period is equal to or less than 100 ms.
 13.  A production system for inspecting a product through use of a plurality of inspection processes, the production system comprising: a plurality of edge devices performing measurement associated with the plurality of inspection processes and
generating measurement data;  a master device managing the entire production system;  an abnormality detection device provided corresponding to a predetermined edge device among the plurality of edge devices and detecting an abnormality of the
measurement data from the predetermined edge device;  and a communication network connecting the plurality of edge devices, the master device, and the abnormality detection device, wherein the measurement data is a digital data group forming an analog
waveform, wherein the master device learns characteristics of a plurality of the digital data groups accumulated in past times through use of artificial intelligence to generate a learned model for determining whether an abnormality is present in the
digital data group, and wherein the abnormality detection device holds the generated learned model and determines whether an abnormality is present in the digital data group which is a current processing target by using the held learned model.
 14.  The production system according to claim 13, wherein the master device classifies the plurality of digital data groups into a plurality of categories and learns to generate a plurality of the learned models corresponding to the plurality of
categories, respectively, and wherein the abnormality detection device recognizes a category of the digital data group which is the current processing target and determines whether an abnormality is present by using the learned model corresponding to the
recognized category.
 15.  The production system according to claim 14, wherein the master device determines a classification method of the plurality of categories based on an instruction of an engineer and registers the determined classification method in the
abnormality detection device, and wherein the abnormality detection device recognizes the category of the digital data group which is the current processing target based on the registered classification method.
 16.  The production system according to claim 13, wherein the abnormality detection device includes a memory circuit holding the learned model, and a processor circuit determining whether an abnormality is present by using the held learned
model.  "
"10,261,936","
     April 16, 2019
","PCIe switch with data and control path systolic array
"," The present subject disclosure provides a PCIe switch architecture with
     data and control path systolic array that can be used for real time data
     analysis or Artificial Intelligence (AI) learning. A systolic array is
     described which analyzes the TLPs received by an uplink port and
     processes the TLPs according to pre-programmed rules. Then the TLP is
     forwarded to a destination port. The reverse operation is described as
     well.
",G06F 13/4022 (20130101); G06F 13/4282 (20130101); G06F 13/4265 (20130101); G06F 13/36 (20130101); G06F 2213/0026 (20130101),G06F 13/42 (20060101); G06F 13/40 (20060101); G06F 13/36 (20060101),"[['\n4493048', '\nJanuary 1985'], ['\n4698151', '\nOctober 1987'], ['\n4807183', '\nFebruary 1989'], ['\n5689508', '\nNovember 1997'], ['\n7382787', '\nJune 2008'], ['\n7450438', '\nNovember 2008'], ['\n7694047', '\nApril 2010'], ['\n8995302', '\nMarch 2015'], ['\n9146890', '\nSeptember 2015'], ['\n2006/0117126', '\nJune 2006'], ['\n2007/0195951', '\nAugust 2007'], ['\n2014/0214911', '\nJuly 2014'], ['\n2018/0082084', '\nMarch 2018']]",[0]," What is claimed is:  1.  A method for Peripheral Component Interconnect Express (PCIe) switching, comprising: receiving a transaction layer packet (TLP) at an uplink port of a PCIe switch;  at
the PCIe switch: determining a nature of the TLP by evaluating a parameter of the TLP at a programmable filter coupled to the uplink port;  determining if the TLP should be routed to a systolic array at the PCIe switch based on the nature of the TLP
determined by the programmable filter;  routing the TLP to a systolic array at the PCIe switch based on a determination that the TLP should be routed to the systolic array at the PCIe switch;  analyzing, at the PCIe switch, the TLP in the systolic array; and forwarding the TLP to a destination port of the PCIe switch based on the analysis of the TLP performed by the systolic array of the PCIe switch.
 2.  The method of claim 1, wherein only a header portion of the TLP is routed to the systolic array.
 3.  The method of claim 1, wherein the TLP routed to the systolic array is the TLP received at the uplink port.
 4.  The method of claim 1, wherein the TLP routed to the systolic array is a replica of the TLP directly forwarded to the destination port.
 5.  The method of claim 1, wherein the systolic array has an interface to communicate with a plurality of destination ports.
 6.  The method of claim 5, wherein each of the destination ports has an associated filter.
 7.  A Peripheral Component Interconnect Express (PCIe) switch, comprising: an uplink port for receiving a transaction layer packet (TLP) at the PCIe switch;  a programmable filter coupled to the uplink port, the programmable filter for
determining a nature of the TLP received at the uplink port by evaluating a parameter of the received TLP, determining if the TLP should be routed to a systolic array at the PCIe switch based on the nature of the TLP determined by the programmable filter
and routing the TLP to the systolic array at the PCIe switch based on a determination that the TLP should be routed to the systolic array at the PCIe switch;  the systolic array for analyzing the TLP at the PCIe switch and forwarding the TLP to a
destination port of the PCIe switch based on the analysis of the TLP performed by the systolic array of the PCIe switch;  and a destination port for receiving the TLP.
 8.  The switch of claim 7, wherein the systolic array only receives a header portion of the TLP.
 9.  The switch of claim 7, wherein the systolic array receives the TLP as it was received by the uplink port.
 10.  The switch of claim 7, wherein the programmable filter creates a replica of the TLP that it forwards to the destination port, and forwards the replica to the systolic array.
 11.  The switch of claim 7, wherein the destination port comprises a plurality of downlink ports.
 12.  The switch of claim 7, wherein the systolic array comprises an interface to communicate with a plurality of downlink ports.
 13.  The switch of claim 12, wherein each of the downlink ports has an associated filter.
 14.  The switch of claim 7, wherein the systolic array comprises a plurality of data processing units (DPUs).
 15.  A Peripheral Component Interconnect Express (PCIe) switch, comprising: an uplink port for receiving a transaction layer packet (TLP) at the PCIe switch;  a first programmable filter coupled to the uplink port, the programmable filter for
determining a nature of the TLP received at the uplink port by evaluating a parameter of the received TLP, determining if the TLP should be routed to a systolic array at the PCIe switch based on the nature of the TLP determined by the programmable filter
and routing the TLP to the systolic array at the PCIe switch based on a determination that the TLP should be routed to the systolic array at the PCIe switch;  the systolic array for analyzing the TLP at the PCIe switch, determining a first destination
port of a plurality of destination ports based on the analysis of the TLP performed by the systolic array of the PCIe switch and forwarding the TLP to the determined first destination port;  the plurality of destination ports for receiving the TLP;  and
a plurality of second programmable filters associated with each of the plurality of second programmable filter associated with a respective one of the destination ports.
 16.  The switch of claim 15, wherein the systolic array only receives a header portion of the TLP.
 17.  The switch of claim 15, wherein the systolic array receives the TLP as it was received by the uplink port.
 18.  The switch of claim 15, wherein the first programmable filter creates a replica of the TLP that it forwards to any of the destination ports, and forwards the replica to the systolic array.
 19.  The switch of claim 15, wherein the systolic array comprises a plurality of data processing units (DPUs).  "
"10,262,121","
     April 16, 2019
","Turing test via failure
"," Current CAPTCHA tests are designed to be difficult for a bot and simple
     for a human-user to answer; however, as artificial intelligence improves,
     bots are more capable of using techniques such as optical character
     recognition to resolve current CAPTCHAs in similar manners as
     human-users. By providing a CAPTCHA challenge from a library or set of
     challenges that are designed in a manner that causes or likely causes a
     human-user to trivially get the answer to the challenge wrong, helps to
     confirm that a user is a human-user, as a bot would answer the challenge
     correctly.
",G06F 21/31 (20130101); G06F 21/36 (20130101); G06F 2221/2133 (20130101),G06F 21/00 (20130101); G06F 21/31 (20130101); G06F 21/36 (20130101),"[['\n4971561', '\nNovember 1990'], ['\n6732112', '\nMay 2004'], ['\n7841940', '\nNovember 2010'], ['\n7904517', '\nMarch 2011'], ['\n7945952', '\nMay 2011'], ['\n8132255', '\nMarch 2012'], ['\n8316310', '\nNovember 2012'], ['\n8495518', '\nJuly 2013'], ['\n9105034', '\nAugust 2015'], ['\n9648029', '\nMay 2017'], ['\n2005/0157662', '\nJuly 2005'], ['\n2007/0026372', '\nFebruary 2007'], ['\n2008/0207316', '\nAugust 2008'], ['\n2009/0241174', '\nSeptember 2009'], ['\n2009/0249477', '\nOctober 2009'], ['\n2009/0319271', '\nDecember 2009'], ['\n2010/0046790', '\nFebruary 2010'], ['\n2010/0082998', '\nApril 2010'], ['\n2010/0106671', '\nApril 2010'], ['\n2010/0162404', '\nJune 2010'], ['\n2010/0262462', '\nOctober 2010'], ['\n2010/0302255', '\nDecember 2010'], ['\n2011/0166916', '\nJuly 2011'], ['\n2011/0178831', '\nJuly 2011'], ['\n2011/0212767', '\nSeptember 2011'], ['\n2012/0023574', '\nJanuary 2012'], ['\n2012/0158503', '\nJune 2012'], ['\n2012/0159564', '\nJune 2012'], ['\n2012/0159586', '\nJune 2012'], ['\n2012/0166409', '\nJune 2012'], ['\n2012/0167204', '\nJune 2012'], ['\n2012/0222100', '\nAugust 2012'], ['\n2012/0246008', '\nSeptember 2012'], ['\n2012/0246737', '\nSeptember 2012'], ['\n2013/0019286', '\nJanuary 2013'], ['\n2013/0036360', '\nFebruary 2013'], ['\n2013/0065517', '\nMarch 2013'], ['\n2013/0074150', '\nMarch 2013'], ['\n2013/0145314', '\nJune 2013'], ['\n2013/0160098', '\nJune 2013'], ['\n2013/0276125', '\nOctober 2013'], ['\n2014/0196133', '\nJuly 2014'], ['\n2014/0273987', '\nSeptember 2014'], ['\n2014/0317744', '\nOctober 2014'], ['\n2015/0067848', '\nMarch 2015'], ['\n2015/0156084', '\nJune 2015'], ['\n2015/0170204', '\nJune 2015'], ['\n2015/0180829', '\nJune 2015'], ['\n2015/0381652', '\nDecember 2015'], ['\n2016/0065559', '\nMarch 2016']]",[0]," What is claimed is:  1.  A computer-implemented method, comprising: performing, by a computer, a verification of a response to a security check that solicits input, the security check perceivable
by humans and automated agents, and comprising one or more of: (1) information soliciting a fact unascertainable from information in the security check;  (2) information soliciting a color of an object in an image or video;  or (3) information selected
from a plurality of permutations of a same type of security check;  as a result of the verification indicating that the response to the security check indicates correct completion of the security check, indicating, by the computer, an entity associated
with the response as associated with automated agent activity;  and for a request associated with the entity, performing, by the computer, one or more actions as a result of the entity being indicated as associated with automated agent activity.
 2.  The computer-implemented method of claim 1, wherein responses to the security check are classifiable as correct or incorrect.
 3.  The computer-implemented method of claim 1, wherein the security check solicits a count of items in content.
 4.  The computer-implemented method of claim 3, wherein content is text.
 5.  A system, comprising: at least one computing device implementing one or more services, wherein the one or more services: receive, from a computing device, a response to a security check that solicits input, the security check perceivable by
humans and automated agents, and comprising one or more of: (1) information soliciting a fact unascertainable from information in the security check;  (2) information soliciting a color of an object in an image or video;  or (3) information selected from
a plurality of permutations of a same type of security check;  as a result of the response from the computing device indicating correct completion of the security check, indicate an entity associated with the response as associated with automated agent
activity;  and for a network activity associated with the entity, perform one or more actions as a result of the entity being indicated as associated with automated agent activity.
 6.  The system of claim 5, wherein the security check solicits a count of items in text.
 7.  The system of claim 5, wherein the one or more services further: receive a second response to a second security check, the second security check soliciting second input;  as a result of the second response indicating incorrect completion of
the second security check, indicate a second entity associated with the second response as associated with human activity;  and for a network activity associated with the entity, perform one or more actions as a result of the second entity being
indicated as associated with human activity.
 8.  The system of claim 5, wherein the security check solicits a color of an item presented on a user interface.
 9.  The system of claim 5, wherein the security check solicits a characteristic of an item presented on a user interface.
 10.  The system of claim 5, wherein a correct answer to the security check is not ascertainable from the security check.
 11.  The system of claim 5, wherein the network activity comprises web page requests.
 12.  The system of claim 5, wherein: the security check presents a first set of first items and a second set of second items, the first items being similar to the second items;  and the security check solicits information about the first set of
first items.
 13.  A non-transitory computer-readable storage medium having stored thereon executable instructions that, if executed by one or more processors of a computer system, cause the computer system to at least: perform, by the computer system, a
verification of a response to a humanly perceivable security check that solicits input, the security check perceivable by humans and automated agents, and comprising one or more of: (1) information soliciting a fact unascertainable from information in
the security check;  (2) information soliciting a color of an object in an image or video;  or (3) information selected from a plurality of permutations of a same type of security check;  as a result of the verification indicating that the response to
the security check indicates correct completion of the security check, indicate by the computer system, an entity associated with the response as associated with automated agent activity;  and for a request associated with the entity, perform, by the
computer system, one or more actions as a result of the entity being indicated as associated with automated agent activity.
 14.  The non-transitory computer-readable storage medium of claim 13, wherein the one or more actions affect access to resources on a network.
 15.  The non-transitory computer-readable storage medium of claim 13, wherein the security check solicits information regarding information presented in a graphical user interface.
 16.  The non-transitory computer-readable storage medium of claim 13, wherein the instructions, if executed by the one or more processors of the computer system, further cause the computer system to: perform a verification that a second response
to a second security check that solicits the input;  and as a result of the verification indicating that the second response to the second security check indicates incorrect completion of the security check, indicate a second entity associated with the
second response as associated with human activity.
 17.  The non-transitory computer-readable storage medium of claim 13, wherein the security check solicits information about a passage of text.  "
"10,268,669","
     April 23, 2019
","Intelligent graphical word processing system and method
"," A graphical word processing system includes: a computing device including
     a display and a user input device; a user-facing application operable to
     be displayed on the computing device, the user-facing application
     including a text editing area for receiving textual user input from the
     user input device, the text editing area including at least a first tab
     and a second tab and a selectable shortcut button displayed on the
     computing device and selectable by the user; an artificial intelligence
     module for interpreting the textual user input of the first tab,
     retrieving content from a network based on the textual user input and a
     browsing history of the user on the computing device, and for displaying
     content based on the textual user input and retrieved content on the
     second tab of the user-facing application.
",G06F 3/04842 (20130101); G06F 40/106 (20200101); G06F 40/131 (20200101); G06F 3/0486 (20130101); G06F 40/216 (20200101); G06F 40/186 (20200101); G06F 40/169 (20200101); G06F 3/0483 (20130101); G06F 3/0482 (20130101); G06F 40/137 (20200101),G06F 17/00 (20190101); G06F 17/21 (20060101); G06F 17/22 (20060101); G06F 3/0482 (20130101); G06F 3/0483 (20130101); G06F 17/24 (20060101),"[['\n5341293', '\nAugust 1994'], ['\n6233583', '\nMay 2001'], ['\n8083523', '\nDecember 2011'], ['\n8137106', '\nMarch 2012'], ['\n8413045', '\nApril 2013'], ['\n9558577', '\nJanuary 2017'], ['\n9946434', '\nApril 2018'], ['\n2002/0156774', '\nOctober 2002'], ['\n2003/0046082', '\nMarch 2003'], ['\n2003/0177140', '\nSeptember 2003'], ['\n2004/0126749', '\nJuly 2004'], ['\n2005/0032027', '\nFebruary 2005'], ['\n2007/0156747', '\nJuly 2007'], ['\n2007/0174041', '\nJuly 2007'], ['\n2009/0222551', '\nSeptember 2009'], ['\n2011/0153324', '\nJune 2011'], ['\n2012/0254713', '\nOctober 2012'], ['\n2014/0136190', '\nMay 2014'], ['\n2014/0237359', '\nAugust 2014'], ['\n2014/0274254', '\nSeptember 2014'], ['\n2014/0323225', '\nOctober 2014'], ['\n2017/0256027', '\nSeptember 2017'], ['\n2017/0263248', '\nSeptember 2017']]","[2, '10,963,627', 'D877,162']"," What is claimed is:  1.  A graphical word processing system for constructing an electronic written document comprising: a computing device including a display and a user input device;  a
user-facing application operable to be displayed on the computing device, the user-facing application including a text editing area for receiving textual user input from the user input device, the text editing area including at least a first tab and a
second tab and one or more selectable shortcut buttons displayed on the computing device and selectable by the user;  an artificial intelligence module for interpreting the textual user input of the first tab, retrieving content from a network based on
the textual user input and a browsing history of the user on the computing device, and for displaying content based on the textual user input and retrieved content on the second tab of the user-facing application;  and a tile module for displaying
textual input received in the text editing area of the user-facing application as movable word tiles, the word tiles arrangeable by the user on the user-facing application to create one or more sentences;  wherein the electronic written document is
constructed from sentences created by the movable word tiles as arranged by the user on the user facing application.
 2.  The graphical word processing system of claim 1, the text editing area further including a third tab, wherein the artificial intelligence module displays hyperlinks corresponding to content retrieved by the artificial intelligence module in
the third tab.
 3.  The graphical word processing system of claim 1, the user-facing application further including a Category Organizer including a category area displaying one or more categories selectable by the user, wherein the movable tiles may be dropped
into one or more categories of the Organizer Area by the user.
 4.  The graphical word processing system of claim 1, wherein each of the movable tiles corresponding to words received in the text editing area includes a selectable trash button, wherein selection of the trash button moves the movable tile to a
trash portion of the user-facing application.
 5.  The graphical word processing system of claim 1, wherein the tile module converts each word within the text editing area into separate movable tiles displayed on the user-facing application.
 6.  The graphical word processing system of claim 1, wherein the tile module aggregates a plurality of separate movable tiles into individual tiles having multiple words on the tiles based on user selection of the movable tiles on the
user-facing application.
 7.  The graphical word processing system of claim 1, further comprising a trash portion for receiving discarded text.
 8.  A graphical word processing system for constructing an electronic written document comprising: a computing device including a display and a user input device;  a user-facing application operable to be displayed on the computing device, the
user-facing application including a text editing area for receiving textual user input from the user input device, the text editing area including at least a first tab and a second tab and a selectable shortcut button displayed on the computing device
and selectable by the user;  an artificial intelligence module for interpreting the textual user input of the first tab, retrieving content from a network based on the textual user input and a browsing history of the user on the computing device, and for
displaying content based on the textual user input and retrieved content on the second tab of the user-facing application;  and a tile module for displaying textual input received in the text editing area of the user-facing application as movable word
tiles wherein the tile module converts each word within the text editing area into separate movable tiles displayed on the user-facing application, the word tiles arrangeable by the user on the user-facing application to create one or more sentences; 
wherein the electronic written document is constructed from sentences created by the movable word tiles as arranged by the user on the user facing application.
 9.  The graphical word processing system of claim 8, wherein the tile module aggregates a plurality of separate movable tiles into individual tiles having multiple words on the tiles based on user selection of the movable tiles on the
user-facing application.  "
"10,268,969","
     April 23, 2019
","Artificial intelligence controlled entertainment performance
"," A process and computer program product to record performance related data
     for a plurality of entertainment performances having a plurality of
     audiences. Further, the process and computer program product determine
     external data that is associated with the plurality of audiences and/or
     environmental factors corresponding to locations of the plurality of
     entertainment performances. In addition, the process and computer program
     product annotate the performance related data with the external data to
     generate annotated performance related data. The process and computer
     program product also train an artificial intelligence system based upon
     the annotated performance related data. The process and computer program
     product generate, at the artificial intelligence performance instructions
     to provide a performance. Further, the process and computer program
     product provide, from the artificial intelligence system to a performance
     device, the performance instructions to provide a performance.
",G06N 3/006 (20130101); G06N 20/00 (20190101); G06Q 30/00 (20130101); A63J 2005/001 (20130101); G06N 5/00 (20130101),G06N 5/00 (20060101); G06N 3/00 (20060101); G06Q 30/00 (20120101); G06N 99/00 (20190101); A63J 5/00 (20060101),[],[1]," We claim:  1.  A method comprising: recording performance related data for a plurality of real-world entertainment performances having a plurality of audiences;  configuring one or more audience
sensors within an entertainment performance environment to sense real-world external data corresponding to one or more measurements of human sensory reception at real-world locations of the plurality of entertainment performances measured in response to
the plurality of real-world entertainment performances, the one or more audience sensors being selected from the group consisting: of a thermometer, a sound sensor, and a video recorder, the real-world external data being selected from the group
consisting of: gestures, timing, fidgeting, attention, silence, demographics, audience age, geographic location, room size, laughter, applause, booing, groaning, and emotional reaction;  annotating the performance related data with metadata corresponding
to the real-world external data;  providing a plurality of rules based on the metadata to an artificial intelligence system, the plurality of rules comprising one or more conditions associated with the real-world external data of the real-world
performance measured at the real-world locations within the entertainment performance environment;  training the artificial intelligence system to modify a selection of content for the real-world performance based upon the one or more conditions being
met;  generating, at the artificial intelligence system, performance instructions to provide the real-world performance, the performance instructions being based on the training;  and providing, from the artificial intelligence system to a performance
device, the performance instructions to provide the real-world performance.
 2.  The method of claim 1, further comprising receiving, at the artificial intelligence system, the real-world external data and modifying, at the artificial intelligence system, the performance instructions based upon the real-world external
data.
 3.  The method of claim 1, wherein the performance instructions comprise mapping instructions that map code instructions to a device in the performance device that performs an operation.
 4.  The method of claim 1, further comprising providing a script to the artificial based intelligence system such that the performance instructions are also based upon the script.
 5.  The method of claim 1, further comprising adjusting a portion of the performance instructions based upon a show shaper instruction.
 6.  A system comprising: a recording device that records performance related data for a plurality of real-world entertainment performances having a plurality of audiences;  one or more audience sensors configured to sense real-world external
data within an entertainment performance environment, the real-world external data corresponding to one or more measurements of human sensory reception at real-world locations of the plurality of entertainment performances measured in response to the
plurality of real-world entertainment performances, the one or more audience sensors being selected from the group consisting of: a thermometer, a sound sensor, and a video recorder, the real-world external data being selected from the group consisting
of: gestures, timing, fidgeting, attention, silence, demographics, audience age, geographic location, room size, laughter, applause, booing, groaning, and emotional reaction;  an annotation device that annotates the performance related data with metadata
corresponding to the real-world external data;  and an artificial intelligence system that is trained to modify a selection of content for the real-world performance based on one or more conditions being met, that generates performance instructions to
provide a real-world performance, and that provides the performance instructions to a performance device to provide the real-world performance, the one or more conditions being determined according to a plurality of rules based on the metadata, the one
or more conditions being associated with the real-world external data of the real-world performance measured at the real-world locations within the performance environment, the performance instructions being based on the training.
 7.  The system of claim 6, further comprising a plurality of real-world sensors that provide the real-world external data to the artificial intelligence system so that the artificial intelligence system modifies the performance instructions
based upon the real-world external data.
 8.  The system of claim 6, wherein the performance device is an animatronic.
 9.  The system of claim 6, wherein the performance device is a playback device.
 10.  The system of claim 6, further comprising a rules engine that provides the plurality of rules to the artificial based intelligence system.
 11.  The system of claim 6, further comprising a script engine that provides a script to the artificial based intelligence system such that the performance instructions are also based upon the script.
 12.  A computer program product comprising a computer readable storage device having a computer readable program stored thereon, wherein the computer readable program while executing on a computer causes the computer to: record performance
related data for a plurality of real-world entertainment performances having a plurality of audiences;  configure one or more audience sensors within an entertainment performance environment to sense real-world external data corresponding to one or more
measurements of human sensory reception at locations of the plurality of real-world entertainment performances measured in response to the plurality of real-world entertainment performances, the one or more audience sensors being selected from the group
consisting: of a thermometer, a sound sensor, and a video recorder, the real-world external data being selected from the group consisting of: gestures, timing, fidgeting, attention, silence, demographics, audience age, geographic location, room size,
laughter, applause, booing, groaning, and emotional reaction;  annotate the performance related data with metadata corresponding to the real-world external data;  provide a plurality of rules based on the metadata to an artificial intelligence system,
the plurality of rules comprising one or more conditions associated with the real-world external data of the real-world performance measured at the real-world locations within the performance environment;  train the artificial intelligence system to
modify a selection of content for the real-world performance based upon the one or more conditions being met;  generate, at the artificial intelligence system, performance instructions to provide the real-world performance, the performance instructions
being based on the training;  and provide, from the artificial intelligence system to a performance device, the performance instructions to provide the real-world performance.
 13.  The computer program product of claim 12, wherein the computer is further caused to receive, at the artificial intelligence system, the real-world external data and modify, at the artificial intelligence system, the performance instructions
based upon the external data.
 14.  The computer program product of claim 12, wherein the performance instructions comprise mapping instructions that map code instructions to a device in the performance device that performs an operation.
 15.  The computer program product of claim 12, further comprising providing a script to the artificial based intelligence system such that the performance instructions are also based upon the script. 
"
"10,270,599","
     April 23, 2019
","Data reproducibility using blockchains
"," Data verification in federate learning is faster and simpler. As
     artificial intelligence grows in usage, data verification is needed to
     prove custody and/or control. Electronic data representing an original
     version of training data may be hashed to generate one or more digital
     signatures. The digital signatures may then be incorporated into one or
     more blockchains for historical documentation. Any auditor may then
     quickly verify and/or reproduce the training data using the digital
     signatures. For example, a current version of the training data may be
     hashed and compared to the digital signatures generated from the current
     version of the training data. If the digital signatures match, then the
     training data has not changed since its creation. However, if the digital
     signatures do not match, then the training data has changed since its
     creation. The auditor may thus flag the training data for additional
     investigation and scrutiny.
",H04L 9/3236 (20130101); H04L 9/0891 (20130101); H04L 9/085 (20130101); H04L 9/0637 (20130101); H04L 9/3247 (20130101); H04L 9/0643 (20130101); G06N 20/00 (20190101); H04L 9/50 (20220501),H04W 12/06 (20090101); H04L 29/06 (20060101); H04L 9/32 (20060101); H04L 9/06 (20060101); G06N 20/00 (20190101),"[['\n4309569', '\nJune 1982'], ['\n5499294', '\nMarch 1996'], ['\n5862218', '\nJanuary 1999'], ['\n5966446', '\nOctober 1999'], ['\n7572179', '\nAugust 2009'], ['\n7729950', '\nJune 2010'], ['\n8245038', '\nAugust 2012'], ['\n8266439', '\nSeptember 2012'], ['\n8442903', '\nMay 2013'], ['\n8560722', '\nOctober 2013'], ['\n8706616', '\nApril 2014'], ['\n8712887', '\nApril 2014'], ['\n8867741', '\nOctober 2014'], ['\n8943332', '\nJanuary 2015'], ['\n9124423', '\nSeptember 2015'], ['\n9396006', '\nJuly 2016'], ['\n9407431', '\nAugust 2016'], ['\n9411524', '\nAugust 2016'], ['\n9411976', '\nAugust 2016'], ['\n9411982', '\nAugust 2016'], ['\n9424576', '\nAugust 2016'], ['\n9436935', '\nSeptember 2016'], ['\n9472069', '\nOctober 2016'], ['\n9489827', '\nNovember 2016'], ['\n9584493', '\nFebruary 2017'], ['\n9722790', '\nAugust 2017'], ['\n9876646', '\nJanuary 2018'], ['\n9882918', '\nJanuary 2018'], ['\n2003/0018563', '\nJanuary 2003'], ['\n2004/0085445', '\nMay 2004'], ['\n2005/0206741', '\nSeptember 2005'], ['\n2006/0075228', '\nApril 2006'], ['\n2006/0184443', '\nAugust 2006'], ['\n2007/0094272', '\nApril 2007'], ['\n2007/0296817', '\nDecember 2007'], ['\n2008/0010466', '\nJanuary 2008'], ['\n2009/0025063', '\nJanuary 2009'], ['\n2009/0287597', '\nNovember 2009'], ['\n2010/0049966', '\nFebruary 2010'], ['\n2010/0058476', '\nMarch 2010'], ['\n2010/0161459', '\nJune 2010'], ['\n2010/0241537', '\nSeptember 2010'], ['\n2013/0222587', '\nAugust 2013'], ['\n2014/0229738', '\nAugust 2014'], ['\n2014/0344015', '\nNovember 2014'], ['\n2016/0071096', '\nMarch 2016'], ['\n2016/0119134', '\nApril 2016'], ['\n2016/0148198', '\nMay 2016'], ['\n2016/0162897', '\nJune 2016'], ['\n2016/0217436', '\nJuly 2016'], ['\n2016/0253663', '\nSeptember 2016'], ['\n2016/0260091', '\nSeptember 2016'], ['\n2016/0267472', '\nSeptember 2016'], ['\n2016/0267558', '\nSeptember 2016'], ['\n2016/0275294', '\nSeptember 2016'], ['\n2016/0283920', '\nSeptember 2016'], ['\n2016/0292396', '\nOctober 2016'], ['\n2016/0292672', '\nOctober 2016'], ['\n2016/0292680', '\nOctober 2016'], ['\n2016/0300200', '\nOctober 2016'], ['\n2016/0300234', '\nOctober 2016'], ['\n2016/0321675', '\nNovember 2016'], ['\n2016/0321751', '\nNovember 2016'], ['\n2016/0328791', '\nNovember 2016'], ['\n2016/0330031', '\nNovember 2016'], ['\n2016/0330244', '\nNovember 2016'], ['\n2016/0337119', '\nNovember 2016'], ['\n2016/0342977', '\nNovember 2016'], ['\n2016/0342989', '\nNovember 2016'], ['\n2016/0344737', '\nNovember 2016'], ['\n2017/0005797', '\nJanuary 2017'], ['\n2017/0053249', '\nFebruary 2017'], ['\n2017/0061396', '\nMarch 2017'], ['\n2017/0124534', '\nMay 2017'], ['\n2017/0124535', '\nMay 2017'], ['\n2017/0177898', '\nJune 2017'], ['\n2017/0213287', '\nJuly 2017'], ['\n2017/0243208', '\nAugust 2017'], ['\n2017/0243289', '\nAugust 2017'], ['\n2017/0244757', '\nAugust 2017'], ['\n2017/0330279', '\nNovember 2017'], ['\n2017/0352031', '\nDecember 2017'], ['\n2017/0373859', '\nDecember 2017'], ['\n2018/0075527', '\nMarch 2018'], ['\n2018/0091524', '\nMarch 2018'], ['\n2018/0097779', '\nApril 2018'], ['\n2018/0157700', '\nJune 2018']]","[7, '11,341,492', '11,341,487', '11,310,311', '11,251,958', '11,244,306', '11,050,549', '11,032,077']"," The invention claimed is:  1.  A memory device storing instructions that when executed cause a hardware processor to perform operations, the operations comprising: receiving local updates sent
from a mobile device, the local updates generated by a federated learning model executed by the mobile device;  generating a learning modification to the federated learning model based on the local updates sent from the mobile device;  splitting the
local updates into multiple shares via a secret sharing algorithm;  generating cryptographic hash values by hashing the multiple shares using a cryptographic hashing algorithm;  determining a number N.sub.B of different blockchains for a distribution of
the cryptographic hash values, the number N.sub.B of the different blockchains based on a total number N.sub.S of the multiple shares according to a ratio of N.sub.S/N.sub.B having a maximum value;  distributing the cryptographic hash values via the
different blockchains, wherein the different blockchains comprise a master blockchain dedicated to the mobile device and a sub-blockchain dedicated to the federated learning model;  retrieving verification hash values generated by the hashing of current
versions of the local updates associated with the federated learning model using the cryptographic hashing algorithm;  comparing the cryptographic hash values distributed via the different blockchains to the verification hash values generated by the
hashing of the current versions of the local updates;  and verifying that the current versions of the local updates are authentic in response to the verification hash values satisfying the cryptographic hash values distributed via the different
blockchains.
 2.  The memory device of claim 1, wherein the operations further comprise determining a ratio N.sub.S/N.sub.B based on the number N.sub.B of the different blockchains and a total number N.sub.S of the multiple shares split via the secret sharing
algorithm.
 3.  The memory device of claim 1, wherein the operations further comprise determining a number N.sub.R of recipients of the different blockchains.
 4.  The memory device of claim 3, wherein the operations further comprise determining the number N.sub.B of the different blockchains based on the number N.sub.R of recipients of the different blockchains according to a ratio of N.sub.R/N.sub.B
having a maximum value.
 5.  A system, comprising: a hardware processor;  and a memory device, the memory device storing instructions, the instructions when executed causing the hardware processor to perform operations, the operations comprising: receiving local updates
generated by a federated learning model executed by a mobile device;  generating a learning modification to the federated learning model based on the local updates;  splitting the local updates into multiple shares via a secret sharing algorithm; 
generating cryptographic hash values by hashing the multiple shares using a cryptographic hashing algorithm;  determining a number N.sub.B of different blockchains for a distribution of the cryptographic hash values, the number N.sub.B of the different
blockchains based on a total number N.sub.S of the multiple shares according to a ratio of N.sub.S/N.sub.B having a maximum value;  distributing the cryptographic hash values via a master blockchain of the different blockchains dedicated to the mobile
device and via a sub-blockchain of the different blockchains dedicated to the federated learning model;  retrieving verification hash values generated by the hashing of current versions of the local updates associated with the federated learning model
using the cryptographic hashing algorithm;  comparing the cryptographic hash values distributed via the different blockchains to the verification hash values generated by the hashing of the current versions of the local updates;  and verifying that the
current versions of the local updates are authentic in response to the verification hash values satisfying the cryptographic hash values distributed via the different blockchains.
 6.  The system of claim 5, wherein the operations further comprise determining a number N.sub.R of recipients of the different blockchains.
 7.  The system of claim 5, wherein the operations further comprise determining the number N.sub.B of the different blockchains based on the number N.sub.R of recipients of the different blockchains according to a ratio of N.sub.R/N.sub.B having
a maximum value.
 8.  The system of claim 5, wherein the operations further comprise determining that the current versions of the local updates are inauthentic in response to the verification hash values failing to satisfy the cryptographic hash values.
 9.  The system of claim 5, wherein the operations further comprise receiving a noun identifier that identifies the federated learning model.
 10.  The system of claim 5, wherein the operations further comprise receiving a noun identifier that identifies the mobile device.
 11.  A method, comprising: receiving, by a server, local updates sent from a mobile device, the local updates generated by a federated learning model executed by the mobile device;  generating, by the server, a learning modification to the
federated learning model based on the local updates sent from the mobile device;  splitting, by the server, the local updates into multiple shares via a secret sharing algorithm;  generating, by the server, cryptographic hash values by hashing the
multiple shares using a cryptographic hashing algorithm;  determining, by the server, a number N.sub.B of different blockchains for a distribution of the cryptographic hash values, the number N.sub.B of the different blockchains based on a total number
N.sub.S of the multiple shares according to a ratio of N.sub.S/N.sub.B having a maximum value;  distributing, by the server, the cryptographic hash values via a master blockchain of the different blockchains dedicated to the mobile device and via a
sub-blockchain of the different blockchains dedicated to the federated learning model;  retrieving, by the server, verification hash values generated by the hashing of current versions of the local updates associated with the federated learning model
using the cryptographic hashing algorithm;  comparing, by the server, the cryptographic hash values to the verification hash values generated by the hashing of the current versions of the local updates;  and verifying, by the server, that the current
versions of the local updates are authentic in response to the verification hash values satisfying the cryptographic hash values.
 12.  The method of claim 11, further comprising determining a number N.sub.R of recipients of the different blockchains.
 13.  The method of claim 12, further comprising determining the number N.sub.B of the different blockchains based on the number N.sub.R of recipients of the different blockchains according to a ratio of N.sub.R/N.sub.B.
 14.  The method of claim 11, further comprising determining that the current versions of the local updates are inauthentic in response to the verification hash values failing to satisfy the cryptographic hash values.
 15.  The method of claim 11, further comprising receiving a noun identifier that identifies the federated learning model.
 16.  The method of claim 11, further comprising receiving a noun identifier that identifies the mobile device.  "
"10,275,176","
     April 30, 2019
","Data transformation offloading in an artificial intelligence
     infrastructure
"," Data transformation offloading in an artificial intelligence
     infrastructure that includes one or more storage systems and one or more
     graphical processing unit (`GPU`) servers, including: storing, within the
     storage system, a dataset; identifying, in dependence upon one or more
     machine learning models to be executed on the GPU servers, one or more
     transformations to apply to the dataset; and generating, by the storage
     system in dependence upon the one or more transformations, a transformed
     dataset.
",G06N 20/00 (20190101); G06F 9/5027 (20130101); G06N 3/08 (20130101); G06T 1/20 (20130101); G06F 3/0608 (20130101); G06F 9/4881 (20130101); G06T 1/60 (20130101); G06F 3/0649 (20130101); G06F 16/245 (20190101); G06F 3/0646 (20130101); G06F 3/067 (20130101); G06F 3/0604 (20130101); G06N 3/063 (20130101); G06T 2200/28 (20130101); G06F 16/248 (20190101); G06F 16/972 (20190101),G06F 12/02 (20060101); G06F 3/06 (20060101); G06N 20/00 (20190101),"[['\n5706210', '\nJanuary 1998'], ['\n5799200', '\nAugust 1998'], ['\n5933598', '\nAugust 1999'], ['\n6012032', '\nJanuary 2000'], ['\n6085333', '\nJuly 2000'], ['\n6643641', '\nNovember 2003'], ['\n6647514', '\nNovember 2003'], ['\n6789162', '\nSeptember 2004'], ['\n7089272', '\nAugust 2006'], ['\n7107389', '\nSeptember 2006'], ['\n7146521', '\nDecember 2006'], ['\n7334124', '\nFebruary 2008'], ['\n7437530', '\nOctober 2008'], ['\n7493424', '\nFebruary 2009'], ['\n7669029', '\nFebruary 2010'], ['\n7689609', '\nMarch 2010'], ['\n7743191', '\nJune 2010'], ['\n7899780', '\nMarch 2011'], ['\n8042163', '\nOctober 2011'], ['\n8086585', '\nDecember 2011'], ['\n8200887', '\nJune 2012'], ['\n8271700', '\nSeptember 2012'], ['\n8387136', '\nFebruary 2013'], ['\n8437189', '\nMay 2013'], ['\n8465332', '\nJune 2013'], ['\n8527544', '\nSeptember 2013'], ['\n8566546', '\nOctober 2013'], ['\n8578442', '\nNovember 2013'], ['\n8613066', '\nDecember 2013'], ['\n8620970', '\nDecember 2013'], ['\n8751463', '\nJune 2014'], ['\n8762642', '\nJune 2014'], ['\n8769622', '\nJuly 2014'], ['\n8800009', '\nAugust 2014'], ['\n8812860', '\nAugust 2014'], ['\n8850546', '\nSeptember 2014'], ['\n8898346', '\nNovember 2014'], ['\n8909854', '\nDecember 2014'], ['\n8931041', '\nJanuary 2015'], ['\n8949863', '\nFebruary 2015'], ['\n8984602', '\nMarch 2015'], ['\n8990905', '\nMarch 2015'], ['\n9081713', '\nJuly 2015'], ['\n9124569', '\nSeptember 2015'], ['\n9134922', '\nSeptember 2015'], ['\n9189334', '\nNovember 2015'], ['\n9209973', '\nDecember 2015'], ['\n9250823', '\nFebruary 2016'], ['\n9300660', '\nMarch 2016'], ['\n9311182', '\nApril 2016'], ['\n9444822', '\nSeptember 2016'], ['\n9507532', '\nNovember 2016'], ['\n9632870', '\nApril 2017'], ['\n10176217', '\nJanuary 2019'], ['\n2002/0013802', '\nJanuary 2002'], ['\n2003/0145172', '\nJuly 2003'], ['\n2003/0191783', '\nOctober 2003'], ['\n2003/0225961', '\nDecember 2003'], ['\n2004/0080985', '\nApril 2004'], ['\n2004/0111573', '\nJune 2004'], ['\n2004/0153844', '\nAugust 2004'], ['\n2004/0193814', '\nSeptember 2004'], ['\n2004/0260967', '\nDecember 2004'], ['\n2005/0010565', '\nJanuary 2005'], ['\n2005/0160416', '\nJuly 2005'], ['\n2005/0188246', '\nAugust 2005'], ['\n2005/0216800', '\nSeptember 2005'], ['\n2006/0015771', '\nJanuary 2006'], ['\n2006/0129817', '\nJune 2006'], ['\n2006/0161726', '\nJuly 2006'], ['\n2006/0230245', '\nOctober 2006'], ['\n2006/0239075', '\nOctober 2006'], ['\n2007/0022227', '\nJanuary 2007'], ['\n2007/0028068', '\nFebruary 2007'], ['\n2007/0055702', '\nMarch 2007'], ['\n2007/0109856', '\nMay 2007'], ['\n2007/0150689', '\nJune 2007'], ['\n2007/0168321', '\nJuly 2007'], ['\n2007/0220227', '\nSeptember 2007'], ['\n2007/0294563', '\nDecember 2007'], ['\n2007/0294564', '\nDecember 2007'], ['\n2008/0005587', '\nJanuary 2008'], ['\n2008/0077825', '\nMarch 2008'], ['\n2008/0133568', '\nJune 2008'], ['\n2008/0162674', '\nJuly 2008'], ['\n2008/0195833', '\nAugust 2008'], ['\n2008/0270678', '\nOctober 2008'], ['\n2008/0282045', '\nNovember 2008'], ['\n2009/0077340', '\nMarch 2009'], ['\n2009/0100115', '\nApril 2009'], ['\n2009/0198889', '\nAugust 2009'], ['\n2010/0052625', '\nMarch 2010'], ['\n2010/0211723', '\nAugust 2010'], ['\n2010/0246266', '\nSeptember 2010'], ['\n2010/0257142', '\nOctober 2010'], ['\n2010/0262764', '\nOctober 2010'], ['\n2010/0325345', '\nDecember 2010'], ['\n2010/0332754', '\nDecember 2010'], ['\n2011/0072290', '\nMarch 2011'], ['\n2011/0125955', '\nMay 2011'], ['\n2011/0131231', '\nJune 2011'], ['\n2011/0167221', '\nJuly 2011'], ['\n2012/0023144', '\nJanuary 2012'], ['\n2012/0054264', '\nMarch 2012'], ['\n2012/0079175', '\nMarch 2012'], ['\n2012/0079318', '\nMarch 2012'], ['\n2012/0131253', '\nMay 2012'], ['\n2012/0303919', '\nNovember 2012'], ['\n2012/0311000', '\nDecember 2012'], ['\n2013/0006976', '\nJanuary 2013'], ['\n2013/0007845', '\nJanuary 2013'], ['\n2013/0031414', '\nJanuary 2013'], ['\n2013/0036272', '\nFebruary 2013'], ['\n2013/0054716', '\nFebruary 2013'], ['\n2013/0071087', '\nMarch 2013'], ['\n2013/0145172', '\nJune 2013'], ['\n2013/0145447', '\nJune 2013'], ['\n2013/0191555', '\nJuly 2013'], ['\n2013/0198459', '\nAugust 2013'], ['\n2013/0205173', '\nAugust 2013'], ['\n2013/0219164', '\nAugust 2013'], ['\n2013/0227201', '\nAugust 2013'], ['\n2013/0290607', '\nOctober 2013'], ['\n2013/0311434', '\nNovember 2013'], ['\n2013/0318297', '\nNovember 2013'], ['\n2013/0332614', '\nDecember 2013'], ['\n2014/0016913', '\nJanuary 2014'], ['\n2014/0020083', '\nJanuary 2014'], ['\n2014/0074850', '\nMarch 2014'], ['\n2014/0082715', '\nMarch 2014'], ['\n2014/0086146', '\nMarch 2014'], ['\n2014/0090009', '\nMarch 2014'], ['\n2014/0096220', '\nApril 2014'], ['\n2014/0101434', '\nApril 2014'], ['\n2014/0164774', '\nJune 2014'], ['\n2014/0173232', '\nJune 2014'], ['\n2014/0195636', '\nJuly 2014'], ['\n2014/0201512', '\nJuly 2014'], ['\n2014/0201541', '\nJuly 2014'], ['\n2014/0208155', '\nJuly 2014'], ['\n2014/0215590', '\nJuly 2014'], ['\n2014/0229654', '\nAugust 2014'], ['\n2014/0230017', '\nAugust 2014'], ['\n2014/0258526', '\nSeptember 2014'], ['\n2014/0282983', '\nSeptember 2014'], ['\n2014/0285917', '\nSeptember 2014'], ['\n2014/0325262', '\nOctober 2014'], ['\n2014/0351627', '\nNovember 2014'], ['\n2014/0373104', '\nDecember 2014'], ['\n2014/0373126', '\nDecember 2014'], ['\n2015/0026387', '\nJanuary 2015'], ['\n2015/0074463', '\nMarch 2015'], ['\n2015/0089569', '\nMarch 2015'], ['\n2015/0095515', '\nApril 2015'], ['\n2015/0100412', '\nApril 2015'], ['\n2015/0113203', '\nApril 2015'], ['\n2015/0121137', '\nApril 2015'], ['\n2015/0134920', '\nMay 2015'], ['\n2015/0149822', '\nMay 2015'], ['\n2015/0193169', '\nJuly 2015'], ['\n2015/0331622', '\nNovember 2015'], ['\n2015/0378888', '\nDecember 2015'], ['\n2016/0085774', '\nMarch 2016'], ['\n2016/0098323', '\nApril 2016'], ['\n2016/0350009', '\nDecember 2016'], ['\n2016/0352720', '\nDecember 2016'], ['\n2016/0352830', '\nDecember 2016'], ['\n2016/0352834', '\nDecember 2016'], ['\n2017/0039774', '\nFebruary 2017'], ['\n2017/0249729', '\nAugust 2017'], ['\n2018/0181877', '\nJune 2018'], ['\n2018/0314603', '\nNovember 2018']]",[1]," What is claimed is:  1.  A method of data transformation offloading in an artificial intelligence infrastructure that includes one or more storage systems and one or more graphical processing
unit (`GPU`) servers, the method comprising: scheduling, by a unified management plane, one or more transformations for one or more of the storage systems to apply to the dataset;  scheduling, by the unified management plane, execution of one or more
machine learning algorithms associated with the machine learning model by the one or more GPU servers;  storing, within the storage system, a dataset;  identifying, in dependence upon one or more machine learning models to be executed on the GPU servers,
one or more transformations to apply to the dataset;  and generating, by the storage system in dependence upon the one or more transformations, a transformed dataset.
 2.  The method of claim 1 further comprising transmitting, from the storage system to the one or more GPU servers, the transformed dataset.
 3.  The method of claim 2 wherein transmitting, from the storage system to the one or more GPU servers, the transformed dataset further comprises transmitting the transformed dataset from the one or more storage systems directly to application
memory on the GPU servers.
 4.  The method of claim 1 further comprising executing, by one or more of the GPU servers, one or more machine learning algorithms associated with the machine learning model using the transformed dataset as input.
 5.  The method of claim 1 further comprising maintaining, by the storage system, information describing the dataset, the one or more transformations applied to the dataset, and the transformed dataset.
 6.  The method of claim 1 further comprising: receiving a first request to transmit the transformed dataset to one or more of the GPU servers;  transmitting, from the storage system to the one or more GPU servers, the transformed dataset; 
receiving a second request to transmit the transformed dataset to one or more of the GPU servers;  and transmitting, from the storage system to the one or more GPU servers without performing an additional transformation of the dataset, the transformed
dataset.
 7.  An artificial intelligence infrastructure that includes one or more storage systems and one or more graphical processing unit (`GPU`) servers, the artificial intelligence infrastructure configured to carry out the steps of: scheduling, by a
unified management plane, one or more transformations for one or more of the storage systems to apply to the dataset;  scheduling, by the unified management plane, execution of one or more machine learning algorithms associated with the machine learning
model by the one or more GPU servers;  storing, within the storage system, a dataset;  identifying, in dependence upon one or more machine learning models to be executed on the GPU servers, one or more transformations to apply to the dataset;  and
generating, by the storage system in dependence upon the one or more transformations, a transformed dataset.
 8.  The artificial intelligence infrastructure of claim 7 wherein the artificial intelligence infrastructure is further configured to carry out the step of transmitting, from the storage system to the one or more GPU servers, the transformed
dataset.
 9.  The artificial intelligence infrastructure of claim 8 wherein the artificial intelligence infrastructure is further configured to carry out the step of transmitting, from the storage system to the one or more GPU servers, the transformed
dataset further comprises transmitting the transformed dataset from the one or more storage systems directly to application memory on the GPU servers.
 10.  The artificial intelligence infrastructure of claim 7 wherein the artificial intelligence infrastructure is further configured to carry out the step of executing, by one or more of the GPU servers, one or more machine learning algorithms
associated with the machine learning model using the transformed dataset as input.
 11.  The artificial intelligence infrastructure of claim 7 wherein the artificial intelligence infrastructure is further configured to carry out the step of maintaining, by the storage system, information describing the dataset, the one or more
transformations applied to the dataset, and the transformed dataset.
 12.  The artificial intelligence infrastructure of claim 7 wherein the artificial intelligence infrastructure is further configured to carry out the steps of: receiving a first request to transmit the transformed dataset to one or more of the
GPU servers;  transmitting, from the storage system to the one or more GPU servers, the transformed dataset;  receiving a second request to transmit the transformed dataset to one or more of the GPU servers;  and transmitting, from the storage system to
the one or more GPU servers without performing an additional transformation of the dataset, the transformed dataset.
 13.  An apparatus for data transformation offloading in an artificial intelligence infrastructure that includes one or more storage systems and one or more graphical processing unit (`GPU`) servers, the apparatus comprising a computer processor
and a computer memory operatively coupled to the computer processor, the computer memory having disposed within it computer program instructions that, when executed by the computer processor, cause the apparatus to carry out the steps of: scheduling, by
a unified management plane, one or more transformations for one or more of the storage systems to apply to the dataset;  scheduling, by the unified management plane, execution of one or more machine learning algorithms associated with the machine
learning model by the one or more GPU servers;  storing, within the storage system, a dataset;  identifying, in dependence upon one or more machine learning models to be executed on the GPU servers, one or more transformations to apply to the dataset; 
and generating, by the storage system in dependence upon the one or more transformations, a transformed dataset.
 14.  The apparatus of claim 13 further comprising computer program instructions that, when executed by the computer processor, cause the apparatus to carry out the step of transmitting, from the storage system to the one or more GPU servers, the
transformed dataset.
 15.  The apparatus of claim 14 wherein transmitting, from the storage system to the one or more GPU servers, the transformed dataset further comprises transmitting the transformed dataset from the one or more storage systems directly to
application memory on the GPU servers.
 16.  The apparatus of claim 13 further comprising computer program instructions that, when executed by the computer processor, cause the apparatus to carry out the step of maintaining, by the storage system, information describing the dataset,
the one or more transformations applied to the dataset, and the transformed dataset.
 17.  The apparatus of claim 13 further comprising computer program instructions that, when executed by the computer processor, cause the apparatus to carry out the steps of: receiving a first request to transmit the transformed dataset to one or
more of the GPU servers;  transmitting, from the storage system to the one or more GPU servers, the transformed dataset;  receiving a second request to transmit the transformed dataset to one or more of the GPU servers;  and transmitting, from the
storage system to the one or more GPU servers without performing an additional transformation of the dataset, the transformed dataset.  "
"10,275,285","
     April 30, 2019
","Data transformation caching in an artificial intelligence infrastructure
"," Data transformation caching in an artificial intelligence infrastructure
     that includes one or more storage systems and one or more graphical
     processing unit (`GPU`) servers, including: identifying, in dependence
     upon one or more machine learning models to be executed on the GPU
     servers, one or more transformations to apply to a dataset; generating,
     in dependence upon the one or more transformations, a transformed
     dataset; storing, within one or more of the storage systems, the
     transformed dataset; receiving a plurality of requests to transmit the
     transformed dataset to one or more of the GPU servers; and responsive to
     each request, transmitting, from the one or more storage systems to the
     one or more GPU servers without re-performing the one or more
     transformations on the dataset, the transformed dataset.
",G06F 9/5027 (20130101); G06N 3/063 (20130101); G06F 16/245 (20190101); G06F 3/0604 (20130101); G06F 3/0649 (20130101); G06F 3/067 (20130101); G06T 1/20 (20130101); G06F 3/0608 (20130101); G06T 1/60 (20130101); G06N 3/08 (20130101); G06F 9/4881 (20130101); G06N 20/00 (20190101); G06F 3/0646 (20130101); G06F 16/248 (20190101); G06T 2200/28 (20130101); G06F 16/972 (20190101),G06F 9/50 (20060101); G06F 9/48 (20060101); G06T 1/60 (20060101); G06N 3/063 (20060101); G06T 1/20 (20060101); G06N 3/08 (20060101),"[['\n5706210', '\nJanuary 1998'], ['\n5799200', '\nAugust 1998'], ['\n5933598', '\nAugust 1999'], ['\n6012032', '\nJanuary 2000'], ['\n6085333', '\nJuly 2000'], ['\n6643641', '\nNovember 2003'], ['\n6647514', '\nNovember 2003'], ['\n6789162', '\nSeptember 2004'], ['\n7089272', '\nAugust 2006'], ['\n7107389', '\nSeptember 2006'], ['\n7146521', '\nDecember 2006'], ['\n7334124', '\nFebruary 2008'], ['\n7437530', '\nOctober 2008'], ['\n7493424', '\nFebruary 2009'], ['\n7669029', '\nFebruary 2010'], ['\n7689609', '\nMarch 2010'], ['\n7743191', '\nJune 2010'], ['\n7899780', '\nMarch 2011'], ['\n8042163', '\nOctober 2011'], ['\n8086585', '\nDecember 2011'], ['\n8200887', '\nJune 2012'], ['\n8271700', '\nSeptember 2012'], ['\n8387136', '\nFebruary 2013'], ['\n8437189', '\nMay 2013'], ['\n8465332', '\nJune 2013'], ['\n8527544', '\nSeptember 2013'], ['\n8566546', '\nOctober 2013'], ['\n8578442', '\nNovember 2013'], ['\n8613066', '\nDecember 2013'], ['\n8620970', '\nDecember 2013'], ['\n8751463', '\nJune 2014'], ['\n8762642', '\nJune 2014'], ['\n8769622', '\nJuly 2014'], ['\n8800009', '\nAugust 2014'], ['\n8812860', '\nAugust 2014'], ['\n8850546', '\nSeptember 2014'], ['\n8898346', '\nNovember 2014'], ['\n8909854', '\nDecember 2014'], ['\n8931041', '\nJanuary 2015'], ['\n8949863', '\nFebruary 2015'], ['\n8984602', '\nMarch 2015'], ['\n8990905', '\nMarch 2015'], ['\n9081713', '\nJuly 2015'], ['\n9124569', '\nSeptember 2015'], ['\n9134922', '\nSeptember 2015'], ['\n9189334', '\nNovember 2015'], ['\n9209973', '\nDecember 2015'], ['\n9250823', '\nFebruary 2016'], ['\n9300660', '\nMarch 2016'], ['\n9311182', '\nApril 2016'], ['\n9444822', '\nSeptember 2016'], ['\n9507532', '\nNovember 2016'], ['\n9632870', '\nApril 2017'], ['\n10025813', '\nJuly 2018'], ['\n2002/0013802', '\nJanuary 2002'], ['\n2003/0145172', '\nJuly 2003'], ['\n2003/0191783', '\nOctober 2003'], ['\n2003/0225961', '\nDecember 2003'], ['\n2004/0080985', '\nApril 2004'], ['\n2004/0111573', '\nJune 2004'], ['\n2004/0153844', '\nAugust 2004'], ['\n2004/0193814', '\nSeptember 2004'], ['\n2004/0260967', '\nDecember 2004'], ['\n2005/0010565', '\nJanuary 2005'], ['\n2005/0160416', '\nJuly 2005'], ['\n2005/0188246', '\nAugust 2005'], ['\n2005/0216800', '\nSeptember 2005'], ['\n2006/0015771', '\nJanuary 2006'], ['\n2006/0129817', '\nJune 2006'], ['\n2006/0161726', '\nJuly 2006'], ['\n2006/0230245', '\nOctober 2006'], ['\n2006/0239075', '\nOctober 2006'], ['\n2007/0022227', '\nJanuary 2007'], ['\n2007/0028068', '\nFebruary 2007'], ['\n2007/0055702', '\nMarch 2007'], ['\n2007/0109856', '\nMay 2007'], ['\n2007/0150689', '\nJune 2007'], ['\n2007/0168321', '\nJuly 2007'], ['\n2007/0220227', '\nSeptember 2007'], ['\n2007/0294563', '\nDecember 2007'], ['\n2007/0294564', '\nDecember 2007'], ['\n2008/0005587', '\nJanuary 2008'], ['\n2008/0077825', '\nMarch 2008'], ['\n2008/0133568', '\nJune 2008'], ['\n2008/0143707', '\nJune 2008'], ['\n2008/0162674', '\nJuly 2008'], ['\n2008/0195833', '\nAugust 2008'], ['\n2008/0270678', '\nOctober 2008'], ['\n2008/0282045', '\nNovember 2008'], ['\n2009/0077340', '\nMarch 2009'], ['\n2009/0100115', '\nApril 2009'], ['\n2009/0198889', '\nAugust 2009'], ['\n2010/0052625', '\nMarch 2010'], ['\n2010/0211723', '\nAugust 2010'], ['\n2010/0246266', '\nSeptember 2010'], ['\n2010/0257142', '\nOctober 2010'], ['\n2010/0262764', '\nOctober 2010'], ['\n2010/0325345', '\nDecember 2010'], ['\n2010/0332754', '\nDecember 2010'], ['\n2011/0072290', '\nMarch 2011'], ['\n2011/0125955', '\nMay 2011'], ['\n2011/0131231', '\nJune 2011'], ['\n2011/0167221', '\nJuly 2011'], ['\n2012/0023144', '\nJanuary 2012'], ['\n2012/0054264', '\nMarch 2012'], ['\n2012/0079318', '\nMarch 2012'], ['\n2012/0131253', '\nMay 2012'], ['\n2012/0303919', '\nNovember 2012'], ['\n2012/0311000', '\nDecember 2012'], ['\n2013/0006976', '\nJanuary 2013'], ['\n2013/0007845', '\nJanuary 2013'], ['\n2013/0031414', '\nJanuary 2013'], ['\n2013/0036272', '\nFebruary 2013'], ['\n2013/0054716', '\nFebruary 2013'], ['\n2013/0071087', '\nMarch 2013'], ['\n2013/0145447', '\nJune 2013'], ['\n2013/0191555', '\nJuly 2013'], ['\n2013/0198459', '\nAugust 2013'], ['\n2013/0205173', '\nAugust 2013'], ['\n2013/0219164', '\nAugust 2013'], ['\n2013/0227201', '\nAugust 2013'], ['\n2013/0290607', '\nOctober 2013'], ['\n2013/0311434', '\nNovember 2013'], ['\n2013/0318297', '\nNovember 2013'], ['\n2013/0332614', '\nDecember 2013'], ['\n2014/0016913', '\nJanuary 2014'], ['\n2014/0020083', '\nJanuary 2014'], ['\n2014/0074850', '\nMarch 2014'], ['\n2014/0082715', '\nMarch 2014'], ['\n2014/0086146', '\nMarch 2014'], ['\n2014/0090009', '\nMarch 2014'], ['\n2014/0096220', '\nApril 2014'], ['\n2014/0101434', '\nApril 2014'], ['\n2014/0164774', '\nJune 2014'], ['\n2014/0173232', '\nJune 2014'], ['\n2014/0195636', '\nJuly 2014'], ['\n2014/0201512', '\nJuly 2014'], ['\n2014/0201541', '\nJuly 2014'], ['\n2014/0208155', '\nJuly 2014'], ['\n2014/0215590', '\nJuly 2014'], ['\n2014/0229654', '\nAugust 2014'], ['\n2014/0230017', '\nAugust 2014'], ['\n2014/0258526', '\nSeptember 2014'], ['\n2014/0282983', '\nSeptember 2014'], ['\n2014/0285917', '\nSeptember 2014'], ['\n2014/0325262', '\nOctober 2014'], ['\n2014/0351627', '\nNovember 2014'], ['\n2014/0373104', '\nDecember 2014'], ['\n2014/0373126', '\nDecember 2014'], ['\n2015/0026387', '\nJanuary 2015'], ['\n2015/0074463', '\nMarch 2015'], ['\n2015/0089569', '\nMarch 2015'], ['\n2015/0095515', '\nApril 2015'], ['\n2015/0100412', '\nApril 2015'], ['\n2015/0113203', '\nApril 2015'], ['\n2015/0121137', '\nApril 2015'], ['\n2015/0134920', '\nMay 2015'], ['\n2015/0149822', '\nMay 2015'], ['\n2015/0193169', '\nJuly 2015'], ['\n2015/0331622', '\nNovember 2015'], ['\n2015/0378888', '\nDecember 2015'], ['\n2016/0085774', '\nMarch 2016'], ['\n2016/0098323', '\nApril 2016'], ['\n2016/0350009', '\nDecember 2016'], ['\n2016/0352720', '\nDecember 2016'], ['\n2016/0352830', '\nDecember 2016'], ['\n2016/0352834', '\nDecember 2016'], ['\n2016/0379083', '\nDecember 2016'], ['\n2017/0039774', '\nFebruary 2017'], ['\n2017/0153926', '\nJune 2017'], ['\n2017/0249729', '\nAugust 2017'], ['\n2017/0364703', '\nDecember 2017'], ['\n2018/0075115', '\nMarch 2018'], ['\n2018/0122101', '\nMay 2018'], ['\n2018/0204111', '\nJuly 2018'], ['\n2018/0217883', '\nAugust 2018'], ['\n2018/0260125', '\nSeptember 2018'], ['\n2018/0262864', '\nSeptember 2018']]",[1]," What is claimed is:  1.  A method of data transformation caching in an artificial intelligence infrastructure that includes one or more storage systems and one or more graphical processing unit
(`GPU`) servers, the method comprising: identifying, by one or more computer processors of the artificial intelligence infrastructure, in dependence upon one or more machine learning models to be executed on the GPU servers, one or more transformations
to apply to a dataset;  generating, in dependence upon the one or more transformations, a transformed dataset;  storing, within one or more of the storage systems, the transformed dataset;  receiving a plurality of requests to transmit the transformed
dataset to one or more of the GPU servers;  and responsive to each request, transmitting, from the one or more storage systems to the one or more GPU servers without re-performing the one or more transformations on the dataset, the transformed dataset.
 2.  The method of claim 1 wherein generating, in dependence upon the one or more transformations, a transformed dataset further comprises generating, by the storage system in dependence upon the one or more transformations, transformed dataset.
 3.  The method of claim 1 wherein transmitting, from the one or more storage systems to the one or more GPU servers without re-performing the one or more transformations on the dataset, the transformed dataset further comprises transmitting the
transformed dataset from the one or more storage systems directly to application memory on the GPU servers.
 4.  The method of claim 3 wherein transmitting the transformed dataset from the one or more storage systems directly to application memory on the GPU servers further comprises transmitting the transformed data dataset from the one or more
storage systems to the GPU servers via remote direct memory access (`RDMA`).
 5.  The method of claim 1 further comprising executing, by one or more of the GPU servers, one or more machine learning algorithms associated with the machine learning model using the transformed dataset as input.
 6.  The method of claim 1 further comprising: scheduling, by a unified management plane, one or more transformations for one or more of the storage systems to apply to the dataset;  and scheduling, by the unified management plane, execution of
one or more machine learning algorithms associated with the machine learning model by the one or more GPU servers.
 7.  The method of claim 1 further comprising providing, by a unified management plane to the one or more GPU servers, information describing the dataset, the one or more transformations applied to the dataset, and the transformed dataset.
 8.  An artificial intelligence infrastructure that includes one or more memory of storage systems and one or more graphical processing unit (`GPU`) servers, and one or more computer processors of the artificial intelligence infrastructure that
are configured to carry out the steps of: identifying, by the one or more computer processors of the artificial intelligence infrastructure, in dependence upon one or more machine learning models to be executed on the GPU servers, one or more
transformations to apply to a dataset;  generating, in dependence upon the one or more transformations, a transformed dataset;  storing, within the one or more memory of the storage systems, the transformed dataset;  receiving a plurality of requests to
transmit the transformed dataset to one or more of the GPU servers;  and responsive to each request, transmitting, from the one or more storage systems to the one or more GPU servers without re-performing the one or more transformations on the dataset,
the transformed dataset.
 9.  The artificial intelligence infrastructure of claim 8 wherein generating, in dependence upon the one or more transformations, a transformed dataset further comprises generating, by the storage system in dependence upon the one or more
transformations, transformed dataset.
 10.  The artificial intelligence infrastructure of claim 8 wherein transmitting, from the one or more storage systems to the one or more GPU servers without re-performing the one or more transformations on the dataset, the transformed dataset
further comprises transmitting the transformed dataset from the one or more storage systems directly to application memory on the GPU servers.
 11.  The artificial intelligence infrastructure of claim 10 wherein transmitting the transformed dataset from the one or more storage systems directly to application memory on the GPU servers further comprises transmitting the transformed data
dataset from the one or more storage systems to the GPU servers via remote direct memory access (`RDMA`).
 12.  The artificial intelligence infrastructure of claim 8 wherein the artificial intelligence infrastructure is further configured to carry out the step of executing, by one or more of the GPU servers, one or more machine learning algorithms
associated with the machine learning model using the transformed dataset as input.
 13.  The artificial intelligence infrastructure of claim 8 wherein the artificial intelligence infrastructure is further configured to carry out the steps of: scheduling, by a unified management plane, one or more transformations for one or more
of the storage systems to apply to the dataset;  and scheduling, by the unified management plane, execution of one or more machine learning algorithms associated with the machine learning model by the one or more GPU servers.
 14.  The artificial intelligence infrastructure of claim 8 wherein the artificial intelligence infrastructure is further configured to carry out the step of providing, by a unified management plane to the one or more GPU servers, information
describing the dataset, the one or more transformations applied to the dataset, and the transformed dataset.
 15.  An apparatus for data transformation offloading in an artificial intelligence infrastructure that includes one or more storage systems and one or more graphical processing unit (`GPU`) servers, the apparatus comprising a computer processor,
a computer memory operatively coupled to the computer processor, the computer memory having disposed within it computer program instructions that, when executed by the computer processor, cause the apparatus to carry out the steps of: identifying, in
dependence upon one or more machine learning models to be executed on the GPU servers, one or more transformations to apply to a dataset;  generating, in dependence upon the one or more transformations, a transformed dataset;  storing, within one or more
of the storage systems, the transformed dataset;  receiving a plurality of requests to transmit the transformed dataset to one or more of the GPU servers;  and responsive to each request, transmitting, from the one or more storage systems to the one or
more GPU servers without re-performing the one or more transformations on the dataset, the transformed dataset.
 16.  The apparatus of claim 15 wherein generating, in dependence upon the one or more transformations, a transformed dataset further comprises generating, by the storage system in dependence upon the one or more transformations, transformed
dataset.
 17.  The apparatus of claim 15 wherein transmitting, from the one or more storage systems to the one or more GPU servers without re-performing the one or more transformations on the dataset, the transformed dataset further comprises transmitting
the transformed dataset from the one or more storage systems directly to application memory on the GPU servers.
 18.  The apparatus of claim 15 further comprising computer program instructions that, when executed by the computer processor, cause the apparatus to carry out the steps of: scheduling, by a unified management plane, one or more transformations
for one or more of the storage systems to apply to the dataset;  and scheduling, by the unified management plane, execution of one or more machine learning algorithms associated with the machine learning model by the one or more GPU server.
 19.  The apparatus of claim 15 further comprising computer program instructions that, when executed by the computer processor, cause the apparatus to carry out the step of providing, by a unified management plane to the one or more GPU servers,
information describing the dataset, the one or more transformations applied to the dataset, and the transformed dataset.
 20.  The apparatus of claim 15 further comprising computer program instructions that, when executed by the computer processor, cause the apparatus to carry out the step of executing, by one or more of the GPU servers, one or more machine
learning algorithms associated with the machine learning model using the transformed dataset as input.  "
"10,288,439","
     May 14, 2019
","Systems and methods using artificial intelligence for routing electric
     vehicles
"," The present invention provides specific systems, methods and algorithms
     based on artificial intelligence expert system technology for
     determination of preferred routes of travel for electric vehicles (EVs).
     The systems, methods and algorithms provide such route guidance for
     battery-operated EVs in-route to a desired destination, but lacking
     sufficient battery energy to reach the destination from the current
     location of the EV. The systems and methods of the present invention
     disclose use of one or more specifically programmed computer machines
     with artificial intelligence expert system battery energy management and
     navigation route control. Such specifically programmed computer machines
     may be located in the EV and/or cloud-based or remote computer/data
     processing systems for the determination of preferred routes of travel,
     including intermediate stops at designated battery charging or
     replenishing stations. Expert system algorithms operating on combinations
     of expert defined parameter subsets for route selection are disclosed.
     Specific fuzzy logic methods are also disclosed based on defined
     potential route parameters with fuzzy logic determination of crisp
     numerical values for multiple potential routes and comparison of those
     crisp numerical values for selection of a particular route. Application
     of the present invention systems and methods to autonomous or driver-less
     EVs is also disclosed.
",G05D 1/0088 (20130101); B60L 58/16 (20190201); G01C 21/343 (20130101); G05D 1/0217 (20130101); G05D 1/0278 (20130101); G06N 5/048 (20130101); G05D 1/0285 (20130101); G01C 21/3469 (20130101); B60L 58/12 (20190201); G01C 21/3476 (20130101); G01C 21/3492 (20130101); B60L 2240/622 (20130101); B60L 2240/72 (20130101); B60L 2240/64 (20130101); G05D 2201/0213 (20130101); B60L 2240/68 (20130101); Y02T 90/167 (20130101); Y04S 30/12 (20130101); B60L 2250/10 (20130101); Y02T 90/16 (20130101); B60L 2250/16 (20130101); G08G 1/096811 (20130101); G08G 1/096833 (20130101); Y02T 10/72 (20130101); B60L 2240/66 (20130101); Y02T 10/70 (20130101); Y02T 90/168 (20130101),G01C 21/00 (20060101); G06N 3/04 (20060101); G05D 1/00 (20060101); G05D 1/02 (20060101); G01C 21/34 (20060101); G08G 1/0968 (20060101),"[['\n3662401', '\nMay 1972'], ['\n4852001', '\nJuly 1989'], ['\n5301320', '\nApril 1994'], ['\n5745687', '\nApril 1998'], ['\n5768506', '\nJune 1998'], ['\n5862346', '\nJanuary 1999'], ['\n5958071', '\nSeptember 1999'], ['\n6334137', '\nDecember 2001'], ['\n6487477', '\nNovember 2002'], ['\n6636884', '\nOctober 2003'], ['\n7024669', '\nApril 2006'], ['\n7408907', '\nAugust 2008'], ['\n7693486', '\nApril 2010'], ['\n7856360', '\nDecember 2010'], ['\n8170737', '\nMay 2012'], ['\n8229458', '\nJuly 2012'], ['\n8364171', '\nJanuary 2013'], ['\n8437776', '\nMay 2013'], ['\n8447331', '\nMay 2013'], ['\n8515459', '\nAugust 2013'], ['\n8566236', '\nOctober 2013'], ['\n8595824', '\nNovember 2013'], ['\n8626194', '\nJanuary 2014'], ['\n8635645', '\nJanuary 2014'], ['\n8639263', '\nJanuary 2014'], ['\n8826175', '\nSeptember 2014'], ['\n8880238', '\nNovember 2014'], ['\n8965669', '\nFebruary 2015'], ['\n9026347', '\nMay 2015'], ['\n9103686', '\nAugust 2015'], ['\n9112382', '\nAugust 2015'], ['\n9156369', '\nOctober 2015'], ['\n9170118', '\nOctober 2015'], ['\n9199548', '\nDecember 2015'], ['\n9302594', '\nApril 2016'], ['\n9333873', '\nMay 2016'], ['\n9335179', '\nMay 2016'], ['\n9346365', '\nMay 2016'], ['\n9610853', '\nApril 2017'], ['\n9713962', '\nJuly 2017'], ['\n9714837', '\nJuly 2017'], ['\n9739624', '\nAugust 2017'], ['\n2002/0038228', '\nMarch 2002'], ['\n2002/0147642', '\nOctober 2002'], ['\n2004/0022416', '\nFebruary 2004'], ['\n2006/0129313', '\nJune 2006'], ['\n2007/0281716', '\nDecember 2007'], ['\n2008/0133336', '\nJune 2008'], ['\n2008/0319597', '\nDecember 2008'], ['\n2009/0063680', '\nMarch 2009'], ['\n2009/0312903', '\nDecember 2009'], ['\n2010/0114798', '\nMay 2010'], ['\n2011/0022254', '\nJanuary 2011'], ['\n2011/0160992', '\nJune 2011'], ['\n2011/0191220', '\nAugust 2011'], ['\n2011/0288765', '\nNovember 2011'], ['\n2011/0301806', '\nDecember 2011'], ['\n2012/0036529', '\nFebruary 2012'], ['\n2012/0109519', '\nMay 2012'], ['\n2012/0136574', '\nMay 2012'], ['\n2012/0166012', '\nJune 2012'], ['\n2012/0179323', '\nJuly 2012'], ['\n2012/0179359', '\nJuly 2012'], ['\n2012/0246650', '\nSeptember 2012'], ['\n2012/0270573', '\nOctober 2012'], ['\n2012/0296678', '\nNovember 2012'], ['\n2013/0009765', '\nJanuary 2013'], ['\n2013/0041850', '\nFebruary 2013'], ['\n2013/0122851', '\nMay 2013'], ['\n2013/0138542', '\nMay 2013'], ['\n2013/0217366', '\nAugust 2013'], ['\n2013/0226441', '\nAugust 2013'], ['\n2013/0339072', '\nDecember 2013'], ['\n2013/0346902', '\nDecember 2013'], ['\n2014/0052373', '\nFebruary 2014'], ['\n2014/0142770', '\nMay 2014'], ['\n2014/0188304', '\nJuly 2014'], ['\n2014/0278104', '\nSeptember 2014'], ['\n2014/0354228', '\nDecember 2014'], ['\n2014/0371969', '\nDecember 2014'], ['\n2014/0379183', '\nDecember 2014'], ['\n2015/0045985', '\nFebruary 2015'], ['\n2015/0106001', '\nApril 2015'], ['\n2015/0149221', '\nMay 2015'], ['\n2015/0158486', '\nJune 2015'], ['\n2015/0241233', '\nAugust 2015'], ['\n2015/0253144', '\nSeptember 2015'], ['\n2015/0266356', '\nSeptember 2015'], ['\n2016/0052413', '\nFebruary 2016'], ['\n2016/0068121', '\nMarch 2016'], ['\n2016/0075247', '\nMarch 2016'], ['\n2016/0091338', '\nMarch 2016'], ['\n2016/0126732', '\nMay 2016'], ['\n2016/0332616', '\nNovember 2016'], ['\n2017/0245127', '\nAugust 2017'], ['\n2018/0017399', '\nJanuary 2018'], ['\n2018/0086264', '\nMarch 2018']]","[2, '11,314,259', '10,882,411']"," The embodiments of the invention in which an exclusive property or privilege is claimed are defined as follows:  1.  A method for routing an Electric Vehicle (EV) from a current position to a
destination wherein said method comprises one or more specifically programmed computer machines with artificial intelligence expert system battery energy management and route selection optimization control, said method further comprising: a step of
storing in electronic memory of said one or more specifically programmed computer machines artificial intelligence expert system software program code for battery energy management and route selection optimization control, said software program code
including;  battery energy and route selection optimization parameter definitions including range of parameter values and subsets of those defined ranges;  expert system propositional logic statements defining relationships between said battery energy
parameters and route selection optimization parameters based on parameter membership in said subset ranges;  a step of storing in in electronic memory of said one or more specifically programmed computer machines one or more of the following: EV
descriptive information, EV energy requirements, EV battery specification information, and EV current position and the location of the destination of said EV;  a step of monitoring and storing in electronic memory of said one or more specifically
programmed computer machines the status of said EV stored battery energy;  a step of executing said program code of said one or more specifically programmed computer machines with artificial intelligence expert system battery energy management and route
selection optimization control comprising: a step of comparing current EV stored battery energy to one or more defined thresholds;  a step of transmitting information from said EV to one more cloud or remote computer/data processing systems when said
battery energy is less than a selected threshold, wherein said transmitted information comprises one or more of the following: EV descriptive information, EV energy requirements, EV battery specification information, EV stored battery energy status, and
EV current GPS position and the EV destination address location;  a step of said EV receiving artificial intelligence expert system route selection optimization information from said one or more cloud or remote computer/data processing systems for
potential routes of travel, wherein said received route selection optimization information comprises: information regarding potential routes of travel for said EV to reach one or more battery charging or replacement stations and, after charging or
replacement, to continue on to said destination;  information regarding one or more route selection optimization parameters for each of said potential routes;  a step of artificial intelligence expert system evaluation of a potential route of travel by
one or more specifically programmed computer machines with artificial intelligence expert system battery energy management and route selection optimization control based at least in part on route selection optimization parameter membership in defined
parameter subsets and artificial intelligence expert system propositional logic statements;  and, a step of artificial intelligence expert system selection of a particular route of travel by one or more specifically programmed computer machine with
artificial intelligence expert system battery energy management and route selection optimization control based at least in part on comparisons of results from said individual route evaluations of potential routes of travel based on said received
information.
 2.  The method of claim 1 wherein said steps of artificial intelligence expert system evaluation and selection of a particular route of travel are executed by one or more specifically programmed computer machines located in the EV with
artificial intelligence expert system battery energy management and route selection optimization control.
 3.  The method of claim 1 wherein said steps of artificial intelligence expert system evaluation and selection of a particular route of travel are executed by one or more specifically programmed cloud based or remote computer/data processing
systems with artificial intelligence expert system battery energy management and route selection optimization control.
 4.  The method of claim 1 wherein said transmitted EV descriptive information comprises one or more of the following: vehicle type;  vehicle loaded weight;  and, vehicle energy requirement history.
 5.  The method of claim 1 wherein said transmitted EV battery specification information comprises one or more of the following: battery type;  battery capacity;  battery charging requirements;  battery age;  and, battery charging time.
 6.  The method of claim 1 wherein said route selection optimization parameters define for each EV potential route of travel the expected total travel time from the EV current location to the destination including intermediate battery charging or
replacement times and the total expected energy required to travel from the current position to the desired destination.
 7.  The method of claim 6 wherein said EV total travel time for each potential route includes route roadway considerations including at least one of roadway conditions, traffic congestion, weather conditions and/or emergency traffic
considerations.
 8.  The method of claim 7 wherein said EV route selection optimization information further includes consideration of actual or probable requests for route including battery charging or replacement station usage from other EVs traveling within a
defined radius or distance from said EV position.
 9.  The method of claim 1 wherein said EV is a self-driving vehicle.
 10.  The method of claim 9 wherein said system software program code for battery energy management and route selection optimization control comprises expert system artificial intelligence code with no required driver input for route decision
making.
 11.  A method for routing an Electric Vehicle (EV) from a current position to a destination wherein the said method comprises one or more specifically programmed computer machines with artificial intelligence expert system fuzzy logic battery
energy management and route selection optimization control, said method comprising: a step of storing in electronic memory of said one or more specifically programmed computer machines artificial intelligence expert system fuzzy logic software program
code for battery energy management and route selection optimization control;  a step of storing in in electronic memory of said one or more specifically programmed computer machines EV descriptive information, EV energy requirements, EV battery
specification information, the current position of said EV and the location of the destination of said EV;  a step of monitoring and storing in electronic memory of said one or more specifically programmed computer machines status of said EV stored
battery energy;  a step of executing said program code of said one or more specifically programmed computer machines with artificial intelligence expert system fuzzy logic battery energy management and route selection optimization control comprising: a
step of comparing current EV stored battery energy to defined thresholds to estimate sufficiency of said stored energy to reach said destination;  a step of transmitting information from said EV to one more cloud or remote computer/data processing
systems when said battery energy is less than a selected threshold, wherein said transmitted information comprises: EV descriptive information, EV battery specification information, EV energy requirements, EV stored battery energy status, and EV current
position and the destination location;  a step of receiving artificial intelligence expert system fuzzy logic derived route selection optimization information for potential routes of travel from said one or more cloud or remote computer/data processing
system, wherein said received route guidance information comprises: information regarding potential routes of travel for said EV to reach one or more battery charging or replacement stations and, after battery charging or replacement, to continue on to
said destination;  information regarding one or more route selection optimization parameters for each of said potential routes;  a step of artificial intelligence expert system fuzzy logic selection of a particular route of travel based at least in part
on said received information.
 12.  The method of claim 11 wherein said steps of artificial intelligence expert system fuzzy logic evaluation and selection of a particular route of travel are executed by one or more specifically programmed computer machines located in the EV
with artificial intelligence expert system fuzzy logic battery energy management and route selection optimization control.
 13.  The method of claim 12 wherein said steps of artificial intelligence expert system fuzzy logic evaluation and selection of a particular route of travel are executed by one or more specifically programmed cloud based or remote computer/data
processing system computer machines with artificial intelligence expert system fuzzy logic battery energy management and route selection optimization control.
 14.  The method of claim 11 wherein said route evaluation criteria includes relative predictions of considered route travel parameters including energy required by said EV to travel to said destination and travel time of said EV to said
destination.
 15.  The method of claim 14, wherein said relative predictions comprise defined fuzzy sets with possible overlapping parameter ranges and further wherein said artificial intelligence expert system fuzzy logic decisions are based on calculation
of a degree of membership in defined fuzzy sets for particular considered route evaluation parameters.
 16.  The method of claim 15 further comprising a step of defuzzifying multiple fuzzy logic degree of membership results to derive crisp numerical route selection indices values for particular routes considered.
 17.  The method of claim 16 further comprising selecting a particular recommended route of travel from among multiple such potential routes by comparing said derived crisp numerical route selection indices values for considered routes.
 18.  A method for routing a driverless Electric Vehicle (EV) from a current position to a destination wherein said method comprises one or more specifically programmed computer machines with artificial intelligence expert system battery energy
management and route selection optimization control, said method further comprising: a step of storing in electronic memory of said one or more specifically programmed computer machines artificial intelligence expert system software program code for
battery energy management and route selection optimization control, said software program code including;  battery energy and route selection optimization parameter definitions including range of parameter values and subsets of those defined ranges
expert system propositional logic statements defining relationships between said battery energy parameters and route selection optimization parameters based on parameter membership in said subset ranges;  a step of storing in in electronic memory of said
one or more specifically programmed computer machines one or more of the following: EV descriptive information, EV energy requirements, EV battery specification information, and EV current position and the location of the destination of said EV;  a step
of monitoring and storing in electronic memory of said one or more specifically programmed computer machines the status of said EV stored battery energy;  a step of executing said program code of said one or more specifically programmed computer machines
with artificial intelligence expert system battery energy management and route selection optimization control comprising: a step of comparing current EV stored battery energy to one or more defined thresholds;  a step of transmitting information from
said EV to one more cloud or remote computer/data processing systems when said battery energy is less than a selected threshold, wherein said transmitted information comprises one or more of the following: EV descriptive information, EV energy
requirements, EV battery specification information, EV stored battery energy status, and EV current GPS position and the EV destination address location;  a step of said EV receiving artificial intelligence expert system derived route selection
optimization information from said one or more cloud or remote computer/data processing systems for potential routes of travel, wherein said received route selection optimization information comprises: information regarding potential routes of travel for
said EV to reach one or more battery charging or replacement stations and, after charging or replacement, to continue on to said destination;  information regarding one or more route selection optimization parameters for each of said potential routes;  a
step of artificial intelligence expert system evaluation of a particular route of travel by one or more specifically programmed computer machines with artificial intelligence expert system battery energy management and route selection optimization
control based at least in part on said received information, route selection optimization parameter membership in defined parameter subsets and artificial intelligence expert system propositional logic statements;  and, a step of artificial intelligence
expert system selection of a particular route of travel by one or more specifically programmed computer machines with artificial intelligence expert system battery energy management and navigation route selection optimization control based at least in
part on comparisons of results from said individual route evaluations of potential routes of travel based on said received information, whereby said driverless EV is guided along an artificial intelligence expert system selected route chosen from one or
more other potential routes based on defined route selection optimization criteria without requiring additional EV driver input or control actions.
 19.  The method of claim 18 wherein said steps of artificial intelligence expert system evaluation and selection of a particular route of travel are executed by one or more specifically programmed computer machines located in the EV with
artificial intelligence expert system battery energy management and route selection optimization control.
 20.  The method of claim 18 wherein said steps of artificial intelligence expert system evaluation and selection of a particular route of travel are executed by one or more specifically programmed cloud based or remote computer/data processing
system computer machines with artificial intelligence expert system battery energy management and route selection optimization control.  "
"10,289,076","
     May 14, 2019
","Concierge robot system, concierge service method, and concierge robot
"," A concierge robot system, a concierge service method, and a concierge
     robot are provided. The system provides an artificial intelligence type
     of concierge service, and includes: a user interface device that receives
     an external image and an external voice, and outputs the received image
     or voice on a screen or by voice; a storage device where a program that
     provides data through the user interface device based on learning data
     generated by using a neural network model is stored; and a processor that
     executes the program, wherein the program includes instructions for
     recognizing an emotion of a user, identified from the external image
     based on the learning data, outputting data that represents an emotion
     according to the emotion recognition to the screen, generating a
     conversation sentence that corresponds to natural language of web data
     externally collected through web scraping based on the learning data and
     outputting it by voice, generating user recommendation data for the
     identified user based on the learning data, and outputting the user
     recommendation data on the screen or processing it into natural language
     and outputting a corresponding conversation sentence by voice.
",G06N 3/0445 (20130101); B25J 11/001 (20130101); G06Q 20/209 (20130101); G06Q 30/0277 (20130101); G06N 3/084 (20130101); G06N 3/0454 (20130101); G06Q 30/0269 (20130101); G06N 5/022 (20130101); G06N 3/008 (20130101); G06Q 20/202 (20130101); G06N 3/08 (20130101); G05B 13/027 (20130101); G06Q 20/00 (20130101); G06Q 20/10 (20130101); G06N 20/10 (20190101); G05B 13/041 (20130101),G06N 3/08 (20060101); G06Q 20/10 (20120101); G05B 13/04 (20060101); G06Q 30/02 (20120101); G06Q 20/20 (20120101); G05B 13/02 (20060101); B25J 11/00 (20060101); G06N 5/02 (20060101),"[['\n2012/0191460', '\nJuly 2012'], ['\n2016/0114488', '\nApril 2016']]","[3, '11,163,987', '10,937,446', '10,565,434']"," What is claimed is:  1.  A concierge robot comprising: a main body having a robot shape;  a camera that is provided in the main body to capture an external image;  a microphone that is provided
in the main body to receive an external voice;  a speaker that is provided in the main body to output a voice;  a display device that is provided in the main body to output data on a screen;  an input/output port that is provided in the main body and
connected with a communication network and an external device;  a point of sales (POS) device that is provided in the main body and processes an order of a product or a service according to a result of a voice recognition, recognizes a payment means in a
contact or non-contact manner, performs a payment process with a payment server that is connected with the communication network, and outputs details of the order and payment;  a light emission unit that is provided in the main body and includes at least
one light emitting diode (LEDs) that is driven according to a control signal;  an actuator driver that is provided in the main body and generates a driving signal;  an operation member that is provided in the main body to rotate within a range of a
predetermined rotation angle or move in a predetermined direction by being driven according to the driving signal received from the actuator driver;  a short-distance communication device that is provided in the main body and performs short-distance
wireless communication with a user terminal;  a memory that is provided in the main body to store a program that controls operation of the concierge robot based on learning data generated by using a neural network model;  and a processor that executes
the program, wherein the program includes instructions for: recognizing a face image from the external image input from the camera, recognizing an emotion that corresponds to the face image using a convolutional neural network (CNN) model, generating
emotion expression data including an emoticon or graphic data and outputting the generated emotion expression data to the display device, outputting a signal that controls at least one of light emission time, a color of light emission, and a light
emission amount according to the generated emotion expression data to the light emission unit, generating a first conversation sentence that corresponds to a natural language of web data externally collected through web scraping based on the learning
data and outputting the first conversation sentence by the voice, recognizing the external voice received from the microphone, generating a second conversation sentence corresponding to an answer according to a recognition result of the external voice
and outputting the second conversation sentence by the voice, requesting the POS device to process the order of the product or and service according to the recognition result of the external voice and collecting the details of the order and payment from
the POS device and storing the details as user personal data, generating a relationship between the user personal data, emotion learning data and customer relationship management (CRM) data as a neural network and generating a user recommendation data
including menu recommendation data or promotion information by backward propagating the neural network, outputting the user recommendation data to the display device, processing the user recommendation data into the natural language and outputting a
corresponding third conversation sentence by the voice, processing the generated emotion expression data and the user recommendation data, learning a fourth conversation sentence that corresponds to the natural language by using the neural network model,
and generating the fourth conversation sentence that corresponds to each the emotion expression data and the user recommendation data by using a result of the learning data generated by using the neural network model, broadcasting a concierge service
providing URL for providing marketing information that includes at least one of service recommendation information, promotion information and advertisement information derived by matching the user personal data and the CRM data through a short-distance
communication device, processing a concierge service data into the natural language and outputting a corresponding fifth conversation sentence by the voice, wherein the concierge service data is defined by a model, view, and controller (MVC) and
comprises at least one of membership guide information, a personal information usage agreement request information, service shop information, service menu information, a waiting number guide information, a product information or service order
information, and an online payment service information, the concierge service providing URL comprising at least one of an access address of a concierge service server, an access address of a web space and an access address of a storage device, and the
user terminal that selects the concierge service providing URL receives the concierge service data from the access address.
 2.  The concierge robot system of claim 1, further comprising a server interface device that transmits and receives data to/from an external cloud server in a wired or wireless manner, wherein the program include instructions for transmitting
the face image recognized from the external image to the cloud server, receiving the emotion learning data generated by emotion learning that corresponds to the face image from the external cloud server;  and for receiving emotion recognition
corresponding to the face image by transmitting an emotion recognition request with respect to the face image to the external cloud server.  "
"10,289,522","
     May 14, 2019
","Autonomous information technology diagnostic checks
"," A rules-based engine uses artificial intelligence to perform an automated
     diagnostic check of a computer chassis. The rules-based engine consults a
     dynamically-changing dependency map to determine what diagnostic data is
     retrieved. The rules-based engine also monitors a database of events to
     determine when the diagnostic data is retrieved. A baseboard management
     controller may then execute a diagnostic algorithm that conducts the
     automated diagnostic check according to the dependency map and the
     database of events. Exemplary embodiments thus offer a machine learning,
     artificial intelligence scheme for autonomously performed diagnostic
     testing of processor, memory, software, and networking functions.
",G06F 11/3476 (20130101); G06N 5/045 (20130101); G06F 11/3452 (20130101); G06N 20/00 (20190101),G06F 11/34 (20060101); G06N 20/00 (20190101),"[['\n64497369', '\nSeptember 2002'], ['\n7401143', '\nJuly 2008'], ['\n10003665', '\nJune 2018'], ['\n2006/0053094', '\nMarch 2006']]",[0]," What is claimed is:  1.  A method for conducting a diagnostic check associated with a computer chassis, comprising: generating, by a baseboard management controller, using rule-based artificial
intelligence, an event specifying when to conduct the diagnostic check;  generating, by the baseboard management controller, using the rule-based artificial intelligence, a dependency map specifying diagnostic data that is electronically associated with
the event;  triggering, by the baseboard management controller, the diagnostic check based on the event generated using the rule-based artificial intelligence;  retrieving, by the baseboard management controller, the diagnostic data that is
electronically associated with the event;  and executing, by the baseboard management controller, the diagnostic check of an internal component operating within the computer chassis, the internal component specified by the event, the internal component
diagnostically checked according to the diagnostic data.
 2.  The method of claim 1, further comprising generating a diagnostic result of the diagnostic check.
 3.  The method of claim 2, further comprising logging the diagnostic result of the diagnostic check.
 4.  The method of claim 2, further comprising generating a notification of the diagnostic result of the diagnostic check.
 5.  The method of claim 1, further comprising identifying the diagnostic check in response to the event generated using the rule-based artificial intelligence.
 6.  The method of claim 1, further comprising querying the dependency map for the event generated using the rule-based artificial intelligence.
 7.  The method of claim 6, further comprising identifying the diagnostic data specified by the dependency map that is electronically associated with the event.
 8.  An information handling system, comprising: a baseboard management controller;  and a memory device accessible to the baseboard management controller, the memory device storing instructions, the instructions when executed causing the
baseboard management controller to perform operations, the operations including: generating, using rule-based artificial intelligence, an event specifying when to conduct a diagnostic check;  generating, using the rule-based artificial intelligence, a
dependency map specifying diagnostic data that is electronically associated with the event;  triggering the diagnostic check based on the event generated using the rule-based artificial intelligence;  retrieving the diagnostic data that is electronically
associated with the event;  and executing the diagnostic check of an internal component operating within a computer chassis housing the baseboard management controller, the internal component specified by the event, the internal component diagnostically
checked according to the diagnostic data.
 9.  The system of claim 8, wherein the operations further comprise generating a diagnostic result of the diagnostic check.
 10.  The system of claim 9, wherein the operations further comprise logging the diagnostic result of the diagnostic check.
 11.  The system of claim 9, wherein the operations further comprise generating a notification of the diagnostic result of the diagnostic check.
 12.  The system of claim 8, wherein the operations further comprise identifying the diagnostic check in response to the event generated using the rule-based artificial intelligence.
 13.  The system of claim 8, wherein the operations further comprise querying the dependency map for the event generated using the rule-based artificial intelligence.
 14.  The system of claim 8, wherein the operations further comprise identifying the diagnostic data 22 specified by the dependency map that is electronically associated with the event.
 15.  A memory device storing instructions that when executed cause a baseboard management controller to perform operations, the operations comprising: generating, using rule-based artificial intelligence, an event associated with a diagnostic
check of an internal component operating within a computer chassis;  generating, using the rule-based artificial intelligence, a dependency map specifying diagnostic data that is electronically associated with the event;  triggering the event generated
using the rule-based artificial intelligence;  querying the dependency map in response to the triggering of the event, the dependency map electronically associating events to the diagnostic data and to diagnostic checks;  retrieving the diagnostic data
from the dependency map that is electronically associated with the event;  identifying the diagnostic check of the diagnostic checks in the dependency map that is electronically associated with the event;  and executing the diagnostic check of the
internal component operating within the computer chassis according to the diagnostic data.
 16.  The memory device of claim 15, wherein the operations further comprise generating a diagnostic result of the diagnostic check.
 17.  The memory device of claim 16, wherein the operations further comprise logging the diagnostic result of the diagnostic check.
 18.  The memory device of claim 16, wherein the operations further comprise generating a notification of the diagnostic result of the diagnostic check.
 19.  The memory device of claim 15, wherein the operations further comprise determining a date and time at which the event triggers.
 20.  The memory device of claim 15, wherein the operations further comprise diagnostically testing a software application executed by the internal component operating within the computer chassis.  "
"10,289,959","
     May 14, 2019
","Artificial intelligence and knowledge based automation enhancement
"," This invention generally relates to a process, system and computer code
     for updating of computer applications based on collecting automation
     information related to a current application such as processing power,
     load, footprint, and performance attributes, determining a system
     automation profile; using an artificial intelligence based modeler for
     analyzing data, applying the data to an artificial intelligence model for
     training and predicting performance, adjusting the artificial
     intelligence model to achieve an updated automation criteria with optimal
     values, wherein the optimal values provide input to an automation
     criteria library for storing and updating a prior automation criteria,
     and exporting the upgraded automation criteria values for incorporation
     in a computer-to-be-updated, to achieve a reliable automatic update.
",G06N 20/00 (20190101); G06N 5/04 (20130101); G06N 3/08 (20130101),G06N 3/08 (20060101); G06N 20/00 (20190101),"[['\n8504803', '\nAugust 2013'], ['\n9462042', '\nOctober 2016'], ['\n2016/0019049', '\nJanuary 2016']]","[11, '11,354,222', '11,354,131', '11,221,854', '11,093,378', '10,929,278', '10,922,083', '10,915,428', '10,747,544', '10,733,540', '10,642,719', '10,637,928']"," We claim:  1.  A computerized method for updating a remote computer based on artificial intelligence methods comprises the steps of: A. (a) utilizing one or more of hard-wired circuitry or
software instructions for collecting automation information related to a current application by comparing system inputs and outputs to and from a service provider computer and a remote computer as a candidate for updating, respectively, to stored results
in the remote computer to optimize automation fit in the remote computer, such as processing power, load, footprint and performance attributes;  (b) locating application controls;  (c) scanning the computer to extract data on processing power, load,
footprint, and performance attributes;  (d) using the data in an automation profile generator for determining a system automation profile;  (e) using an artificial intelligence based modeler for analyzing input and output data;  (f) and applying the data
to an artificial intelligence model for training and predicting performance, and wherein if the training and predicting function is not satisfied during test, the artificial intelligence model is adjusted and the process of creating a satisfactory
performance model is repeated, until the training and predicting function is satisfied;  (g) then merging the artificial intelligence model within a cloud based database;  (h) adjusting the artificial intelligence model to achieve an updated automation
criteria with optimal values, wherein the optimal values provide input to an automation criteria library for storing and updating a prior automation criteria;  and (i) utilizing one or more of hard-wired circuitry or software instruction for exporting
the upgraded automation criteria values for incorporation in the remote computer to optimize automation fit in the remote computer such as processing power, load, footprint and performance attributes, and wherein one of hard-wired circuitry or software
instruction implements a process for: B. executing (a) one or more of a series of commands, executable as variables by the remote computer's application;  (b) one or more tasks assembled into execution files, which validate the tasks and organize nested
tasks, including (i) collecting nested task information for each task, (ii) accounting for all dependencies to insure that files, tasks, and environments for operating the remote computer to achieve optimal automation fit such as processing power, load,
footprint and performance attributes.
 2.  The method of claim 1, wherein the artificial intelligence model utilizes one of supervised and unsupervised learning methods, regression analysis, Bayesian statistical models, and clustering algorithms.
 3.  The method of claim 1, further including comparing system inputs and automation outputs from the service provider computer to stored results to obtain the optimal automation fit for the remote computer under examination, as a candidate for
updating.
 4.  The method of claim 1, further including utilizing one or more of hard-wired circuitry or software instructions for determining system automation profiles for a remote computer's processing power, processing load, speed, processor type,
manufacture application footprint, characteristics and specific application.
 5.  The method of claim 1, further including constructing artificial intelligence based learning mechanisms to derive key automation criteria.
 6.  The method of claim 1, further including combining artificial intelligence based learning on the service provider system, with broader learning from multiple similar systems, to provide recommendations on automation criteria such as optimal
automation speed, best automation technique, and recommendations for optimal delays.
 7.  The method of claim 1, further including creating portable automation criteria libraries, correlated to a system with a specific automation profiles.
 8.  A non-transitory computer-readable medium for creating adapters that enable updating a remote computer including steps of: A. (a) collecting automation information related to a current application by comparing system inputs and outputs to
and from a service provider computer system and a remote computer as a candidate for updating, respectively, to stored results in the remote computer as a candidate for updating to optimize automation fit in the remote computer such as processing power,
load, footprint and performance attributes;  (b) locating application controls;  (c) scanning the computer application to extract data on processing power, load, footprint, and performance attributes;  (d) using the data in an automation profile
generator for determining a system automation profile;  (e) using an artificial intelligence based modeler for analyzing input and output data;  (f) and applying the data to an artificial intelligence model for training and predicting performance, and
wherein if the training and predicting function is not satisfied during test, the artificial intelligence model is adjusted and the process of creating a satisfactory performance model is repeated, until the training and predicting function is satisfied; (g) adjusting the artificial intelligence model to achieve an updated automation criteria with optimal values, wherein the optimal values provide input to an automation criteria library for storing and updating a prior automation criteria;  and (h)
exporting the upgraded automation criteria values for incorporation in a remote computer to optimize automation fit in the remote computer such as processing power, load, footprint and performance attributes wherein: B. one or more tasks in the service
provider computer have (a) one or more of a series of commands, executable as variables by the remote computer application;  (b) one or more tasks assembled into execution files, which validate the tasks and organize nested tasks, including (i)
collecting nested task information for each task, (ii) accounting for all dependencies to insure that files, tasks, and environments for operating the remote computer to optimize automation fit in the remote computer, such as processing power, load,
footprint and performance attributes.
 9.  A computerized method for creating adapters to update a computer-to-be-updated based on artificial intelligence methods comprises the steps of: (a) utilizing one or more of hard-wired circuitry or software instructions for collecting
automation information related to a current computer by comparing system inputs and outputs to and from a service provider computer and a computer-to-be-updated as a candidate for updating, respectively, to stored results in the remote computer to
optimize automation fit in the computer-to-be-updated, such as processing power, load, footprint and performance attributes, as a candidate for updating;  (b) locating application controls;  (c) scanning the computer-to-be-updated to extract data on
processing power, load, footprint, and performance attributes;  (d) using the data in an automation profile generator for determining a system automation profile;  (e) using an artificial intelligence based modeler for analyzing input and output data; 
(f) and applying the data to an artificial intelligence model for training and predicting performance, and wherein if the training and predicting function is not satisfied during test, the artificial intelligence model is adjusted and the process of
creating a satisfactory performance model is repeated, until the training and predicting function is satisfied;  (g) then merging the artificial intelligence model within a cloud based database;  (h) adjusting the artificial intelligence model to achieve
an updated automation criteria with optimal values, wherein the optimal values provide input to an automation criteria library for storing and updating a prior automation criteria;  and (i) utilizing one or more of hard-wired circuitry or software
instruction for exporting the upgraded automation criteria values for incorporation in the computer-to-be-updated to achieve a reliable automatic update to optimize automation fit in the computer-to-be-updated such as processing power, load, footprint
and performance attributes incorporation in the computer-to-be-updated to achieve a reliable automatic update.  "
"10,291,067","
     May 14, 2019
","Computer modeling for resonant power transfer systems
"," A method for modeling, designing, and/or optimizing a wireless power
     transfer system, such as a resonant power transfer system. The method may
     be based on artificial intelligence or expert systems. A computer program
     product for optimizing a power transfer system is also disclosed.
",H02J 50/10 (20160201); H02J 50/40 (20160201); H02J 50/12 (20160201),H02J 5/00 (20160101); H02J 50/10 (20160101); H02J 50/12 (20160101); H02J 50/40 (20160101),"[['\n4041955', '\nAugust 1977'], ['\n4352960', '\nOctober 1982'], ['\n4561443', '\nDecember 1985'], ['\n4561444', '\nDecember 1985'], ['\n4630615', '\nDecember 1986'], ['\n4679560', '\nJuly 1987'], ['\n4726378', '\nFebruary 1988'], ['\n4736747', '\nApril 1988'], ['\n4924171', '\nMay 1990'], ['\n4945305', '\nJuly 1990'], ['\n5070223', '\nDecember 1991'], ['\n5346458', '\nSeptember 1994'], ['\n5350413', '\nSeptember 1994'], ['\n5569156', '\nOctober 1996'], ['\n5630836', '\nMay 1997'], ['\n5690693', '\nNovember 1997'], ['\n5702431', '\nDecember 1997'], ['\n5755748', '\nMay 1998'], ['\n5771438', '\nJune 1998'], ['\n5831248', '\nNovember 1998'], ['\n5948006', '\nSeptember 1999'], ['\n6123726', '\nSeptember 2000'], ['\n6149683', '\nNovember 2000'], ['\n6212430', '\nApril 2001'], ['\n6296533', '\nOctober 2001'], ['\n6312338', '\nNovember 2001'], ['\n6320354', '\nNovember 2001'], ['\n6324431', '\nNovember 2001'], ['\n6327504', '\nDecember 2001'], ['\n6389318', '\nMay 2002'], ['\n6400991', '\nJune 2002'], ['\n6442434', '\nAugust 2002'], ['\n6451055', '\nSeptember 2002'], ['\n6458164', '\nOctober 2002'], ['\n6478820', '\nNovember 2002'], ['\n6553263', '\nApril 2003'], ['\n6579315', '\nJune 2003'], ['\n6591139', '\nJuly 2003'], ['\n6605032', '\nAugust 2003'], ['\n6647298', '\nNovember 2003'], ['\n6650213', '\nNovember 2003'], ['\n6723039', '\nApril 2004'], ['\n6772011', '\nAugust 2004'], ['\n6801807', '\nOctober 2004'], ['\n6810289', '\nOctober 2004'], ['\n6850803', '\nFebruary 2005'], ['\n6894456', '\nMay 2005'], ['\n6895281', '\nMay 2005'], ['\n6949065', '\nSeptember 2005'], ['\n6960968', '\nNovember 2005'], ['\n6967621', '\nNovember 2005'], ['\n6985773', '\nJanuary 2006'], ['\n7015769', '\nMarch 2006'], ['\n7107103', '\nSeptember 2006'], ['\n7126310', '\nOctober 2006'], ['\n7225032', '\nMay 2007'], ['\n7246040', '\nJuly 2007'], ['\n7286880', '\nOctober 2007'], ['\n7428438', '\nSeptember 2008'], ['\n7471986', '\nDecember 2008'], ['\n7496733', '\nFebruary 2009'], ['\n7505816', '\nMarch 2009'], ['\n7515012', '\nApril 2009'], ['\n7522878', '\nApril 2009'], ['\n7532901', '\nMay 2009'], ['\n7565187', '\nJuly 2009'], ['\n7571007', '\nAugust 2009'], ['\n7574173', '\nAugust 2009'], ['\n7587241', '\nSeptember 2009'], ['\n7599743', '\nOctober 2009'], ['\n7650187', '\nJanuary 2010'], ['\n7650192', '\nJanuary 2010'], ['\n7711433', '\nMay 2010'], ['\n7720546', '\nMay 2010'], ['\n7741734', '\nJune 2010'], ['\n7761164', '\nJuly 2010'], ['\n7774069', '\nAugust 2010'], ['\n7782190', '\nAugust 2010'], ['\n7805200', '\nSeptember 2010'], ['\n7812481', '\nOctober 2010'], ['\n7818036', '\nOctober 2010'], ['\n7818037', '\nOctober 2010'], ['\n7825543', '\nNovember 2010'], ['\n7830114', '\nNovember 2010'], ['\n7865245', '\nJanuary 2011'], ['\n7872367', '\nJanuary 2011'], ['\n7904170', '\nMarch 2011'], ['\n7932696', '\nApril 2011'], ['\n7962222', '\nJune 2011'], ['\nRE42682', '\nSeptember 2011'], ['\n8076801', '\nDecember 2011'], ['\n8081925', '\nDecember 2011'], ['\n8096954', '\nJanuary 2012'], ['\n8140168', '\nMarch 2012'], ['\n8150529', '\nApril 2012'], ['\n8165694', '\nApril 2012'], ['\n8185212', '\nMay 2012'], ['\n8193766', '\nJune 2012'], ['\n8203434', '\nJune 2012'], ['\n8244367', '\nAugust 2012'], ['\n8247926', '\nAugust 2012'], ['\n8258653', '\nSeptember 2012'], ['\n8265770', '\nSeptember 2012'], ['\n8278784', '\nOctober 2012'], ['\n8292052', '\nOctober 2012'], ['\n8299652', '\nOctober 2012'], ['\n8301079', '\nOctober 2012'], ['\n8319473', '\nNovember 2012'], ['\n8362742', '\nJanuary 2013'], ['\n8373310', '\nFebruary 2013'], ['\n8378522', '\nFebruary 2013'], ['\n8378523', '\nFebruary 2013'], ['\n8463395', '\nJune 2013'], ['\n8489200', '\nJuly 2013'], ['\n8551163', '\nOctober 2013'], ['\n8562508', '\nOctober 2013'], ['\n8581793', '\nNovember 2013'], ['\n8587154', '\nNovember 2013'], ['\n8620447', '\nDecember 2013'], ['\n8628460', '\nJanuary 2014'], ['\n8629578', '\nJanuary 2014'], ['\n8668473', '\nMarch 2014'], ['\n8694117', '\nApril 2014'], ['\n8810071', '\nAugust 2014'], ['\n8884468', '\nNovember 2014'], ['\n8909351', '\nDecember 2014'], ['\n8971958', '\nMarch 2015'], ['\n9002468', '\nApril 2015'], ['\n9106083', '\nAugust 2015'], ['\n9192704', '\nNovember 2015'], ['\n9302093', '\nApril 2016'], ['\n9515494', '\nDecember 2016'], ['\n9515495', '\nDecember 2016'], ['\n9560787', '\nJanuary 2017'], ['\n2002/0038138', '\nMarch 2002'], ['\n2002/0087204', '\nJuly 2002'], ['\n2002/0093456', '\nJuly 2002'], ['\n2003/0171792', '\nSeptember 2003'], ['\n2004/0138725', '\nJuly 2004'], ['\n2004/0256146', '\nDecember 2004'], ['\n2005/0006083', '\nJanuary 2005'], ['\n2005/0288743', '\nDecember 2005'], ['\n2006/0199997', '\nSeptember 2006'], ['\n2006/0271129', '\nNovember 2006'], ['\n2007/0096686', '\nMay 2007'], ['\n2007/0123948', '\nMay 2007'], ['\n2007/0142696', '\nJune 2007'], ['\n2007/0191706', '\nAugust 2007'], ['\n2008/0009198', '\nJanuary 2008'], ['\n2008/0027293', '\nJanuary 2008'], ['\n2008/0054638', '\nMarch 2008'], ['\n2008/0100294', '\nMay 2008'], ['\n2008/0149736', '\nJune 2008'], ['\n2008/0167531', '\nJuly 2008'], ['\n2008/0211320', '\nSeptember 2008'], ['\n2009/0018616', '\nJanuary 2009'], ['\n2009/0051224', '\nFebruary 2009'], ['\n2009/0072628', '\nMarch 2009'], ['\n2009/0081943', '\nMarch 2009'], ['\n2009/0174264', '\nJuly 2009'], ['\n2009/0212736', '\nAugust 2009'], ['\n2009/0226328', '\nSeptember 2009'], ['\n2009/0270679', '\nOctober 2009'], ['\n2009/0284220', '\nNovember 2009'], ['\n2010/0019985', '\nJanuary 2010'], ['\n2010/0033021', '\nFebruary 2010'], ['\n2010/0035453', '\nFebruary 2010'], ['\n2010/0045114', '\nFebruary 2010'], ['\n2010/0063347', '\nMarch 2010'], ['\n2010/0066305', '\nMarch 2010'], ['\n2010/0069992', '\nMarch 2010'], ['\n2010/0109958', '\nMay 2010'], ['\n2010/0114143', '\nMay 2010'], ['\n2010/0122995', '\nMay 2010'], ['\n2010/0171368', '\nJuly 2010'], ['\n2010/0184371', '\nJuly 2010'], ['\n2010/0190459', '\nJuly 2010'], ['\n2010/0194334', '\nAugust 2010'], ['\n2010/0210233', '\nAugust 2010'], ['\n2010/0211134', '\nAugust 2010'], ['\n2010/0222848', '\nSeptember 2010'], ['\n2010/0222849', '\nSeptember 2010'], ['\n2010/0225174', '\nSeptember 2010'], ['\n2010/0225271', '\nSeptember 2010'], ['\n2010/0244576', '\nSeptember 2010'], ['\n2010/0253340', '\nOctober 2010'], ['\n2010/0256708', '\nOctober 2010'], ['\n2010/0277120', '\nNovember 2010'], ['\n2010/0277121', '\nNovember 2010'], ['\n2010/0308939', '\nDecember 2010'], ['\n2010/0314946', '\nDecember 2010'], ['\n2010/0331919', '\nDecember 2010'], ['\n2011/0025132', '\nFebruary 2011'], ['\n2011/0043050', '\nFebruary 2011'], ['\n2011/0046699', '\nFebruary 2011'], ['\n2011/0057607', '\nMarch 2011'], ['\n2011/0101790', '\nMay 2011'], ['\n2011/0109263', '\nMay 2011'], ['\n2011/0115431', '\nMay 2011'], ['\n2011/0127848', '\nJune 2011'], ['\n2011/0148215', '\nJune 2011'], ['\n2011/0178361', '\nJuly 2011'], ['\n2011/0181235', '\nJuly 2011'], ['\n2011/0205083', '\nAugust 2011'], ['\n2011/0234155', '\nSeptember 2011'], ['\n2011/0241436', '\nOctober 2011'], ['\n2011/0245892', '\nOctober 2011'], ['\n2011/0266880', '\nNovember 2011'], ['\n2011/0276110', '\nNovember 2011'], ['\n2011/0278948', '\nNovember 2011'], ['\n2011/0291489', '\nDecember 2011'], ['\n2011/0291613', '\nDecember 2011'], ['\n2011/0295345', '\nDecember 2011'], ['\n2011/0298294', '\nDecember 2011'], ['\n2011/0301667', '\nDecember 2011'], ['\n2011/0313238', '\nDecember 2011'], ['\n2012/0001485', '\nJanuary 2012'], ['\n2012/0032522', '\nFebruary 2012'], ['\n2012/0039102', '\nFebruary 2012'], ['\n2012/0057322', '\nMarch 2012'], ['\n2012/0065458', '\nMarch 2012'], ['\n2012/0080957', '\nApril 2012'], ['\n2012/0091951', '\nApril 2012'], ['\n2012/0104997', '\nMay 2012'], ['\n2012/0109256', '\nMay 2012'], ['\n2012/0119914', '\nMay 2012'], ['\n2012/0146575', '\nJune 2012'], ['\n2012/0149229', '\nJune 2012'], ['\n2012/0150259', '\nJune 2012'], ['\n2012/0153739', '\nJune 2012'], ['\n2012/0153954', '\nJune 2012'], ['\n2012/0157753', '\nJune 2012'], ['\n2012/0157754', '\nJune 2012'], ['\n2012/0158407', '\nJune 2012'], ['\n2012/0161539', '\nJune 2012'], ['\n2012/0164943', '\nJune 2012'], ['\n2012/0169132', '\nJuly 2012'], ['\n2012/0169133', '\nJuly 2012'], ['\n2012/0169137', '\nJuly 2012'], ['\n2012/0169139', '\nJuly 2012'], ['\n2012/0169278', '\nJuly 2012'], ['\n2012/0175967', '\nJuly 2012'], ['\n2012/0235364', '\nSeptember 2012'], ['\n2012/0239118', '\nSeptember 2012'], ['\n2012/0245649', '\nSeptember 2012'], ['\n2012/0245664', '\nSeptember 2012'], ['\n2012/0259398', '\nOctober 2012'], ['\n2012/0274148', '\nNovember 2012'], ['\n2012/0306433', '\nDecember 2012'], ['\n2013/0007949', '\nJanuary 2013'], ['\n2013/0060103', '\nMarch 2013'], ['\n2013/0119773', '\nMay 2013'], ['\n2013/0127253', '\nMay 2013'], ['\n2013/0149960', '\nJune 2013'], ['\n2013/0159956', '\nJune 2013'], ['\n2013/0190551', '\nJuly 2013'], ['\n2013/0197607', '\nAugust 2013'], ['\n2013/0214731', '\nAugust 2013'], ['\n2013/0241306', '\nSeptember 2013'], ['\n2013/0241468', '\nSeptember 2013'], ['\n2013/0271088', '\nOctober 2013'], ['\n2013/0289334', '\nOctober 2013'], ['\n2013/0310630', '\nNovember 2013'], ['\n2013/0320773', '\nDecember 2013'], ['\n2013/0331638', '\nDecember 2013'], ['\n2014/0005466', '\nJanuary 2014'], ['\n2014/0011447', '\nJanuary 2014'], ['\n2014/0028110', '\nJanuary 2014'], ['\n2014/0028111', '\nJanuary 2014'], ['\n2014/0031606', '\nJanuary 2014'], ['\n2014/0152252', '\nJune 2014'], ['\n2014/0163644', '\nJune 2014'], ['\n2014/0265620', '\nSeptember 2014'], ['\n2014/0265621', '\nSeptember 2014'], ['\n2014/0275727', '\nSeptember 2014'], ['\n2015/0123654', '\nMay 2015'], ['\n2015/0229289', '\nAugust 2015'], ['\n2015/0290373', '\nOctober 2015'], ['\n2016/0135684', '\nMay 2016'], ['\n2016/0218732', '\nJuly 2016'], ['\n2016/0250484', '\nSeptember 2016'], ['\n2016/0254703', '\nSeptember 2016'], ['\n2016/0254704', '\nSeptember 2016']]",[0]," What is claimed:  1.  A method for modeling a wireless energy transmission circuit having a transmitter exciter coil, a transmitter resonator coil, a receiver exciter coil, and a receiver
resonator coil, the method performed using a computer and comprising: randomly choosing, using the computer, a plurality of different sets of input parameters for the wireless energy transmission circuit;  selecting, from the plurality of sets of
randomly chosen input parameters, using the computer, a selected set of input parameters;  generating, using the computer, an initial population of circuits based on the selected set of input parameters, the initial population of circuits having the same
circuit structure as the wireless energy transmission circuit;  selecting, using the computer, individual component values for the initial population of circuits based on a resonant frequency analysis;  randomly varying, using the computer, the selected
individual component values to generate a larger population of multiple different circuits;  evaluating, using the computer, the larger population of circuits;  selecting, using the computer, a set of circuits based on the evaluating;  and generating,
using the computer, a new population of circuits based on the selected set of circuits.
 2.  The method according to claim 1, wherein the evaluating comprises scoring each circuit in the larger population of circuits.
 3.  The method according to claim 2, wherein the scoring is based on a plurality of system operating parameters.
 4.  The method according to claim 2, wherein the operating parameters are selected from the group comprising efficiency of wireless power transfer, voltage gain, input current, power lost at the receiver, resonator voltage, and any combination
of the same.
 5.  The method according to claim 2, wherein the score is the RMS sum of scores for each operating parameter.
 6.  The method according to claim 3, wherein the scores of the operating parameters are weighted.
 7.  The method according to claim 1, wherein the selecting comprises identifying the set of circuits by highest score.
 8.  The method according to claim 1, wherein the selected set of circuits includes a plurality of circuits.
 9.  The method according to claim 1, wherein the selected set of circuits includes at least one model circuit having diverse characteristics from the other circuits.
 10.  The method according to claim 1, further comprising adding to the set of selected circuits by mating together portions from different circuits.
 11.  The method according to claim 1, further comprising repeating the evaluating, selecting, and generating until an optimized circuit having a desired characteristic is discovered.
 12.  A non-transitory computer readable medium comprising instructions for modeling a wireless energy transmission circuit having a transmitter exciter coil, a transmitter resonator coil, a receiver exciter coil, and a receiver resonator coil,
wherein when executed, the instructions cause a computer to: randomly choose a plurality of different sets of input parameters for the wireless energy transmission circuit;  select, from the plurality of sets of randomly chosen input parameters, a
selected set of input parameters;  generate an initial population of circuits based on the selected set of input parameters, the initial population of circuits having the same circuit structure as the wireless energy transmission circuit;  select
individual component values for the initial population of circuits based on a resonant frequency analysis;  randomly vary the selected individual component values to generate a larger population of multiple different circuits;  evaluate the larger
population of circuits;  select at least one circuit based on the evaluation;  and generate a new population based on at least one selected circuit.
 13.  The non-transitory computer readable medium according to claim 12, wherein the instructions cause the computer to generate a new population by generating a plurality of circuits based on combinations of parts of circuits in the initial
population.
 14.  The non-transitory computer readable medium according to claim 12, wherein the instructions cause the computer to select at least one circuit by assigning a score based on a plurality of parameters.
 15.  The non-transitory computer readable medium according to claim 14, wherein the parameters are selected from the group comprising efficiency of wireless power transfer, voltage gain, input current, power lost at the receiver, resonator
voltage, and any combination of the same.  "
"10,296,794","
     May 21, 2019
","On-demand artificial intelligence and roadway stewardship system
"," The present disclosure relates to artificial intelligence based systems
     and method for determination of traffic violations. The present
     disclosure provides systems and methods that use deep convolutional
     neural networks and machine vision based algorithms to perform a task of
     detection and recognition to provide complete solution to safe, legal and
     comfortable parking, driving and riding for commuters on the roadways.
     Roadway stewardship systems, Parking management systems when made
     on-demand and crowdsourced, can play a very strong role in regulating
     driving conditions in cities and highways. By allowing the on-demand,
     crowdsourced, roadway stewardship system to be automated, through the use
     of Artificial Intelligence (AI) sub-systems, users can be trained to
     recognize and be educated as well in the laws & regulations around the
     use of roadways; can help the process through an interactive
     console/game-play, which can also be used for monetization for
     individuals to earn money for their contribution. The AI assisted with
     Human Intelligence (HI) together called HAI in particular, can play a
     valuable role in reducing traffic density, traffic movement restrictions
     and fuel and time waste in large cities. Also proper driving on the roads
     can lead to faster and safer commute. In Addition, multiple other objects
     of interest can also be identified and trained to be recognized using the
     Stewardship System disclosed herein.
",G06N 3/0454 (20130101); G06V 20/54 (20220101); G06N 3/084 (20130101); G08G 1/141 (20130101); G08G 1/054 (20130101); G06V 10/255 (20220101); G06N 3/02 (20130101); G06Q 50/26 (20130101); G08G 1/147 (20130101); G06N 20/00 (20190101); G08G 1/143 (20130101); G08G 1/0175 (20130101); G06K 9/6267 (20130101); G06V 10/25 (20220101); G06F 15/76 (20130101); G06N 3/0481 (20130101); G06N 20/10 (20190101),G06K 9/00 (20060101); G06K 9/32 (20060101); G08G 1/14 (20060101); G08G 1/017 (20060101); G08G 1/054 (20060101); G06Q 50/26 (20120101); G06F 15/76 (20060101); G06N 3/02 (20060101); G06N 20/00 (20190101); G06K 9/62 (20060101),"[['\n6546119', '\nApril 2003'], ['\n6560529', '\nMay 2003'], ['\n7363133', '\nApril 2008'], ['\n7720580', '\nMay 2010'], ['\n8041080', '\nOctober 2011'], ['\n8184863', '\nMay 2012'], ['\n8446467', '\nMay 2013'], ['\n8509486', '\nAugust 2013'], ['\n8587649', '\nNovember 2013'], ['\n8983136', '\nMarch 2015'], ['\n9286524', '\nMarch 2016'], ['\n9305223', '\nApril 2016'], ['\n9317752', '\nApril 2016'], ['\n9428192', '\nAugust 2016'], ['\n9477892', '\nOctober 2016'], ['\n9704060', '\nJuly 2017'], ['\n9760806', '\nSeptember 2017'], ['\n9779314', '\nOctober 2017'], ['\n9779331', '\nOctober 2017'], ['\n9916755', '\nMarch 2018'], ['\n2009/0273711', '\nNovember 2009'], ['\n2011/0109476', '\nMay 2011'], ['\n2012/0148105', '\nJune 2012'], ['\n2013/0049988', '\nFebruary 2013'], ['\n2013/0311075', '\nNovember 2013'], ['\n2014/0376769', '\nDecember 2014'], ['\n2015/0235091', '\nAugust 2015'], ['\n2015/0278609', '\nOctober 2015'], ['\n2015/0279021', '\nOctober 2015'], ['\n2015/0310274', '\nOctober 2015'], ['\n2015/0310624', '\nOctober 2015'], ['\n2016/0148058', '\nMay 2016'], ['\n2016/0293002', '\nOctober 2016'], ['\n2017/0017848', '\nJanuary 2017'], ['\n2017/0024619', '\nJanuary 2017'], ['\n2017/0124409', '\nMay 2017'], ['\n2017/0217442', '\nAugust 2017'], ['\n2017/0300763', '\nOctober 2017'], ['\n2017/0372161', '\nDecember 2017'], ['\n2018/0165548', '\nJune 2018'], ['\n2018/0211117', '\nJuly 2018']]","[13, '11,356,349', '11,353,577', '11,347,228', '11,233,979', '11,205,319', '11,188,773', '11,184,517', '11,048,265', '11,037,443', '11,003,919', '10,990,835', '10,706,311', '10,642,275']"," What is claimed is:  1.  An artificial intelligence (AI) based system, part of a Roadway Stewardship Network, comprising: a non-transitory storage device having embodied therein one or more
routines operable to detect objects in images using Artificial Neural Networks;  and one or more processors coupled to the non-transitory storage device and operable to execute the one or more routines, wherein the one or more routines include: a
receiver module, which when executed by the one or more processors, receives at least an object detection signal from one or more vision sensing systems, said object detection signal comprises or is accompanied by said images or series of images or a
video associated with said objects;  a detector module, which when executed by the one or more processors, determines, for said objects, a region of interest (ROI) selected from the received images or a series of images or a video;  a training module
which takes manually classified objects, obtained from the images or a series of images or a video and trains the Neural Network to improve the detector performance;  a logic module which takes as input detected objects of interest in a series of one or
more images or videos and determines various actions or events of interest, wherein the various actions or events of interest are a location, a position, a movement, and a category associated with said objects contained in the one or more images or
videos based on comparison with accumulated training data.
 2.  The AI based system of claim 1, wherein said system is a module added to the Stewardship network to act as a Specialist or an Auditor or a Data Collector.
 3.  The AI based system of claim 1, wherein the detector module, also called a detection module, which when executed by the one or more processors, recognizes a region of interest in an image and classifies it as a vehicle or a part of a vehicle
or a person or static/dynamic markings or a policeman.
 4.  The AI based system of claim 3, wherein the region of interest (ROI) is a vehicle or at least a part of the vehicle, in a series of images along with at least one another ROIs selected from of road signs, road markings, policeman, person, in
the same series of images is sufficient to determine a violation of motor vehicle laws/guidelines.
 5.  The AI based system of claim 4, wherein the region of interest (ROI) is a vehicle or at least a part of the vehicle in a single image along with at least one another ROIs selected from road signs, road markings, policeman, person, in the
same image is sufficient to determine a violation of motor vehicle laws/guidelines.
 6.  The AI based system of claim 5, wherein the region of interest (ROI) is at least a vehicle or at least a part of the vehicle or at least the driver of the vehicle in a single image is sufficient to determine a violation of motor vehicle
laws/guidelines.
 7.  The AI based system of claim 6, wherein the AI based system further comprising an annotation module, which when executed by the one or more processors, configured to read license plates detected by the detector module.
 8.  The AI based system of claim 7, wherein the AI based system further comprising an occlusion module, which when executed by the one or more processors, hides objects ROI or hides non-ROI information in the image or series of images or video.
 9.  The AI based system of claim 3, wherein the detection module, which when executed by the one or more processors, is further configured to detect traffic violations based on the orientation and position of the vehicle or wherein the detection
module further takes into account the position, orientation and location of other vehicles, relative to the vehicle of interest to determine traffic violations.
 10.  The AI based system of claim 9, wherein the AI based system further comprising: a calculation module, which when executed by the one or more processors, configured to calculate a speed of said object utilizing object detection from the
location of the said object in different frames of the video or series of images and the relative velocity of the vehicle.
 11.  The AI based system of claim 10, wherein the AI based system further comprising: an allocation module, which when executed by the one or more processors, configured to allocate, one or more parking slot options, based on detection of said
parking slots or by matching a host of a parking spot to the driver in need of a parking spot.
 12.  A method for detecting objects in images and videos using at least one or more artificial neural networks, the method comprising: receiving, by a system, at least an object detection signal from one or more vision sensing systems, said
object detection signal comprises said images associated with said objects;  determining by the system, a Region Of Interest (ROI) selected from the received images for said objects;  detecting by the system, a location, a position, a movement, and a
category associated with said objects contained in the received images, using the artificial neural networks which are trained using accumulated training data, wherein the accumulated training data is obtained from specialists by having them perform
among activities involving: identifying and annotating license plates of vehicles, identifying the 2D boundary around vehicles, identifying the types, make, model and other information about the vehicles, identifying various roadway violations committed
by the vehicles, identifying roadway markings and signs, identifying and marking ROI for objects of interest.
 13.  The method of claim 12, wherein a data collector, configured to obtain the images and videos, has at least one sensor, visually capturing the direction(s) of gaze of person(s) inside the vehicle and at least one sensor capturing the
direction(s) of visual marking(s) outside the vehicle;  wherein when, the general direction(s) of gaze and visual marking(s) match, provides a general confirmation of the visual markings having been witnessed.
 14.  The method of claim 13, wherein the visual markings collected by the data collector are given a score based on number of persons who witness the markings, and wherein the scores are used to qualify the efficacy of placement of the visual
marking(s) and the efficacy of the content(s) of the visual marking(s).
 15.  The method of claim 14, wherein a speed of the vehicle inside the ROI is measured from objects in images and videos collected by the data collector by checking the centroid or median or mean location of a license plates relative to the data
collector.
 16.  The method of claim 15, wherein a Stewardship network, in which the specialists and auditors who review a video of series of images collected by the data collectors, earn points based on a count and accuracy of the detection of traffic
violations, wherein, the collection of points emulates a game.
 17.  The method of claim 16, wherein the detection and identification of the system is supported by re-training the neural networks by adding the reviewed data to the accumulated training data.  "
"10,298,743","
     May 21, 2019
","Mobile terminal
"," A mobile terminal includes a wireless communication unit configured to
     perform wireless communication with an artificial intelligence device, an
     artificial intelligence unit configured to recognize a loss state of the
     mobile terminal if information included in a signal received from the
     artificial intelligence device indicates the loss state of the mobile
     terminal and to generate a control signal for switching an operation mode
     of the mobile terminal according to the recognized loss state, and a
     controller configured to set the operation mode of the mobile terminal to
     a loss mode for restricting use of the mobile terminal according to the
     generated control signal.
",H04L 67/535 (20220501); H04W 48/04 (20130101); G06F 21/316 (20130101); G06N 3/08 (20130101); H04W 12/30 (20210101); G06F 21/88 (20130101); H04M 3/54 (20130101); H04W 4/029 (20180201); H04W 8/245 (20130101); H04W 88/02 (20130101); H04W 12/08 (20130101); H04W 12/126 (20210101); G10L 17/00 (20130101); H04M 1/72457 (20210101); H04W 4/50 (20180201); G06V 40/172 (20220101); H04W 52/0209 (20130101); H04M 1/72463 (20210101); H04W 12/63 (20210101); H04W 4/02 (20130101); G06N 5/022 (20130101); H04W 12/33 (20210101); Y02D 30/70 (20200801); G06N 20/00 (20190101); G06F 2221/2111 (20130101); G06N 3/126 (20130101),H04W 4/50 (20180101); G06K 9/00 (20060101); G10L 17/00 (20130101); H04M 3/54 (20060101); H04W 4/02 (20180101); H04W 8/24 (20090101); H04W 12/08 (20090101); H04W 52/02 (20090101); G06F 21/88 (20130101); H04W 48/04 (20090101); H04W 88/02 (20090101); G06F 21/31 (20130101); H04M 1/725 (20060101); G06N 99/00 (20190101),"[['\n7783281', '\nAugust 2010'], ['\n8864847', '\nOctober 2014'], ['\n2004/0180673', '\nSeptember 2004'], ['\n2011/0047033', '\nFebruary 2011'], ['\n2012/0252411', '\nOctober 2012'], ['\n2014/0057597', '\nFebruary 2014'], ['\n2015/0207917', '\nJuly 2015'], ['\n2015/0235058', '\nAugust 2015'], ['\n2016/0343235', '\nNovember 2016']]",[1]," What is claims is:  1.  A mobile terminal comprising: a display;  a camera;  a wireless communication unit configured to perform wireless communication with an external device and comprising a
position information module configured to acquire a position of the mobile terminal;  and a controller configured to: receive a signal from the external device via the wireless communication unit;  recognize a loss state of the mobile terminal based on
recognized content of the received signal;  generate a control signal for switching an operation mode of the mobile terminal according to the recognized loss state;  set the operation mode of the mobile terminal to a loss state mode for restricting use
of the mobile terminal according to the generated control signal;  cause the display to display a power off screen indicating that the mobile terminal is being turned off according to a command to turn off the mobile terminal while the mobile terminal is
in the loss state mode;  change a power mode of the mobile terminal to a power saving mode after displaying the power off screen;  according to the command and after displaying the power off screen, capture an image of a finder of the mobile terminal
while the mobile terminal is in the loss state mode;  transmit the captured image of the finder and position information of the mobile terminal to the external device;  receive, from the external device, a remote control command for enabling the external
device to remotely control operation of the mobile terminal, wherein the remote control command comprises a request for incoming messages received from another terminal;  authenticate identification information of the external device included in the
remote control command based on pre-registered identification information;  when authentication is successful and in response to the remote control command, extract question messages which include questions from the incoming messages received from the
another terminal and transmit the extracted question messages to the external device while the mobile terminal is in the loss state mode;  receive one or more response messages from the external device responding to the extracted question messages;  and
transmit the one or more response messages to the another terminal.
 2.  The mobile terminal according to claim 1, wherein the received signal comprises a text message and the recognized content corresponds to text indicating the loss state.
 3.  The mobile terminal according to claim 1, wherein the received signal comprises voice information and the recognized content corresponds to recognized speech information indicating the loss state.
 4.  The mobile terminal according to claim 3, wherein the controller is further configured to recognize a voice of the voice information based on pre-stored voice data and the recognized content further comprises recognizing the voice of the
voice information.
 5.  The mobile terminal according to claim 1, wherein the loss state mode corresponds to setting the mobile terminal to a locked state, restricting data communication of the mobile terminal, or setting the mobile terminal to a power saving mode.
 6.  The mobile terminal according to claim 1, wherein the controller is further configured to transmit time information to the external device corresponding to when the loss state is determined.
 7.  The mobile terminal according to claim 6, wherein the controller is further configured to transmit the time information to the external device associated with a person that is most frequently in communication with the mobile terminal.
 8.  The mobile terminal according to claim 1, wherein the controller is further configured to forward an incoming call to the external device while the mobile terminal is in the loss state mode.
 9.  The mobile terminal according to claim 8, wherein: the received signal comprises a message and the loss state is determined based on an indication in the message indicating the loss state;  and the external device is designated by the
message which comprises contact information of the external device.
 10.  A mobile terminal comprising: a display;  a camera;  a wireless communication unit configured to perform wireless communication with an external device and comprising a position information module configured to acquire a position of the
mobile terminal;  and a controller configured to: collect context information of the mobile terminal, recognize a loss state of the mobile terminal based on the collected context information;  and set the operation mode of the mobile terminal to a loss
state mode for restricting use of the mobile terminal according to the collected context information;  cause the display to display a power off screen indicating that the mobile terminal is being turned off according to a command to turn off the mobile
terminal while the mobile terminal is in the loss state mode;  change a power mode of the mobile terminal to a power saving mode after displaying the power off screen;  according to the command and after displaying the power off screen, capture an image
of a finder of the mobile terminal while the mobile terminal is in the loss state mode;  transmit the captured image of the finder and position information of the mobile terminal to the external device;  receive, from the external device, a remote
control command for enabling the external device to remotely control operation of the mobile terminal, wherein the remote control command comprises a request for incoming messages received from another terminal;  authenticate identification information
of the external device included in the remote control command based on pre-registered identification information;  when authentication is successful and in response to the remote control command, extract question messages which include questions from the
incoming messages received from the another terminal and transmit the extracted question messages to the external device while the mobile terminal is in the loss state mode;  receive one or more response messages from the external device responding to
the extracted question messages;  and transmit the one or more response messages to the another terminal.
 11.  The mobile terminal according to claim 10, wherein the context information comprises a location pattern of the mobile terminal in combination with user information of the mobile terminal.
 12.  The mobile terminal according to claim 11, wherein the controller is further configured to determine the loss state of the mobile terminal when the location pattern of the mobile terminal does not match a pre-stored location pattern and the
user information comprising an image of a user's face and voice input of the user is not received for a predetermined time period.
 13.  The mobile terminal according to claim 12, wherein the controller is further configured to cause the display to turn off when the loss state mode is determined and set the mobile terminal to a power saving mode.
 14.  The mobile terminal according to claim 10, wherein the controller is further configured to forward an incoming call to another designated terminal while the mobile terminal is in the loss state mode.
 15.  The mobile terminal according to claim 14, wherein: the collected context information comprises a received message and the loss state is determined based on an indication in the received message indicating the loss state;  and the another
designated terminal is designated by the received message which comprises contact information of the another designated terminal.  "
"10,302,398","
     May 28, 2019
","Vehicle based independent range system (VBIRS)
"," A Vehicle Based Independent Range System (VBIRS) (10) comprised of
     individual stacked chambered modules that function as a single integrated
     system that provides a self-contained space based range capability, and
     is comprised of a power module (12), an artificial
     intelligence/autonomous engagement/flight termination system module (20),
     a satellite data modem module system (30) and a navigation,
     communications and control module system (40), all of which interface
     with a VBIRS test and checkout system (52) and a weather data system
     (116). The artificial intelligence/autonomous engagement/flight
     termination system module (20) is comprised of an inherent artificial
     intelligence capability that envelopes and interchanges data with an
     autonomous engagement controller (22) that contains all missile/rocket
     autonomous cooperative engagement, destruct decision software and range
     safety algorithm parameters required for optimum mission planning. VBIRS
     employed aboard an aircraft or between any combination of launching
     systems allows that aircraft to launch a missile/rocket from any location
     on earth, whether the missile/rocket is singularly launched by itself or
     as a larger group of missiles/rockets launched in a salvo arrangement,
     while providing collaborative real-time targeting to occur directly
     between missiles/rockets in conjunction with other missile/rocket launch
     platforms or stand-alone mission control centers.
",F41G 7/008 (20130101); F41G 9/00 (20130101); B64G 1/1014 (20130101); G01S 5/0036 (20130101); F41G 7/346 (20130101); F41G 3/04 (20130101); F41G 7/224 (20130101); F41G 7/006 (20130101); B64C 39/024 (20130101); B64G 1/002 (20130101); F41G 7/007 (20130101); B64G 1/242 (20130101); F41G 7/36 (20130101); B64C 2201/14 (20130101); B64G 2001/247 (20130101),F41G 7/00 (20060101); F41G 9/00 (20060101); F41G 7/36 (20060101); B64G 1/24 (20060101); G01S 5/00 (20060101); B64G 1/00 (20060101); B64G 1/10 (20060101); F41G 7/22 (20060101); F41G 3/04 (20060101); F41G 7/34 (20060101),"[['\n5511218', '\nApril 1996'], ['\n5739787', '\nApril 1998'], ['\n5866837', '\nFebruary 1999'], ['\n6167263', '\nDecember 2000'], ['\n6196496', '\nMarch 2001'], ['\n7394047', '\nJuly 2008'], ['\n7494089', '\nFebruary 2009'], ['\n7494090', '\nFebruary 2009'], ['\n7552669', '\nJune 2009'], ['\n7612284', '\nNovember 2009'], ['\n7809370', '\nOctober 2010'], ['\n8084724', '\nDecember 2011'], ['\n8487226', '\nJuly 2013'], ['\n8678321', '\nMarch 2014'], ['\n8748787', '\nJune 2014'], ['\n2004/0007121', '\nJanuary 2004'], ['\n2014/0067164', '\nMarch 2014']]",[0]," We claim:  1.  A Vehicle Based Independent Range System (VBIRS) operating completely independent of ground based tracking, telemetry and command transmit range assets is comprised of the
following individually stacked faraday cage modules systems collectively forming a single integrated deployable system for aircraft, missiles, rockets, unmanned aerial vehicles and any other desired vehicle to provide a self-contained space based range
capability: i. a power module system comprised of a charging access means a battery and a dc-dc converter means with an electromagnetic interference filter means, providing power in proportions as required to the following modules, ii.  an artificial
intelligence and autonomous engagement flight termination module system comprised of an autonomous engagement controller means containing all destruct and engagement decision making software and range safety algorithm parameters required to make destruct
or weapon engagement decisions, communicating with a vehicle propulsion controller means, a vehicle component separation system means, an ordnance means and vehicle destruct system means, iii.  a satellite data module system comprised of a satellite data
modem means, a satellite uplink controller means and a satellite downlink controller means, iv.  a navigation, communications and control module system comprised of a radiation tolerant data processing system means, a communications system means, an
inertial navigation system, a global positioning system antenna system, a global positioning system antenna combiner, and a global positioning system receiver, said navigation, communications and control module system interfaces with a VBIRS test and
checkout system means and a weather data system means, said artificial intelligence and autonomous engagement flight termination module system comprised of an inherent artificial intelligence means that envelopes and interchanges data with an autonomous
engagement controller means, containing all vehicle engagement and destruct decision software and range safety algorithm parameters required for mission planning to make autonomous destruct and weapon engagement decisions for any vehicle at any juncture
from checkout through flight, said artificial intelligence and autonomous engagement flight termination module system interfaces to said vehicle propulsion controller means to selectively control and also terminate vehicle thrust, said vehicle ordnance
and destruct subsystem means to autonomously destroy any vehicle if it becomes a hazard, and said vehicle component separation system means that separates vehicle stages and components as dictated by mission requirements, said artificial intelligence and
autonomous engagement flight termination module system engages with said radiation tolerant central processing means, and a communications system means at the heart of said navigation, communications and control module system, said radiation tolerant
data processing means and communications system means receives direct input from a GPS receiver and an inertial navigation system that ingests GPS data via a GPS antenna system and a GPS antenna signal combiner, said radiation tolerant data processing
system means and said communications system means directly interfaces with said satellite data modem means, communicating with any required communication satellite via said satellite uplink antenna controller means and said satellite downlink antenna
controller means providing control of the antenna system communicating with satellites, whereby said satellite data modem means solves all complex Doppler Shift dynamics that naturally occur at high vehicle velocities and accelerations, in order to
provide a sufficient communications link capability to meet any mission requirements, said navigation, communications and control module system simultaneously communicates with said satellite data module system and said artificial intelligence and
autonomous engagement flight termination module system enabling a salvo launch capability in any sequence amongst any flight vehicles to occur autonomously within a collaborative environment whereby all flight vehicles communicate with each other via a
satellite cross-link communication path from pre-launch through mission completion enabling optimized autonomous vehicle on target allocations to be prioritized amongst all vehicles, while giving the option to engage with human intervention, whereby said
VBIRS enables a collaborative weapon and salvo launch capability in any sequence, while simultaneously operating completely independent of ground based tracking, telemetry and command transmit range assets.
 2.  A Vehicle Based Independent Range System (VBIRS) of claim 1 whereby said VBIRS is operationally installed aboard an aircraft launching system that launches a vehicle from any location on earth with the pilot locally in control of that
vehicle launch, in addition to that functional launch capability being accomplished from anywhere else on earth whether the vehicle is singularly launched alone versus being part of a larger group of vehicles being launched within any theatre in any
sequential arrangement, all in tactical communication with each other via said VBIRS aboard all aircraft launching systems and all vehicles undergoing launch with all vehicles and aircraft launching systems employing an arrangement of collaborative
engagement algorithms means resident within said artificial intelligence and autonomous engagement flight termination module system in order to communicate tactical data between the aircraft launching system and the vehicles being launched.
 3.  A Vehicle Based Independent Range System (VBIRS) of claim 2 whereby said VBIRS is operationally and simultaneously employed on any combination of said air launch platforms and any number of ground launch vehicle launch sites.
 4.  A Vehicle Based Independent Range System (VBIRS) of claim 3 whereby a VBIRS equipped UAV launch platform operates within the same defined theatre that simultaneously communicates with said orbiting satellite cross-link communication path to
participate in said salvo arrangement as part of the same mission.  "
"10,306,059","
     May 28, 2019
","System and method for artificial intelligence on hold call handling
"," Systems and methods of on hold handling of calls are provided. A call may
     be received, from a caller, at a public safety access point. An
     Artificial Intelligence (AI) bot may monitor a context associated with
     the call. The call may be placed on hold. The AI bot may determine, based
     on the context associated with the call, that the call should be taken
     off hold.
",H04M 3/5166 (20130101); G06Q 10/00 (20130101); H04M 3/4283 (20130101); G06Q 10/10 (20130101); G06N 20/00 (20190101); H04M 3/4285 (20130101); H04M 3/5116 (20130101); H04M 2242/04 (20130101); H04M 2201/40 (20130101); H04M 2203/2038 (20130101); G06N 3/006 (20130101),H04M 3/51 (20060101); H04M 3/428 (20060101); G06N 20/00 (20190101),"[['\n4834551', '\nMay 1989'], ['\n7095852', '\nAugust 2006'], ['\n7248683', '\nJuly 2007'], ['\n9438730', '\nSeptember 2016'], ['\n2003/0037146', '\nFebruary 2003'], ['\n2008/0031426', '\nFebruary 2008'], ['\n2010/0124900', '\nMay 2010'], ['\n2013/0019321', '\nJanuary 2013'], ['\n2014/0140494', '\nMay 2014'], ['\n2016/0379470', '\nDecember 2016'], ['\n2017/0289350', '\nOctober 2017']]","[6, '11,223,723', '11,158,004', '11,153,435', '11,122,099', '10,805,458', '10,742,809']"," We claim:  1.  A method of on hold handling of calls comprising: receiving a call, from a caller, at a public safety access point;  monitoring, with an Artificial Intelligence (AI) bot
implemented by a processor, a context associated with the call;  placing the call on hold;  determining, by the AI bot, based on the context associated with the call, that the call should be taken off hold;  providing a recommendation, by the AI bot, to
a call taker that the call should be taken off hold, wherein the recommendation includes a reason why the call should be taken off hold;  taking the call off hold when the recommendation is accepted;  and providing an indication that the recommendation
was correct or incorrect based on when the recommendation is accepted, wherein the AI bot updates historical data based on the indication in order to actively learn from the accuracy of the recommendation.
 2.  The method of claim 1 wherein monitoring the context associated with the call further comprises: monitoring incoming audio from the caller;  and detecting a trigger in the incoming audio from the call.
 3.  The method of claim 1 wherein monitoring the context associated with the call further comprises: monitoring communications associated with the public safety access point;  and detecting a trigger within the public safety access point
associated communications.
 4.  The method of claim 1 wherein monitoring the context associated with the call further comprises: monitoring external information sources;  and detecting a trigger in the external information sources.
 5.  The method of claim 1 further comprising: taking the call off hold wherein taking the call off hold includes connecting the caller to a call taker.
 6.  The method of claim 1 further comprising: taking the call off hold wherein taking the call off hold includes communicating, via the AI bot, information to the caller.
 7.  The method of claim 1 further comprising: obtaining, by the AI bot, pertinent information from the caller while the call is on hold;  and summarizing the pertinent information by the AI bot.
 8.  The method of claim 7 further comprising: providing the summarization to a call taker while the caller is on hold.
 9.  A non-transitory processor readable medium containing a set of processor executable instructions thereon that when executed by a processor cause the processor to: receive a call, from a caller, at a public safety access point;  monitor, with
an Artificial Intelligence (AI) bot, a context associated with the call;  place the call on hold;  determine, by the AI bot, based on the context associated with the call, that the call should be taken off hold;  provide a recommendation, by the AI bot,
to a call taker that the call should be taken off hold, wherein the recommendation includes a reason why the call should be taken off hold;  take the call off hold when the recommendation is accepted;  and provide an indication that the recommendation
was correct or incorrect based on when the recommendation is accepted, wherein the AI bot updates historical data based on the indication in order to actively learn from the accuracy of the recommendation.
 10.  The medium of claim 9 wherein instructions to monitor the context associated with the call further comprises instructions to: monitor incoming audio from the caller;  and detect a trigger in the incoming audio from the call.
 11.  The medium of claim 9 wherein instructions to monitor the context associated with the call further comprises instructions to: monitor communications associated with the public safety access point;  and detect a trigger within the public
safety access point associated communications.
 12.  The medium of claim 9 wherein instructions to monitor the context associated with the call further comprises instructions to: monitor external information sources;  and detect a trigger in the external information sources.
 13.  The medium of claim 9 further comprising instructions to: take the call off hold wherein taking the call off hold includes connecting the caller to a call taker.
 14.  The medium of claim 9 further comprising instructions to: take the call off hold wherein taking the call off hold includes communicating, via the AI bot, information to the caller.
 15.  The medium of claim 9 further comprising instructions to: obtain, by the AI bot, pertinent information from the caller while the call is on hold;  and summarize the pertinent information by the AI bot.
 16.  The medium of claim 15 further comprising instructions to: provide the summarization to a call taker while the caller is on hold.
 17.  A system comprising: a processor;  and a memory coupled to the processor, the memory containing a set of instructions that when executed by the processor cause the processor to: receive a call, from a caller, at a public safety access
point;  monitor, with an Artificial Intelligence (AI) bot, a context associated with the call;  place the call on hold;  determine, by the AI bot, based on the context associated with the call, that the call should be taken off hold;  provide a
recommendation, by the AI bot, to a call taker that the call should be taken off hold, wherein the recommendation includes a reason why the call should be taken off hold;  take the call off hold when the recommendation is accepted;  and provide an
indication that the recommendation was correct or incorrect based on when the recommendation is accepted, wherein the AI bot updates historical data based on the indication in order to actively learn from the accuracy of the recommendation. 
"
"10,311,861","
     June 4, 2019
","System and method for encoding data in a voice recognition integrated
     circuit solution
"," Methods of encoding voice data for loading into an artificial
     intelligence (AI) integrated circuit are provided. The AI integrated
     circuit may have an embedded cellular neural network for implementing AI
     tasks based on the loaded voice data. An encoding method may generate a
     two-dimensional (2D) frequency-time array from an audio waveform, apply a
     probability function to the 2D frequency-time array to generate a set of
     2D arrays, load the set of 2D arrays into the AI integrated circuit,
     execute programming instructions contained in the AI integrated circuit
     to feed the set of 2D arrays into the embedded cellular neural network in
     the AI integrated circuit to generate a voice recognition result, and
     output the voice recognition result. The encoding method also trains a
     convolution neural network (CNN) and loads the weights of the CNN into
     the AI integrated circuit for implementing the AI tasks.
",G10L 15/063 (20130101); G06N 3/08 (20130101); G10L 15/02 (20130101); G10L 15/22 (20130101); G10L 15/16 (20130101); G06N 3/0454 (20130101); G10L 25/18 (20130101); G10L 17/00 (20130101),G10L 15/16 (20060101); G10L 15/06 (20130101); G06N 3/08 (20060101); G10L 25/18 (20130101); G10L 15/22 (20060101),"[['\n2016/0217367', '\nJuly 2016'], ['\n2018/0046913', '\nFebruary 2018'], ['\n2018/0190268', '\nJuly 2018'], ['\n2018/0315399', '\nNovember 2018']]",[0]," We claim:  1.  A method of encoding voice data for loading into an artificial intelligence (AI) integrated circuit, the method comprising: receiving, by a processor, voice data comprising at
least a segment of an audio waveform;  generating, by the processor, a two-dimensional (2D) frequency-time array comprising a plurality of pixels, each pixel having a value that represents an audio intensity of the segment of the audio waveform at a time
in the segment and a frequency in the audio waveform;  applying, by the processor, a probability function to the 2D frequency-time array to generate a set of 2D arrays comprising a number of 2D arrays, each 2D array having a plurality of pixels, each
pixel having a value and corresponding to one of the plurality of pixels in the 2D frequency-time array, wherein: the probability function comprises a plurality of entries representing a range of intensity values of the audio waveform, and for each
entry, a probability value for each of a number of sub-functions, each sub-function representing a sub-intensity band of the audio waveform, the number of 2D arrays in the set of 2D arrays is the number of sub-functions in the probability function, and
generating the set of 2D arrays comprises, for each pixel in the 2D frequency-time array, applying the probability function to a value of each pixel to determine values of corresponding pixels in the set of 2D arrays;  loading the set of 2D arrays into
the AI integrated circuit;  by the AI integrated circuit, executing one or more programming instructions contained in the AI integrated circuit to feed the set of 2D arrays into an embedded cellular neural network architecture in the AI integrated
circuit;  generating a voice recognition result from the cellular neural network architecture based on the set of 2D arrays;  and outputting the voice recognition result.
 2.  The method of claim 1, further comprising: receiving a set of sample training voice data comprising at least one sample segment of an audio waveform;  using the set of sample training voice data to generate one or more sample 2D
frequency-time arrays each comprising a plurality of pixels, each pixel having a value that represents an audio intensity of the sample segment of the audio waveform at a time in the sample segment and a frequency in the audio waveform;  applying the
probability function to each of the one or more sample 2D frequency-time arrays to generate a set of 2D training arrays, each 2D training array having a plurality of pixels, each pixel having a value and corresponding to one of the plurality of pixels in
each sample 2D frequency-time array;  using the set of 2D training arrays to train one or more weights of a convolutional neural network;  and loading the one or more trained weights into the embedded cellular neural network architecture of the AI
integrated circuit.
 3.  The method of claim 1, wherein the 2D frequency-time array is a 2D spectrogram.
 4.  The method of claim 1, further comprising scaling the values of each pixel in the set of 2D arrays to an integer within a range of a depth of each channel in the AI integrated circuit.
 5.  A method of encoding voice data for loading into an artificial intelligence (AI) integrated circuit, the method comprising: receiving, by a processor, voice data comprising at least a segment of an audio waveform;  generating, by the
processor, a two-dimensional (2D) frequency-time array comprising a plurality of pixels, each pixel having a value that represents an audio intensity of the segment of the audio waveform at a time in the segment and a frequency in the audio waveform; 
applying, by the processor, a probability function to the 2D frequency-time array to generate a set of 2D arrays comprising a number of 2D arrays, each 2D array having a plurality of pixels, each pixel having a value and corresponding to one of the
plurality of pixels in the 2D frequency-time array, wherein: the probability function comprises a plurality of entries representing a range of frequency values of the audio waveform, and for each entry, a probability value for each of a plurality of
sub-functions, each sub-function representing a sub-frequency band of the audio waveform, the number of 2D arrays in the set of 2D arrays is the number of sub-functions in the probability function, and generating the set of 2D arrays comprises: for each
pixel in the 2D frequency-time, applying the probability function to a value of each pixel to determine values of corresponding pixels in the set of 2D arrays;  loading the set of 2D arrays into the AI integrated circuit;  by the AI integrated circuit,
executing one or more programming instructions contained in the AI integrated circuit to feed the set of 2D arrays into an embedded cellular neural network architecture in the AI integrated circuit;  generating a voice recognition result from the
cellular neural network architecture based on the set of 2D arrays;  and outputting the voice recognition result.
 6.  The method of claim 5, wherein the 2D frequency-time array is a 2D spectrogram.
 7.  The method of claim 5, further comprising scaling the values of each pixel in the set of 2D arrays to a an integer within a range of a depth of each channel in the AI integrated circuit.
 8.  The method of claim 6, further comprising scaling each value of the pixel in the 2D spectrogram by using a Mel scale formula before generating the set of 2D arrays.
 9.  A system for encoding voice data for loading into an artificial intelligence (AI) integrated circuit, the system comprising: a processor;  and a non-transitory computer readable medium containing programming instructions that, when executed,
will cause the processor to: receive voice data comprising at least a segment of an audio waveform, generate a two-dimensional (2D) frequency-time array comprising a plurality of pixels, each pixel having a value that represents an audio intensity of the
segment of the audio waveform at a time in the segment and a frequency in the audio waveform, apply a probability function to the 2D frequency-time array to generate a set of 2D arrays comprising a number of 2D arrays, each 2D array having a plurality of
pixels, each pixel having a value and corresponding to one of the plurality of pixels in the 2D frequency-time array, wherein: the probability function comprises a plurality of entries representing a range of intensity values of the audio waveform, and
for each entry, a probability value for each of a number of sub-functions, each sub-function representing a sub-intensity band of the audio waveform;  the number of 2D arrays in the set of 2D arrays is the number of sub-functions in the probability
function;  and programming instructions for generating the set of 2D arrays comprise programming instructions configured to, for each pixel in the 2D frequency-time array, apply the probability function to a value of each pixel to determine values of
corresponding pixels in the set of 2D arrays, and load the set of 2D arrays into the AI integrated circuit;  wherein the AI integrated circuit comprises: an embedded cellular neural network architecture, and one or more programming instructions
configured to: feed the set of 2D arrays into the embedded cellular neural network architecture in the AI integrated circuit;  generate a voice recognition result from the cellular neural network architecture based on the set of 2D arrays;  and output
the voice recognition result.
 10.  The system of claim 9, further comprising additional programming instructions configured to: receive a set of sample training voice data comprising at least one sample segment of an audio waveform;  use the set of sample training voice data
to generate one or more sample 2D frequency-time arrays each comprising a plurality of pixels, each pixel having a value that represents an audio intensity of the sample segment of the audio waveform at a time in the sample segment and a frequency in the
audio waveform;  apply the probability function to each of the one or more sample 2D frequency-time arrays to generate a set of 2D training arrays, each 2D training array having a plurality of pixels, each pixel having a value and corresponding to one of
the plurality of pixels in each sample 2D frequency-time array;  use the set of 2D training arrays to train one or more weights of a convolutional neural network;  and load the one or more trained weights into the embedded cellular neural network
architecture of the AI integrated circuit.
 11.  The system of claim 9, wherein the 2D frequency-time array is a 2D spectrogram.
 12.  The system of claim 9, further comprising additional programming instructions configured to scale the values of each pixel in the set of 2D arrays to an integer within a range of a depth of each channel in the AI integrated circuit.
 13.  A system for encoding voice data for loading into an artificial intelligence (AI) integrated circuit, the system comprising: a processor;  and a non-transitory computer readable medium containing programming instructions that, when
executed, will cause the processor to: receive voice data comprising at least a segment of an audio waveform, generate a two-dimensional (2D) frequency-time array comprising a plurality of pixels, each pixel having a value that represents an audio
intensity of the segment of the audio waveform at a time in the segment and a frequency in the audio waveform, apply a probability function to the 2D frequency-time array to generate a set of 2D arrays comprising a number of 2D arrays, each 2D array
having a plurality of pixels, each pixel having a value and corresponding to one of the plurality of pixels in the 2D frequency-time array, wherein: the probability function comprises a plurality of entries representing a range of frequency values of the
audio waveform, and for each entry, a probability value for each of a plurality of sub-functions, each sub-function representing a sub-frequency band of the audio waveform;  the number of 2D arrays in the set of 2D arrays is the number of sub-functions
in the probability function;  and programming instructions for generating the set of 2D arrays comprise programming instructions configured to, for each pixel in the 2D frequency-time, apply the probability function to a value of each pixel to determine
values of corresponding pixels in the set of 2D arrays, and load the set of 2D arrays into the AI integrated circuit;  wherein the AI integrated circuit comprises: an embedded cellular neural network architecture, and one or more programming instructions
configured to: feed the set of 2D arrays into the embedded cellular neural network architecture in the AI integrated circuit;  generate a voice recognition result from the cellular neural network architecture based on the set of 2D arrays;  and output
the voice recognition result.
 14.  The system of claim 13, wherein the 2D frequency-time array is a 2D spectrogram.
 15.  The system of claim 13, further comprising additional programming instructions configured to scale the values of each pixel in the set of 2D arrays to a an integer within a range of a depth of each channel in the AI integrated circuit.
 16.  The system of claim 14, further comprising additional programming instructions configured to scale each value of the pixel in the 2D spectrogram by using a Mel scale formula before generating the set of 2D arrays. 
"
"10,311,877","
     June 4, 2019
","Performing tasks and returning audio and visual answers based on voice
     command
"," An artificial intelligence voice interactive system may provide various
     services to a user in response to a voice command by providing an
     interface between the system and a legacy system to enable providing
     various types of existing services in response to user speech without
     modifying systems for the existing services. Such system includes a
     central server, and the central server may perform operations of
     registering a plurality of service servers at the central server and
     storing registration information of each service server, analyzing voice
     command data from the user device and determining at least one task and
     corresponding service servers based on the analysis results, generating
     an instruction message based on the voice command data, the determined at
     least one task, and the registration information of the selected service
     servers, and transmitting the generated instruction message to the
     selected service servers, and receiving task results including audio and
     video data from the selected service servers and outputting the task
     results through at least one device associated with the user device.
",G10L 15/22 (20130101); G10L 15/08 (20130101); G10L 15/30 (20130101); G06F 3/167 (20130101); G10L 2015/088 (20130101); G10L 2015/223 (20130101),G10L 15/30 (20130101); G10L 15/08 (20060101); G10L 15/22 (20060101),"[['\n5777614', '\nJuly 1998'], ['\n6570555', '\nMay 2003'], ['\n6570588', '\nMay 2003'], ['\n7426467', '\nSeptember 2008'], ['\n9199122', '\nDecember 2015'], ['\n9691221', '\nJune 2017'], ['\n2002/0111794', '\nAugust 2002'], ['\n2003/0220799', '\nNovember 2003'], ['\n2007/0120959', '\nMay 2007'], ['\n2008/0183678', '\nJuly 2008'], ['\n2010/0153868', '\nJune 2010'], ['\n2011/0201414', '\nAugust 2011'], ['\n2011/0288868', '\nNovember 2011'], ['\n2012/0022874', '\nJanuary 2012'], ['\n2012/0278076', '\nNovember 2012'], ['\n2013/0035790', '\nFebruary 2013'], ['\n2013/0325466', '\nDecember 2013'], ['\n2014/0143809', '\nMay 2014'], ['\n2014/0278403', '\nSeptember 2014'], ['\n2015/0217199', '\nAugust 2015'], ['\n2015/0256873', '\nSeptember 2015'], ['\n2015/0290795', '\nOctober 2015'], ['\n2015/0379752', '\nDecember 2015'], ['\n2016/0034458', '\nFebruary 2016'], ['\n2016/0140960', '\nMay 2016'], ['\n2016/0174074', '\nJune 2016'], ['\n2016/0210602', '\nJuly 2016'], ['\n2016/0328014', '\nNovember 2016'], ['\n2017/0206797', '\nJuly 2017'], ['\n2017/0264954', '\nSeptember 2017'], ['\n2018/0018793', '\nJanuary 2018'], ['\n2018/0019000', '\nJanuary 2018'], ['\n2018/0054612', '\nFebruary 2018'], ['\n2018/0067381', '\nMarch 2018']]",[1]," What is claimed is:  1.  A method for providing voice interactive services to a user through an artificial intelligence voice interactive system including a central server and a plurality of
service servers each associated with a legacy system, the method comprising: performing a registration operation in response to a predetermined event to register the plurality of service servers and store the registered information of each of the
plurality of service servers;  receiving command data, determining a task to perform based on the command data, and selecting at least one of the plurality of service servers based on the determined task;  generating an instruction message based on the
command data, the determined task, and the registered information of the selected at least one of the plurality of service servers, and transmitting the generated instruction message to the selected at least one of the plurality of service servers;  and
receiving a response message from the selected at least one of the plurality of service servers and outputting at least one of audio and video data based on the response message.
 2.  The method of claim 1, wherein: the registering comprises: storing task essential information for performing a task for each of the plurality of service servers;  and the method further comprises: requesting additional information to a user
device when the command data does not include task essential information for performing the determined task associated with the command data.
 3.  The method of claim 1, wherein the predetermined event includes i) receiving a request message from at least one of the plurality of service servers, ii) receiving a request message from a user device, iii) requesting the at least one of the
plurality of service servers to transmit registration information, and iv) regularly collecting the registration information from the at least one of the plurality of service servers.
 4.  The method of claim 1, wherein the registration information of each one of the plurality of service servers include at least one of information on tasks to perform, service identification words, a communication address, and message formats.
 5.  The method of claim 1, wherein the instruction message is generated based on a message format based on the registration information of the selected at least one of the plurality of service severs in order to cooperate with a non-voice
interactive system to perform the determined task.
 6.  The method of claim 1, wherein determining whether the received command data is related to an internal task or an external task based on service identification words included in the received command data.
 7.  The method of claim 6, when the received command data includes at least one of service identification words, the received command data is a request for the external task that requires one of the service servers.
 8.  The method of claim 1, wherein the selecting the at least one of the plurality of service servers comprises: determining whether the command data includes one of service initiation words, service initiation sentences, sentence patterns, and
keywords of the service servers based on the stored registration information of the service servers;  and selecting the at least one of the plurality of service servers mapped to the one included in the voice command data.
 9.  The method of claim 8, wherein the selecting the at least one of the plurality of service servers further comprises: applying a predetermined selection order to select the at least one of the plurality of service servers;  and the
predetermined selection order is to firstly select one associated with the service initiation word, secondly select one associated with the service initiation sentence, thirdly select one associated with the sentence pattern, and lastly select one
associated with the keyword.
 10.  The method of claim 1, wherein the selecting the at least one of the plurality of service servers comprises: determining whether the command data includes one of a service initiation word and a service initiation sentence of the service
servers;  and when the command data include at least one of the service initiation word and the service initiation sentence, selecting a service server mapped to at least one of the service initiation word and the service initiation sentence.
 11.  The method of claim 1, wherein the selecting the at least one of the plurality of service server comprises: when more than two service servers are capable of performing the determined task associated with the command data, selecting at
least one of the two service servers based on a predetermined selection condition.
 12.  The method of claim 11, wherein the selecting the at least one of the plurality of service server further comprises: when one of a keyword and a sentence pattern included in the voice command data is mapped to more than two service servers,
determining that more than two service servers are capable of performing the determined task associated with the voice command.
 13.  The method of claim 11, wherein the predetermined selection conditions is one of i) selecting one assigned with a higher priority than the others among the plurality of service servers, ii) selecting one previously selected more times than
the others among the plurality of service servers, and iii) selecting one set as a default service server.
 14.  A method for providing, by a user device, various non-voice interactive services in response to a voice command from a user, the method comprising: receiving a voice command from the user;  transmitting command data to a central server; 
receiving a result message from at least one of the central server and service servers, wherein each of the service servers is associated with a legacy system;  and outputting at least one of audio and video data based on the result message through the
user device, wherein: the result message is generated by one of the service servers, as a result of performing at least one task included in the command data through the selected service server, and the selected service server and the central server are
communicated through an instruction message which has information necessary for the selected service server to perform the task included in the command data.
 15.  The method of claim 14, wherein the instruction message is generated based on a format enabling communication between the service servers and the central server.  "
"10,318,335","
     June 11, 2019
","Self-managed virtual networks and services
"," Systems and methods described herein include high-availability layers for
     virtualized systems and services. Virtual network elements include an
     infrastructure layer that automatically detects and repairs failures in
     hardware and an operating system for one of the virtual network elements;
     a virtualization layer that automatically detects and repairs failures in
     a hypervisor executing on the hardware and the operating system; a
     virtual machine layer that automatically detects and repairs failures in
     virtual machines managed by the hypervisor; a virtual network function
     layer that automatically detects and repairs failures in virtual network
     functions executed on the virtual machines; and a connection layer that
     automatically detects and repairs failures in connections between the
     virtual network elements. Each layer includes a fix agent, which uses
     artificial intelligence rules to automatically provide an estimated fix
     time interval for a local failure and to repair the local failure, and a
     fault management agent, which diagnoses failures within the corresponding
     layer.
",G06F 11/0793 (20130101); G06F 11/0712 (20130101); G06F 9/45558 (20130101); G06N 5/022 (20130101); G06F 2009/45591 (20130101); G06N 5/045 (20130101); G06F 2009/45595 (20130101),G06F 9/44 (20180101); G06F 9/455 (20180101); G06F 11/07 (20060101); G06N 5/02 (20060101),"[['\n9317222', '\nApril 2016'], ['\n9367453', '\nJune 2016'], ['\n9497039', '\nNovember 2016'], ['\n2007/0242604', '\nOctober 2007'], ['\n2016/0140515', '\nMay 2016'], ['\n2016/0352578', '\nDecember 2016']]","[2, '11,182,187', '10,949,306']"," What is claimed is:  1.  A system comprising: one or more network devices in a network, the network devices comprising one or more memories and one or more processors for hosting virtual network
elements for a self-managed virtualized system, each of the virtual network elements including: an infrastructure layer that automatically detects and repairs failures in hardware and an operating system for one of the virtual network elements;  a
virtualization layer that automatically detects and repairs failures in a hypervisor executing on the hardware and the operating system;  a virtual machine layer that automatically detects and repairs failures in virtual machines managed by the
hypervisor;  a virtual network function layer that automatically detects and repairs failures in virtual network functions executed on the virtual machines;  a connection layer that automatically detects and repairs failures in connections between the
virtual network elements, wherein each of the infrastructure layer, the virtualization layer, the virtual machine layer, the virtual network function layer, and the connection layer include: a fix agent that uses artificial intelligence rules to
automatically provide an estimated fix time interval for a local failure and repair the local failure in a corresponding layer of the infrastructure layer, the virtualization layer, the virtual machine layer, the virtual network function layer, or the
connection layer if the failed component is locally fixable, and a fault management agent that diagnoses failures within the corresponding layer;  and an orchestrator device, the orchestrator device including a processor to: receive failure notifications
from the infrastructure layer, the virtualization layer, the virtual machine layer, the virtual network function layer, and the connection layer, automatically and remotely detect and repair failures that are not fixable by the infrastructure layer, the
virtualization layer, the virtual machine layer, the virtual network function layer, and the connection layer, and provide another failure notification to indicate an estimated fix time interval for the failed component, wherein the estimated fix time
interval is configured to be the summation of the time for failure identification and the time for switching from a failed entity to a healthy standby entity as recovery.
 2.  The system of claim 1, wherein each of the infrastructure layer, the virtualization layer, the virtual machine layer, the virtual network function layer, and the connection layer further include: a configuration and capacity management agent
that manages capacities for the corresponding layer.
 3.  The system of claim 2, wherein each of the infrastructure layer, the virtualization layer, the virtual machine layer, the virtual network function layer, and the connection layer further include: a resource maintenance agent that monitors
hardware resources within the corresponding layer to protect hardware resources during the failure.
 4.  The system of claim 3, wherein each of the infrastructure layer, the virtualization layer, the virtual machine layer, the virtual network function layer, and the connection layer further include: an analytics agent that collects historical
data and provides data correlation for use by the fix agent and the fault management agent.
 5.  The system of claim 1, wherein each of the infrastructure layer, the virtualization layer, the virtual machine layer, the virtual network function layer, the connection layer, and the orchestrator device use a failure message in a common
message format that includes a fix code indicating one of a layer or entity responsible for repairing the failure.
 6.  The system of claim 5, wherein each of the infrastructure layer, the virtualization layer, the virtual machine layer, the virtual network function layer, and the connection layer further include: a local maintenance agent to populate the
failure message to other layers and management devices in the self-managed virtualized system.
 7.  The system of claim 6, wherein the common message format accommodates failure messages for a loss of signal (LOS), an alarm indication signal (AIS), a loss of frame (LOF), a remote defect indication (RDI), operating system issues, virtual
network function issues, and hardware issues.
 8.  The system of claim 1, wherein each of the infrastructure layer, the virtualization layer, the virtual machine layer, the virtual network function layer, and the connection layer include a pair of maintenance entity group end points, one at
each end of a layer boundary, for a particular service.
 9.  A system, comprising: a network interface to communicate with one or more remote systems;  one or more memories to store instructions;  one or more first processors configured to execute instructions in the one or more memories to host a
virtual network element for a self-managed virtualized system, the virtual network element including: an infrastructure layer that automatically detects and repairs failures in hardware and an operating system for the virtual network element;  a
virtualization layer that automatically detects and repairs failures in a hypervisor executing on the hardware and the operating system;  a virtual machine layer that automatically detects and repairs failures in virtual machines managed by the
hypervisor;  a virtual network function layer that automatically detects and repairs failures in virtual network functions executed on the virtual machines;  and a connection layer that automatically detects and repairs failures in connections between
the virtual network elements, wherein each of the infrastructure layer, the virtualization layer, the virtual machine layer, the virtual network function layer, and the connection layer include: a fix agent that uses artificial intelligence rules to
automatically provide an estimated fix time interval for a local failure and repair the local failure in a corresponding layer of the infrastructure layer, the virtualization layer, the virtual machine layer, the virtual network function layer, or the
connection layer if the failed component is locally fixable, and a fault management agent that diagnoses failures within the corresponding layer;  and one or more second processors configured to execute instructions in the one or more memories to:
receive failure notifications from the infrastructure layer, the virtualization layer, the virtual machine layer, the virtual network function layer, and the connection layer, automatically and remotely detect and repair failures that are not fixable by
the infrastructure layer, the virtualization layer, the virtual machine layer, the virtual network function layer, and the connection layer, and provide another failure notification to indicate an estimated fix time interval for the failed component,
wherein the estimated fix time interval is configured to be the summation of the time for failure identification and the time for switching from a failed entity to a healthy standby entity as recovery.
 10.  The system of claim 9, wherein each of the infrastructure layer, the virtualization layer, the virtual machine layer, the virtual network function layer, and the connection layer further include: a configuration and capacity management
agent that manages capacities for the corresponding layer.
 11.  The system of claim 9, wherein each of the infrastructure layer, the virtualization layer, the virtual machine layer, the virtual network function layer, and the connection layer further include: a resource maintenance agent that monitors
hardware resources within the corresponding layer to protect hardware resource during the failure.
 12.  The system of claim 9, wherein each of the infrastructure layer, the virtualization layer, the virtual machine layer, the virtual network function layer, and the connection layer further include: an analytics agent that collects historical
data and provide data correlation for use by the fix agent and fault management agent.
 13.  The system of claim 9, wherein each of the infrastructure layer, the virtualization layer, the virtual machine layer, the virtual network function layer, the connection layer, and the orchestrator device use a failure message in a common
message format that includes a fix code indicating one of a layer or entity responsible for repairing the failure.
 14.  The system of claim 13, wherein each of the infrastructure layer, the virtualization layer, the virtual machine layer, the virtual network function layer, and the connection layer further include: a local maintenance agent to populate a
failure message to other layers and management devices in the self-managed virtualized system.
 15.  The system of claim 9, wherein the common message format accommodates failure messages for a loss of signal (LOS), an alarm indication signal (AIS), a loss of frame (LOF), a remote defect indication (RDI), operating system issues, and
hardware issues.
 16.  A non-transitory computer-readable medium containing instructions executable by at least one processor, the computer-readable medium comprising one or more instructions to: host a virtual network element for a self-managed virtualized
system, the virtual network element including: an infrastructure layer that automatically detects and repairs failures in hardware and an operating system for the virtual network element;  a virtualization layer that automatically detects and repairs
failures in a hypervisor executing on the hardware and the operating system;  a virtual machine layer that automatically detects and repairs failures in virtual machines managed by the hypervisor;  a virtual network function layer that automatically
detects and repairs failures in virtual network functions executed on the virtual machines;  and a connection layer that automatically detects and repairs failures in connections between the virtual network elements, wherein each of the infrastructure
layer, the virtualization layer, the virtual machine layer, the virtual network function layer, and the connection layer include: a fix agent that uses artificial intelligence rules to automatically provide an estimated fix time interval for a local
failure and repair the local failure in a corresponding layer of the infrastructure layer, the virtualization layer, the virtual machine layer, the virtual network function layer, or the connection layer if the failed component is locally fixable, and a
fault management agent that diagnoses failures within the corresponding layer;  and receive failure notifications from the infrastructure layer, the virtualization layer, the virtual machine layer, the virtual network function layer, and the connection
layer;  automatically and remotely detect and repair failures that are not fixable by the infrastructure layer, the virtualization layer, the virtual machine layer, the virtual network function layer, and the connection layer;  and provide another
failure notification to indicate an estimated fix time interval for the failed component, wherein the estimated fix time interval is configured to be the summation of the time for failure identification and the time for switching from a failed entity to
a healthy standby entity as recovery.
 17.  The non-transitory computer-readable medium of claim 16, wherein each of the infrastructure layer, the virtualization layer, the virtual machine layer, the virtual network function layer, and the connection layer include a pair of
maintenance entity group end points, one at each end of a layer boundary, for a particular service.
 18.  The non-transitory computer-readable medium of claim 16, wherein each of the infrastructure layer, the virtualization layer, the virtual machine layer, the virtual network function layer, and the connection layer further include: a local
maintenance agent to populate a failure message to other layers and management devices in the self-managed virtualized system.
 19.  The non-transitory computer-readable medium of claim 18, wherein the local maintenance agent further requests assistance from an orchestrator device when the fix agent is unable to repair the local failure. 
"
"10,318,876","
     June 11, 2019
","Mood detection with intelligence agents
"," Embodiments of the present invention provide systems and methods for
     increasing the quality of interactions between two or more entities.
     These entities are either individuals (e.g., human beings using a
     computer device) or artificial intelligence (AI) agents. The interactions
     between all of the entities within a computing environment are mapped and
     analyzed. Based on the mapped interactions, a relationship model is
     generated in order to run multiple applications within a computing
     environment.
",G06N 3/006 (20130101); G06N 5/022 (20130101); G06N 5/043 (20130101),G06N 5/04 (20060101),"[['\n8565922', '\nOctober 2013'], ['\n8621416', '\nDecember 2013'], ['\n8838505', '\nSeptember 2014'], ['\n8965828', '\nFebruary 2015'], ['\n9098109', '\nAugust 2015'], ['\n9160773', '\nOctober 2015'], ['\n9277375', '\nMarch 2016'], ['\n2014/0288704', '\nSeptember 2014'], ['\n2015/0086949', '\nMarch 2015'], ['\n2015/0189085', '\nJuly 2015'], ['\n2016/0117593', '\nApril 2016'], ['\n2016/0162474', '\nJune 2016'], ['\n2016/0300570', '\nOctober 2016'], ['\n2017/0228520', '\nAugust 2017'], ['\n2018/0075205', '\nMarch 2018'], ['\n2018/0196796', '\nJuly 2018']]",[0]," What is claimed is:  1.  A method comprising: observing, by one or more processors, interactions between a plurality of individuals and a plurality of artificial intelligence (AI) agents; 
creating, by one or more processors, a cognitive profile for each respective individual of the plurality of individuals and each respective AI agent of the plurality of AI agents using social media data, observation protocols, and biometric data for the
plurality of individuals and special testing data for the plurality of AI agents, wherein the observation protocols comprise monitoring conversation tone and facial expressions during a respective interaction, and wherein the special testing data
comprises sensor data, text data, and speech recognition data obtained during a respective interaction;  mapping, by one or more processors, a set of interactions to each cognitive profile for each respective individual of the plurality of individuals
and each respective AI agent of the plurality of AI agents based on the observed interactions between the plurality of individuals and the plurality of artificial intelligence (AI) agents;  and generating, by one or more processors, an action operation
for each respective individual and AI agent based on the cognitive profile for the respective individual and respective AI agent to ameliorate respective interactions between the plurality of individuals and plurality of AI agents.
 2.  The method of claim 1, wherein mapping the set of interactions, comprises: utilizing, by one or more processors, data objects to represent each of the plurality of individuals and a plurality of artificial intelligence (AI) agents;  and
utilizing, by one or more processors, data links to represent connections between two data objects.
 3.  The method of claim 2, further comprising: analyzing, by one or more processors, the mapped set of interactions to gauge a level of positive sentiment and a level of negative sentiment of each interaction within the mapped set of
interactions.
 4.  The method of claim 3, further comprising: creating, by one or more processors, a plurality of relationship models based on the analyzed mapped set of interactions, wherein each relationship model is associated with a single interaction
within the analyzed mapped set of interactions.
 5.  The method of claim 4, further comprising: utilizing, by one or more processors, the plurality of relationship models to implement solutions for each respective individual of the plurality of individuals and each respective AI agent of the
plurality of AI agents based on the created cognitive profile and the mapped set of interactions for each respective individual of the plurality of individuals and each respective AI agent of the plurality of AI agents.
 6.  A computer program product comprising: a computer readable storage medium and program instructions stored on the computer readable storage medium, the program instructions comprising: program instructions to observe interactions between a
plurality of individuals and a plurality of artificial intelligence (AI) agents;  program instructions to create a cognitive profile for each respective individual of the plurality of individuals and each respective AI agent of the plurality of AI agents
using social media data, observation protocols, and biometric data for the plurality of individuals and special testing data for the plurality of AI agents, wherein the observation protocols comprise monitoring conversation tone and facial expressions
during a respective interaction, and wherein the special testing data comprises sensor data, text data, and speech recognition data obtained during a respective interaction;  program instructions to map a set of interactions to each cognitive profile for
each respective individual of the plurality of individuals and each respective AI agent of the plurality of AI agents based on the observed interactions between the plurality of individuals and the plurality of artificial intelligence (AI) agents;  and
program instructions to generate an action operation for each respective individual and AI agent based on the cognitive profile for the respective individual and respective AI agent to ameliorate respective interactions between the plurality of
individuals and plurality of AI agents.
 7.  The computer program product of claim 6, wherein the program instructions to map the set of interactions, comprise: program instructions to utilize data objects to represent each of the plurality of individuals and a plurality of artificial
intelligence (AI) agents;  and program instructions to utilize data links to represent connections between two data objects.
 8.  The computer program product of claim 7, further comprising: program instructions to analyze the mapped set of interactions to gauge a level of positive sentiments and negative sentiments of each interaction within the mapped set of
interactions.
 9.  The computer program product of claim 8, further comprising: program instructions to create a plurality of relationship models based on the analyzed mapped set of interactions, wherein each relationship model corresponds to a single
interaction within the analyzed mapped set of interactions.
 10.  The computer program product of claim 9, further comprising: program instructions to utilize the plurality of relationship models to implement solutions for each respective individual of the plurality of individuals and each respective AI
agent of the plurality of AI agents based on the created cognitive profile and the mapped set of interactions for each respective individual of the plurality of individuals and each respective AI agent of the plurality of AI agents.
 11.  A computer system comprising: one or more computer processors;  one or more computer readable storage media;  program instructions stored on the one or more computer readable storage media for execution by at least one of the one or more
processors, the program instructions comprising: program instructions to observe interactions between a plurality of individuals and a plurality of artificial intelligence (AI) agents;  program instructions to create a cognitive profile for each
respective individual of the plurality of individuals and each respective AI agent of the plurality of AI agents using social media data, observation protocols, and biometric data for the plurality of individuals and special testing data for the
plurality of AI agents, wherein the observation protocols comprise monitoring conversation tone and facial expressions during a respective interaction, and wherein the special testing data comprises sensor data, text data, and speech recognition data
obtained during a respective interaction;  program instructions to map a set of interactions to each cognitive profile for each respective individual of the plurality of individuals and each respective AI agent of the plurality of AI agents based on the
observed interactions between the plurality of individuals and the plurality of artificial intelligence (AI) agents;  and program instructions to generate an action operation for each respective individual and AI agent based on the cognitive profile for
the respective individual and respective AI agent to ameliorate respective interactions between the plurality of individuals and plurality of AI agents.
 12.  The computer system of claim 11, wherein program instructions to output map the set of interactions, comprise: program instructions to utilize data objects to represent the plurality of individuals and a plurality of artificial intelligence
(AI) agents;  and program instructions to utilize data links to represent connections between two data objects.
 13.  The computer system of claim 12, further comprising: program instructions to analyze the mapped set of interactions to gauge a level of positive sentiments and negative sentiments of each interaction within the mapped set of interactions.
 14.  The computer system of claim 13, further comprising: program instructions to create a plurality of relationship models based on the analyzed mapped set of interactions, wherein each relationship model corresponds to a single interaction
within the analyzed mapped set of interactions.  "
"10,319,364","
     June 11, 2019
","Artificial intelligence-based text-to-speech system and method
"," A technique improves training and speech quality of a text-to-speech
     (TTS) system having an artificial intelligence, such as a neural network.
     The TTS system is organized as a front-end subsystem and a back-end
     subsystem. The front-end subsystem is configured to provide analysis and
     conversion of text into input vectors, each having at least a base
     frequency, f.sub.0, a phenome duration, and a phoneme sequence that is
     processed by a signal generation unit of the back-end subsystem. The
     signal generation unit includes the neural network interacting with a
     pre-existing knowledgebase of phenomes to generate audible speech from
     the input vectors. The technique applies an error signal from the neural
     network to correct imperfections of the pre-existing knowledgebase of
     phenomes to generate audible speech signals. Speech signal specific
     modelling techniques in combination with applied psychoacoustic
     principles drive training efficiency of neural networks with positive
     impact on quality of generated speech signals.
",G10L 19/00 (20130101); G10L 13/08 (20130101); G06K 9/6298 (20130101); G06N 5/02 (20130101); G06K 9/6262 (20130101); G10L 13/04 (20130101); G06N 3/08 (20130101); G06K 9/6247 (20130101); G06N 3/0427 (20130101); G06N 3/02 (20130101),G10L 13/08 (20130101); G10L 13/04 (20130101); G10L 19/00 (20130101); G06N 3/02 (20060101); G06N 5/02 (20060101); G06K 9/62 (20060101),"[['\n6163769', '\nDecember 2000'], ['\n6556682', '\nApril 2003'], ['\n7765103', '\nJuly 2010'], ['\n2004/0122662', '\nJune 2004'], ['\n2006/0181245', '\nAugust 2006'], ['\n2006/0224391', '\nOctober 2006'], ['\n2007/0233489', '\nOctober 2007'], ['\n2009/0018825', '\nJanuary 2009'], ['\n2009/0048844', '\nFebruary 2009'], ['\n2012/0143611', '\nJune 2012'], ['\n2013/0080173', '\nMarch 2013'], ['\n2013/0218568', '\nAugust 2013'], ['\n2014/0122081', '\nMay 2014'], ['\n2014/0278416', '\nSeptember 2014'], ['\n2015/0170637', '\nJune 2015'], ['\n2018/0034433', '\nFebruary 2018'], ['\n2018/0182396', '\nJune 2018'], ['\n2018/0213336', '\nJuly 2018']]","[7, '11,244,670', '11,244,669', '11,017,761', '10,896,669', '10,872,598', '10,872,596', '10,796,686']"," What is claimed is:  1.  A text-to-speech (TTS) system comprising: a front-end subsystem configured to provide analysis and conversion of text into an input vector having a base frequency for a
phoneme, a phenome duration, and a phoneme sequence;  and a back-end subsystem coupled to the front-end subsystem and configured to convert the input vector of the base frequency, the phoneme duration and the phoneme sequence into an intermediate vector
for processing by a signal generation unit of the back-end subsystem, the signal generation unit having a neural network interacting with a pre-existing knowledgebase of phonemes, wherein the signal generation unit is configured to use the neural network
interacting with the pre-existing knowledgebase of phonemes to apply an error signal to correct for speech signal distortions of the pre-existing knowledgebase of phonemes to generate the speech signal.
 2.  The TTS system of claim 1 further comprising a transformation unit that converts a frequency domain signal combined from the neural network and the pre-existing knowledgebase into the speech signal.
 3.  The TTS system of claim 1 wherein the pre-existing knowledgebase of phonemes comprises average basic acoustic signal data of how a speaker speaks derived from averaging of hours of recorded audible speech.
 4.  The TTS system of claim 1 wherein the neural network is configured to correct for psychoacoustic perceived speech signal distortions of the pre-existing knowledgebase of phonemes.
 5.  The TTS system of claim 1 wherein the back-end subsystem is further configured to upsample a frequency of the input vector provided to the neural network and the pre-existing knowledgebase to another frequency of the intermediate vector.
 6.  The TTS system of claim 5 wherein the upsampling unit includes a pitch normalization to normalize a pitch length of the input vector, and wherein the back-end subsystem includes an inverse pitch normalization unit to normalize the speech
signal from the transformation unit.
 7.  The TTS system of claim 1 wherein the neural network is configured to correct errors of voiced phonemes of the pre-existing knowledgebase of phonemes based on principal component analysis.
 8.  The TTS system of claim 7 wherein the principal component analysis is based on lossy modelling.
 9.  The TTS system of claim 7 wherein the back-end subsystem includes another neural network configured to correct errors of unvoiced phonemes of the pre-existing knowledgebase of phonemes based on noise band modelling.
 10.  The TTS system of claim 1 wherein the neural network is configured based on psychoacoustic modeling of phonemes.
 11.  A method of processing text-to-speech comprising: receiving an input vector having a base frequency for a phoneme, a phenome duration for the phoneme, and a phoneme sequence;  upsampling the input vector of the base frequency, the phoneme
duration and the phoneme sequence into an intermediate vector;  generating a speech signal from the intermediate vector using a pre-existing knowledgebase of phonemes;  and applying an error signal from a neural network to correct for speech signal
distortions of the speech signal based on an interaction between the neural network and the pre-existing knowledgebase.
 12.  The method of processing text-to-speech of claim 11 further comprising converting a frequency domain signal combined from the error signal and the generated speech signal of the pre-existing knowledgebase into a time domain speech signal.
 13.  The method of processing text-to-speech of claim 11 wherein the pre-existing knowledgebase of phonemes comprises average basic acoustic signal data of how a speaker speaks derived from averaging of hours of recorded audible speech.
 14.  The method of processing text-to-speech of claim 11 wherein the neural network is configured to correct for psychoacoustic perceived speech signal distortions of the pre-existing knowledgebase of phonemes.
 15.  The method of processing text-to-speech of claim 11 further comprising applying an output of another neural network to correct for speech signal distortions of unvoiced phonemes of the speech signal based on noise band modelling.
 16.  The method of processing text-to-speech of claim 11 wherein the upsampling further includes normalizing a pitch length of the input vector, and where the time domain signal is normalized using inverse pitch normalization.
 17.  The method of processing text-to-speech of claim 11 wherein the neural network configured to correct the signal distortions of the speech signal for voiced phonemes of the pre-existing knowledgebase of phonemes based on principal component
analysis.
 18.  The method of processing text-to-speech of claim 17 wherein the principal component analysis is based on lossy modelling.
 19.  The method of processing text-to-speech of claim 11 wherein the neural network is configured based on psychoacoustic modeling of phonemes.
 20.  A non-transitory computer-readable medium having program instructions which, when executed across one or more processors, causes at least a portion of the one or more processors to perform operations comprising: receiving an input vector
having a base frequency for a phoneme, a phenome duration for the phoneme, and a phoneme sequence;  upsampling the input vector of the base frequency, the phoneme duration and the phoneme sequence into an intermediate vector;  generating a speech signal
from the intermediate vector using a pre-existing knowledgebase of phonemes;  and applying an error signal from a neural network to correct for speech signal distortions of the speech signal based on interactions between the pre-existing knowledgebase.
 "
"10,325,070","
     June 18, 2019
","Treatment intelligence and interactive presence portal for telehealth
"," The present invention relates to a computerized system for and method of
     providing precision healthcare services such as consultation, education,
     assessment, diagnosis, intervention, or treatment at a distance via
     encrypted real-time image and audio presence where the healthcare
     professional's assessment, diagnosis, and intervention activities are
     informed by patient feedback, smart objects, and artificial intelligence
     and patient outcomes are optimized through recursive system feedback. The
     present invention is unlimited with regard to the type of patient entity
     or healthcare professional entity.
",G16H 40/67 (20180101); G16H 30/20 (20180101); G06N 20/00 (20190101),G06N 20/00 (20190101),"[['\n2005/0038669', '\nFebruary 2005'], ['\n2008/0249806', '\nOctober 2008'], ['\n2011/0118555', '\nMay 2011'], ['\n2012/0041911', '\nFebruary 2012'], ['\n2014/0019149', '\nJanuary 2014'], ['\n2014/0058189', '\nFebruary 2014'], ['\n2014/0200913', '\nJuly 2014'], ['\n2014/0365234', '\nDecember 2014'], ['\n2015/0012467', '\nJanuary 2015'], ['\n2015/0261934', '\nSeptember 2015'], ['\n2016/0378928', '\nDecember 2016']]","[22, '11,348,683', '11,337,648', '11,328,807', '11,325,005', '11,317,975', '11,309,085', '11,295,848', '11,289,200', '11,284,797', '11,282,608', '11,282,604', '11,282,599', '11,270,795', '11,264,123', '11,139,060', '11,037,679', '10,991,190', '10,991,185', '10,931,643', '10,873,855', '10,812,426', '10,587,545']"," What is claimed is:  1.  An integrated computer-implemented system for providing secure telehealth and teletherapy services informed by artificial intelligence and patient feedback comprising: a
physical database configured to store application data, patient data including demographic, genetic, assessment, diagnosis, treatment, history, outcome, wearable, mobile device, augmented reality device, and social network, and professional data
including assessment, diagnosis, treatment, intervention, prognosis, homework assignment, theoretical orientation, efficacy rating, discipline, and demographic data both in a raw form and a vectorized form where each patient has one or more vectors
comprised of the aggregation of numerous, diverse, data points about them;  a physical or virtual display configured to display a patient's real-time image and audio presence and relevant emotional, clinical, behavioral habit, and feedback data to a
healthcare professional and a computer-implemented physical processor or collection of processors configured to use artificial intelligence to analyze patterns in vectorized patient data, to transmit data to and receive data from patients across a range
of devices and interfaces including smart phones, mobile devices, augmented reality displays, wearables, and smart devices, to transmit data to and receive data from a healthcare professional, and to receive and execute commands from one or more
healthcare professionals, to continually learn from outputs of patient experiences that are added to said system that further refine further patient recommendations, wherein said computer-implemented system is configured to: receive patient information
chosen from the group consisting of symptom data, demographic data, genetic data, diagnostic data, treatment data, history data, outcome data, wearable data, mobile device data, augmented reality device data, social network data, and combinations
thereof;  store the patient information in the database in a vectorized form;  analyze the vectorized patient information via low-rank matrix factorization and suggest at least one healthcare professional to be used;  receive a selection from the patient
of a healthcare professional;  suggest intervention processes to the healthcare professional for the patient based on the patient information, healthcare professional demographic and theoretical orientation data, and vectorized data from previous
patients analyzed via low-rank matrix factorization;  receive from the healthcare professional an initial plan of service for the patient, the initial plan is based on the suggested intervention processes;  connect a patient's device to the healthcare
professional to receive real-time image and audio on the computer-implemented system and to provide in-session therapeutic intervention process based on the initial plan of service;  augment the healthcare professional's in-session therapeutic
intervention process with real-time data displays about the patient's mood derived from real-time analysis of the patient's speech including tone analysis, and facial expressions including microexpressions, by providing said real-time data to the
healthcare professional during the in-session therapeutic intervention process in which the healthcare professional providing the service for the patient over the computer-implemented system chosen from the group consisting of consultation, education,
assessment, diagnosis, intervention, treatment, and combinations thereof;  analyze feedback from the patient in real-time and suggest revisions to the initial plan of service during the in-session therapeutic intervention process, and push clinical
homework activities assigned by the healthcare professional during the in-session therapeutic intervention process to at least one patient device.
 2.  The system of claim 1, wherein the patient is a person or other entity seeking professional consultation, education, assessment, diagnosis, intervention, or treatment related to psychotherapy or psychoeducation.
 3.  The system of claim 1, wherein the healthcare professional is a person or other entity seeking to offer professional consultation, education, assessment, diagnosis, intervention, or treatment related to psychotherapy, psychoeducation, or
similar professional activities.
 4.  The system of claim 1, wherein the database has been secured through encryption.
 5.  The system of claim 1, wherein the transmission of the patient's real-time image and audio presence and relevant data has been secured through encryption.
 6.  The system of claim 1, wherein the transmission of the healthcare professional's real-time image and audio presence and relevant data has been secured through encryption.
 7.  The system of claim 1, wherein the computer-implemented processor has been configured to use artificial intelligence including low-rank matrix factorization, vectorization and skip thought vectors to analyze patterns in vectorized patient
assessments, genetics, diagnoses, treatments, outcomes, wearables, mobile devices, augmented reality devices, and social networks suggest prognosis and clinical interventions for an individual patient including in-session interventions, patient homework,
prescribed activities, and therapeutic orientation.
 8.  The system of claim 1, wherein the computer-implemented processor has been configured to transmit homework treatment recommendations to patients across a range of devices and interfaces including smart phones, mobile devices, augmented
reality displays, wearables, and smart devices and receive feedback about patient adherence to the treatment recommendations across a range of devices and interfaces.
 9.  The system of claim 1, wherein the computer-implemented processor has been configured to transmit predictive data derived from artificial intelligence processes informed by vectorized patient data and treatment adherence data about a patient
to a healthcare professional to support professional psychotherapeutic activities.
 10.  The system of claim 1, wherein the computer-implemented processor has been configured to receive raw and predictive data about patient treatment adherence from a range of devices including smart phones, mobile devices, augmented reality
displays, wearables, and smart devices and healthcare professional input and to transmit that data to the database in both a raw form and a vectorized form for use in training and improving an artificial intelligence system.
 11.  The system of claim 1, wherein the computer-implemented processor has been configured to use the artificial intelligence including low-rank matrix factorization, vectorization and skip thought vectors to analyze patterns in vectorized
patient assessments, genetics, diagnoses, treatments, outcomes, wearables, mobile devices, augmented reality devices, social networks, treatment adherence, and treatment response to suggest current prognosis as well as clinical interventions for an
individual patient.
 12.  The system of claim 1, wherein the computer-implemented processor has been configured to use artificial intelligence including low-rank matrix factorization, vectorization and skip thought vectors to analyze patterns in vectorized patient
assessments, genetics, diagnoses, treatments, outcomes, wearables, mobile devices, augmented reality devices, social networks, treatment adherence, and treatment response to suggest continuing or discontinuing treatment as well as discharge prognosis.
 13.  The system of claim 1, wherein the computer-implemented processor has been configured to use artificial intelligence including low-rank matrix factorization, vectorization and skip thought vectors to analyze patterns in vectorized
healthcare professional data including assessment, diagnosis, treatment, intervention, prognosis, homework assignment, theoretical orientation, efficacy rating, discipline, and demographic data to initially suggest prognosis as well as clinical
interventions for an individual patient including in-session interventions, patient homework, prescribed activities, and therapeutic orientation.
 14.  The system of claim 1, wherein the computer-implemented processor has been configured to use artificial intelligence including low-rank matrix factorization, vectorization and skip thought vectors to analyze patterns in vectorized
healthcare professional data including assessment, diagnosis, treatment, intervention, prognosis, homework assignment, theoretical orientation, efficacy rating, discipline, and demographic data to suggest current prognosis as well as clinical
interventions for an individual patient including in-session interventions, patient homework, prescribed activities, and therapeutic orientation.
 15.  The system of claim 1, wherein the computer-implemented processor has been configured to use artificial intelligence including low-rank matrix factorization, vectorization and skip thought vectors to analyze patterns in vectorized
healthcare professional data including assessment, diagnosis, treatment, intervention, prognosis, homework assignment, theoretical orientation, efficacy rating, discipline, and demographic data to suggest continuing or discontinuing treatment as well as
discharge prognosis.
 16.  A method of providing psychotherapy customized to a patient at a distance, including the steps of: entering by the patient information about themself into a computing system chosen from the group consisting of symptom data, demographic
data, genetic data, diagnostic data, treatment data, history data, outcome data, wearable data, mobile device data, augmented reality device data, social network data, and combinations thereof;  storing the patient information in a database in a
vectorized form;  analyzing by the computing system the vectorized patient information via low-rank matrix factorization and suggesting at least one healthcare professional to be used;  selecting by the patient a healthcare professional;  suggesting by
the computing system intervention processes to the healthcare professional for the patient based on the patient information, healthcare professional demographic and theoretical orientation data, and vectorized data from previous patients analyzed via
low-rank matrix factorization;  creating by the healthcare professional an initial plan of service for the patient based on the suggestions of the system and entering the plan into the system;  connecting the patient to the healthcare professional via
real-time image and audio on the computing system to provide in-session therapeutic intervention process based on the initial plan of service;  augmenting the healthcare professional's in-session therapeutic intervention process with real-time data
displays about the patient's mood derived from real-time analysis of the patient's speech including tone analysis, and facial expressions including microexpressions, by providing said real-time data to the healthcare professional during the in-session
therapeutic intervention process, the healthcare professional providing the service for the patient over the computing system chosen from the group consisting of consultation, education, assessment, diagnosis, intervention, treatment, and combinations
thereof;  analyzing by the computing system feedback from the patient in real-time and suggesting by the computing system revisions to the initial plan of service during the in-session therapeutic intervention process, and pushing by the computing system
clinical homework activities assigned by the healthcare professional during the in-session therapeutic intervention process to at least one patient device.  "
"10,325,588","
     June 18, 2019
","Acoustic feature extractor selected according to status flag of frame of
     acoustic signal
"," A method, computer system, and a computer program product for adaptively
     selecting an acoustic feature extractor in an Artificial Intelligence
     system is provided. The present invention may include acquiring a frame
     of an acoustic signal. The present invention may include checking a
     status of a flag to be used to indicate a proper acoustic feature
     extractor to be selected. The present invention may include processing
     the frame of the acoustic signal by the selected acoustic feature
     extractor indicated by the checked status. The present invention may
     include determining, based on data generated in the processing of the
     frame of the acoustic signal, an actual status of the frame of the
     acoustic signal. The present invention may include updating the status of
     the flag according to the actual status.
",G10L 25/84 (20130101); G10L 25/03 (20130101); G10L 15/02 (20130101); G10L 25/24 (20130101); G10L 25/30 (20130101); G10L 25/12 (20130101),G10L 15/02 (20060101); G10L 25/24 (20130101); G10L 25/84 (20130101); G10L 19/22 (20130101); G10L 25/30 (20130101),"[['\n5579432', '\nNovember 1996'], ['\n5579435', '\nNovember 1996'], ['\n5715372', '\nFebruary 1998'], ['\n5774847', '\nJune 1998'], ['\n6542866', '\nApril 2003'], ['\n7054810', '\nMay 2006'], ['\n7657427', '\nFebruary 2010'], ['\n8306821', '\nNovember 2012'], ['\n8380506', '\nFebruary 2013'], ['\n8798290', '\nAugust 2014'], ['\n8849231', '\nSeptember 2014'], ['\n9478232', '\nOctober 2016'], ['\n9858915', '\nJanuary 2018'], ['\n2002/0103643', '\nAugust 2002'], ['\n2006/0089958', '\nApril 2006'], ['\n2008/0019537', '\nJanuary 2008'], ['\n2008/0075303', '\nMarch 2008'], ['\n2009/0254352', '\nOctober 2009'], ['\n2011/0191101', '\nAugust 2011'], ['\n2011/0202337', '\nAugust 2011'], ['\n2013/0166308', '\nJune 2013'], ['\n2014/0046658', '\nFebruary 2014'], ['\n2014/0188465', '\nJuly 2014'], ['\n2014/0358525', '\nDecember 2014'], ['\n2015/0287415', '\nOctober 2015'], ['\n2016/0240215', '\nAugust 2016']]",[1]," What is claimed is:  1.  A computer-implemented method for selecting an acoustic feature extractor, the method comprising: acquiring a frame of an acoustic signal;  checking a status of a flag
for indicating an acoustic feature extractor to be selected;  processing the frame of the acoustic signal by the acoustic feature extractor indicated by the checked status;  determining, based on data generated in the processing of the frame of the
acoustic signal, an actual status of the frame of the acoustic signal;  and determining a new status of the flag based on the determined actual status, wherein the checked status of the flag and the determined new status of the flag includes a first
status and a second status, wherein the first status indicates a Linear Predictive Cepstral Coefficients (LPC) based feature extractor associated with a stationary acoustic signal and the second status indicates a Mel-frequency Cepstral Coefficients
(MFCC) based feature extractor associated with a non-stationary acoustic signal.
 2.  The method of claim 1, wherein, processing the frame of the acoustic signal by the acoustic feature extractor indicated by the checked status further comprises: in response to the status of the flag being the first status, selecting the LPC
based feature extractor for processing the frame of the acoustic signal.
 3.  The method of claim 1, wherein, processing the frame of the acoustic signal by the acoustic feature extractor indicated by the checked status further comprises: in response to the flag being the second status, selecting the MFCC based
feature extractor for processing the frame of the acoustic signal.
 4.  The method of claim 1, wherein, determining the new status of the flag according to the actual status further comprises: comparing the actual status with the checked status;  in response to the actual status being the same as the checked
status, maintaining the status of the flag unchanged;  and in response to the actual status being different from the checked status, updating the checked status of the flag to the actual status.
 5.  The method of claim 1, wherein, processing the frame of the acoustic signal by the acoustic feature extractor indicated by the checked status comprising: processing the frame of the acoustic signal by the LPC based feature extractor as the
acoustic feature extractor indicated by the checked status;  and determining, based on data generated in the processing of the frame of the acoustic signal, the actual status of the frame of the acoustic signal comprising: determining, based on whether a
newly generated LPC feature of the frame of the acoustic signal satisfies a statistical distribution of LPC coefficients, the actual status of the frame of the acoustic signal.
 6.  The method of claim 5, wherein the statistical distribution of LPC coefficients is determined by acquiring the newly generated LPC feature for each one of multiple frames of the acoustic signal and generating the statistical distribution of
LPC coefficients based on the acquired newly generated LPC feature.
 7.  The method of claim 1, wherein, processing the frame of the acoustic signal by the acoustic feature extractor indicated by the checked status comprising: processing the frame of the acoustic signal by the MFCC based feature extractor as the
acoustic feature extractor indicated by the checked status;  and determining, based on data generated in the processing of the frame of the acoustic signal, the actual status of the frame of the acoustic signal comprising: acquiring a Fast Fourier
Transform (FFT) value from data generated by the MFCC based feature extractor;  calculating a spectral centroid based on the acquired FFT value;  and determining the actual status based on the relationship between the calculated spectral centroid and a
predetermined threshold.
 8.  The method of claim 1, wherein acquiring a frame of the acoustic signal further comprises: determining whether the frame of the acoustic signal is background noise;  in response to the frame of acoustic data is background noise, acquiring a
next frame of the acoustic signal;  and in response to the frame of the acoustic signal is not background noise, entering a checking status of the flag.
 9.  A computer system for selecting an acoustic feature extractor, the computer system having a processing unit and a memory coupled to the processing unit and storing instructions thereon, the instructions executed by the processing unit to
perform a method comprising: acquiring a frame of an acoustic signal;  checking a status of a flag for indicating an acoustic feature extractor to be selected;  processing the frame of the acoustic signal by the acoustic feature extractor indicated by
the checked status;  determining, based on data generated in the processing of the frame of the acoustic signal, an actual status of the frame of the acoustic signal;  and determining a new status of the flag based on the determined actual status,
wherein the checked status of the flag and the determined new status of the flag includes a first status and a second status, wherein the first status indicates a Linear Predictive Cepstral Coefficients (LPC) based feature extractor associated with a
stationary acoustic signal and the second status indicates a Mel-frequency Cepstral Coefficients (MFCC) based feature extractor associated with a non-stationary acoustic signal.
 10.  The system of claim 9, wherein, processing the frame of the acoustic signal by the acoustic feature extractor indicated by the checked status further comprises: in response to the status of the flag being the first status, selecting the LPC
based feature extractor for processing the frame of the acoustic signal.
 11.  The system of claim 9, wherein, processing the frame of the acoustic signal by the acoustic feature extractor indicated by the checked status further comprises: in response to the flag being the second status, selecting the MFCC based
feature extractor for processing the frame of the acoustic signal.
 12.  The system of claim 9, wherein, determining the new status of the flag according to the actual status further comprises: comparing the actual status with the checked status;  in response to the actual status being the same as the checked
status, maintaining the status of the flag unchanged;  and in response to the actual status being different from the checked status, updating the checked status of the flag to the actual status.
 13.  The system of claim 9, wherein, processing the frame of the acoustic signal by the acoustic feature extractor indicated by the checked status comprising: processing the frame of the acoustic signal by the LPC based feature extractor as the
acoustic feature extractor indicated by the checked status;  and determining, based on data generated in the processing of the frame of the acoustic signal, the actual status of the frame of the acoustic signal comprising: determining, based on whether a
newly generated LPC feature of the frame of the acoustic signal satisfies a statistical distribution of LPC coefficients, the actual status of the frame of the acoustic signal.
 14.  The system of claim 13, wherein the statistical distribution of LPC coefficients is determined by acquiring the newly generated LPC feature for each one of multiple frames of the acoustic signal and generating the statistical distribution
of LPC coefficients based on the acquired newly generated LPC feature.
 15.  The system of claim 9, wherein, processing the frame of the acoustic signal by the acoustic feature extractor indicated by the checked status comprising: processing the frame of the acoustic signal by the MFCC based feature extractor as the
acoustic feature extractor indicated by the checked status;  and determining, based on data generated in the processing of the frame of the acoustic signal, the actual status of the frame of the acoustic signal comprising: acquiring a Fast Fourier
Transform (FFT) value from data generated by the MFCC based feature extractor;  calculating a spectral centroid based on the acquired FFT value;  and determining the actual status based on the relationship between the calculated spectral centroid and a
predetermined threshold.
 16.  The system of claim 9, wherein acquiring a frame of the acoustic signal further comprises: determining whether the frame of the acoustic signal is background noise;  in response to the frame of acoustic data is background noise, acquiring a
next frame of the acoustic signal;  and in response to the frame of the acoustic signal is not background noise, entering a checking status of the flag.
 17.  A computer program product used for selecting an acoustic feature extractor, the computer program product comprising a computer readable storage medium having program instructions embodied therewith, the program instructions executable by
one or more processors to cause the one or more processors to: acquiring a frame of an acoustic signal;  checking a status of a flag for indicating an acoustic feature extractor to be selected;  processing the frame of the acoustic signal by the acoustic
feature extractor indicated by the checked status;  determining, based on data generated in the processing of the frame of the acoustic signal, an actual status of the frame of the acoustic signal;  and determining a new status of the flag based on the
determined actual status, wherein the checked status of the flag and the determined new status of the flag includes a first status and a second status, wherein the first status indicates a Linear Predictive Cepstral Coefficients (LPC) based feature
extractor associated with a stationary acoustic signal and the second status indicates a Mel-frequency Cepstral Coefficients (MFCC) based feature extractor associated with a non-stationary acoustic signal.  "
"10,325,593","
     June 18, 2019
","Method and device for waking up via speech based on artificial
     intelligence
"," A method and a device for waking up via a speech based on artificial
     intelligence are provided in the present disclosure. The method includes:
     clustering phones to select garbage phones for representing the phones;
     constructing an alternative wake-up word approximate to a preset wake-up
     word according to the preset wake-up word; constructing a decoding
     network according to the garbage phones, the alternative wake-up word and
     the preset wake-up word; and waking up via the speech by using the
     decoding network. Due to the data size for the garbage phones is
     significantly smaller than the data size for the garbage words, a problem
     that the data size occupied is too large by using a garbage word model in
     the prior art is solved. Meanwhile, as a word is composed of several
     phones, the garbage phones may be more likely to cover all words than the
     garbage words. Thus, an accuracy of waking up is improved and a
     probability of false waking up is reduced.
",G10L 15/22 (20130101); G10L 15/02 (20130101); G10L 2015/088 (20130101); G10L 2015/081 (20130101); G10L 2015/025 (20130101),G10L 17/00 (20130101); G10L 15/22 (20060101); G10L 15/02 (20060101); G10L 15/08 (20060101),"[['\n2001/0012994', '\nAugust 2001'], ['\n2013/0339028', '\nDecember 2013'], ['\n2015/0095032', '\nApril 2015']]",[0]," What is claimed is:  1.  A method for waking up via a speech based on artificial intelligence, comprising: clustering, by at least one computing device, phones to select garbage phones for
representing the phones;  constructing, by the at least one computing device, an alternative wake-up word approximate to a preset wake-up word according to the preset wake-up word;  constructing, by the at least one computing device, a decoding network
according to the garbage phones, the alternative wake-up word and the preset wake-up word;  and waking up, by the at least one computing device, based upon a calculated score of the speech performed by the decoding network exceeding a preset threshold.
 2.  The method according to claim 1, wherein clustering, by the at least one computing device, phones to select garbage phones for representing the phones comprises: acquiring, by the at least one computing device, the phones and states
corresponding to the phones;  selecting, by the at least one computing device, a target phone from the phones at random;  adding, by the at least one computing device, the target phone into a garbage phone set;  starting, by the at least one computing
device, from the target phone, to inquire a phone having a greatest distance from the target phone among the phones and to add the phone inquired into the garbage phone set;  and taking, by the at least one computing device, the phone inquired as the
target phone, to circularly perform a process of inquiring the phone having a greatest distance from the target phone among the phones and adding the phone inquired into the garbage phone set, until the number of phones in the garbage phone set is a
preset first number.
 3.  The method according to claim 2, wherein acquiring, by the at least one computing device, the phones and states corresponding to the phones comprises: for the phones, fitting, by the at least one computing device, the states corresponding to
the phones with a Gaussian Mixture Model to obtain Gaussian Mixture Models of the phones respectively;  performing, by the at least one computing device, a parameter optimization on the Gaussian Mixture Models obtained with a plurality of speech samples
and speech labels;  and sequencing, by the at least one computing device, the Gaussian Mixture Models according to a similarity among the Gaussian Mixture Models after the parameter optimization, to obtain the phones and the states corresponding to the
phones respectively.
 4.  The method according to claim 2, wherein starting, by the at least one computing device, from the target phone, to inquire a phone having a greatest distance from the target phone among the phones comprises: calculating, by the at least one
computing device, a distance S between the target phone PM and each PN of the phones according to an equation S=(SM.sub.1-SN.sub.1).sup.2+(SM.sub.2-SN.sub.2).sup.2+(SM.sub.3-SN.sub.3)- .sup.2;  wherein SM.sub.1, SM.sub.2 and SM.sub.3 are three states of
the target phone PM respectively, SN.sub.1, SN.sub.2 and SN.sub.3 are three states of the each PN of the phones respectively;  and selecting, by the at least one computing device, the phone with a greatest value of S according to the distance S
calculated.
 5.  The method according to claim 1, wherein after clustering, by at least one computing device, phones to select garbage phones for representing the phones, the method further comprises: identifying, by the at least one computing device, a
plurality of speech samples with each of the phones respectively to obtain sets of matched phones;  selecting, by the at least one computing device, a set having a greatest number of matched phones as a filtrated set;  and filtrating, by the at least one
computing device, the garbage phones according to the filtrated set to conserve the garbage phones comprised in the filtrated set.
 6.  The method according to claim 1, wherein constructing, by the at least one computing device, an alternative wake-up word approximate to a preset wake-up word according to the preset wake-up word comprises: selecting, by the at least one
computing device, a near pronunciation word having characters with a number smaller than that of the preset wake-up word from a list of words having similar pronunciation to the preset wake-up word;  and combining, by the at least one computing device, a
part of the preset wake-up word with the near pronunciation word to construct the alternative wake-up word.
 7.  The method according to claim 1, wherein waking up, by the at least one computing device, via the speech by the decoding network comprises: extracting, by the at least one computing device, acoustic features of an input speech;  inputting,
by the at least one computing device, the acoustic features extracted into the decoding network;  performing, by the at least one computing device, a calculation to obtain an identified result of an optimal path among the decoding network by using a
dynamic programming algorithm;  and waking, by the at least one computing device, a terminal up according to the identified result.
 8.  The method according to claim 7, after performing, by the at least one computing device, a calculation to obtain an identified result of an optimal path among the decoding network, further comprising: calculating, by the at least one
computing device, an average value of the input speech according to a score of the optimal path, calculated in the decoding network, corresponding to each frame of the input speech;  calculating, by the at least one computing device, an average value of
the preset wake-up word according to a score of the optimal path, calculated in the decoding network, corresponding to each frame of the preset wake-up word;  and determining, by the at least one computing device, the identified result is valid if a
difference by subtracting the average value of the input speech from the average value of the preset wake-up word is greater than the preset threshold.
 9.  A device for waking up via a speech based on artificial intelligence, comprising: a processor;  a memory configured to store instructions executable by the processor, wherein the processor is configured to: cluster phones to select garbage
phones for representing the phones;  construct an alternative wake-up word approximate to a preset wake-up word according to the preset wake-up word;  construct a decoding network according to the garbage phones, the alternative wake-up word and the
preset wake-up word;  and wake up based upon a calculated score of the speech performed by the decoding network exceeding a preset threshold.
 10.  The device according to claim 9, wherein the processor is configured to cluster phones to select garbage phones for representing the phones by acts of: acquiring the phones and states corresponding to the phones;  selecting a target phone
from the phones at random;  adding the target phone into a garbage phone set;  starting from the target phone, to inquire a phone having a greatest distance from the target phone among the phones and to add the phone inquired into the garbage phone set; 
and taking the phone inquired as the target phone, to circularly perform a process of inquiring the phone having a greatest distance from the target phone among the phones and adding the phone inquired into the garbage phone set, until the number of
phones in the garbage phone set is a preset first number.
 11.  The device according to claim 10, wherein the processor is configured to acquire the phones and states corresponding to the phones by acts of: for the phones, fitting the states corresponding to the phones with a Gaussian Mixture Model to
obtain Gaussian Mixture Models of the phones respectively;  performing a parameter optimization on the Gaussian Mixture Models obtained with a plurality of speech samples and speech labels;  and sequencing the Gaussian Mixture Models according to a
similarity among the Gaussian Mixture Models after the parameter optimization, to obtain the phones and the states corresponding to the phones respectively.
 12.  The device according to claim 10, wherein the processor is configured to start from the target phone, to inquire a phone having a greatest distance from the target phone among the phones by acts of: calculating a distance S between the
target phone PM and each PN of the phones according to an equation S=(SM.sub.1-SN.sub.1).sup.2+(SM.sub.2-SN.sub.2).sup.2+(SM.sub.3-SN.sub.3)- .sup.2;  wherein SM.sub.1, SM.sub.2 and SM.sub.3 are three states of the target phone PM respectively, SN.sub.1,
SN.sub.2 and SN.sub.3 are three states of the each PN of the phones respectively;  and selecting the phone with a greatest value of S according to the distance S calculated.
 13.  The device according to claim 9, wherein the processor is further configured to: identify a plurality of speech samples with each of the phones respectively to obtain sets of matched phones;  selecting a set having a greatest number of
matched phones as a filtrated set;  and filtrating the garbage phones according to the filtrated set to conserve the garbage phones comprised in the filtrated set.
 14.  The device according to claim 9, wherein the processor is configured to construct an alternative wake-up word approximate to a preset wake-up word according to the preset wake-up word by acts of: selecting a near pronunciation word having
characters with a number smaller than that of the preset wake-up word from a list of words having similar pronunciation to the preset wake-up word;  and combining a part of the preset wake-up word with the near pronunciation word to construct the
alternative wake-up word.
 15.  The device according to claim 9, wherein the processor is configured to wake up via the speech by the decoding network by acts of: extracting acoustic features of an input speech;  inputting the acoustic features extracted into the decoding
network;  performing a calculation to obtain an identified result of an optimal path among the decoding network by using a dynamic programming algorithm;  and waking a terminal up according to the identified result.
 16.  The device according to claim 15, wherein the processor is further configured to: calculate an average value of the input speech according to a score of the optimal path, calculated in the decoding network, corresponding to each frame of
the input speech;  calculate an average value of the preset wake-up word according to a score of the optimal path, calculated in the decoding network, corresponding to each frame of the preset wake-up word;  and determine the identified result is valid
if a difference by subtracting the average value of the input speech from the average value of the preset wake-up word is greater than the preset threshold.
 17.  A non-transitory computer readable storage medium comprising instructions, wherein when the instructions are executed by a processor of a device to perform acts of: clustering phones to select garbage phones for representing the phones; 
constructing an alternative wake-up word approximate to a preset wake-up word according to the preset wake-up word;  constructing a decoding network according to the garbage phones, the alternative wake-up word and the preset wake-up word;  and waking up
based upon a calculated score of the speech performed by the decoding network exceeding a preset threshold.
 18.  The non-transitory computer readable storage medium according to claim 17, wherein clustering phones to select garbage phones for representing the phones comprises: acquiring the phones and states corresponding to the phones;  selecting a
target phone from the phones at random;  adding the target phone into a garbage phone set;  starting from the target phone, to inquire a phone having a greatest distance from the target phone among the phones and to add the phone inquired into the
garbage phone set;  and taking the phone inquired as the target phone, to circularly perform a process of inquiring the phone having a greatest distance from the target phone among the phones and adding the phone inquired into the garbage phone set,
until the number of phones in the garbage phone set is a preset first number.
 19.  The non-transitory computer readable storage medium according to claim 18, wherein acquiring the phones and states corresponding to the phones comprises: for the phones, fitting the states corresponding to the phones with a Gaussian Mixture
Model to obtain Gaussian Mixture Models of the phones respectively;  performing a parameter optimization on the Gaussian Mixture Models obtained with a plurality of speech samples and speech labels;  and sequencing the Gaussian Mixture Models according
to a similarity among the Gaussian Mixture Models after the parameter optimization, to obtain the phones and the states corresponding to the phones respectively.
 20.  The non-transitory computer readable storage medium according to claim 18, wherein starting from the target phone, to inquire a phone having a greatest distance from the target phone among the phones comprises: calculating a distance S
between the target phone PM and each PN of the phones according to an equation S=(SM.sub.1-SN.sub.1).sup.2+(SM.sub.2-SN.sub.2).sup.2+(SM.sub.3-SN.sub.3)- .sup.2;  wherein SM.sub.1, SM.sub.2 and SM.sub.3 are three states of the target phone PM
respectively, SN.sub.1, SN.sub.2 and SN.sub.3 are three states of the each PN of the phones respectively;  and selecting the phone with a greatest value of S according to the distance S calculated.  "
"10,331,367","
     June 25, 2019
","Embedded memory subsystems for a CNN based processing unit and methods of
     making
"," Embedded memory subsystems in a digital integrated circuit for artificial
     intelligence are disclosed. A semi-conductor substrate contains CNN
     processing units. Each CNN processing unit includes CNN logic circuits
     and an embedded memory subsystem. The memory subsystem includes first
     memory and second memory. The first memory contains an array of MTJ
     STT-RAM cells with each cell has a circular planar area with a diameter
     in a range of 40-120 nm. The second memory contains an array of MTJ
     STT-RAM cells with each cell has a circular planar area having a diameter
     in a range of 30-75 nm.
",G11C 11/161 (20130101); G06F 13/16 (20130101); G06N 3/04 (20130101); G06F 3/0688 (20130101); G06F 3/061 (20130101); H01L 43/12 (20130101); H01L 27/222 (20130101); G06F 13/4068 (20130101); G06F 3/0629 (20130101); G06N 3/0454 (20130101); H01L 43/08 (20130101); G06N 3/063 (20130101),G06N 3/04 (20060101); G11C 11/16 (20060101); G06F 3/06 (20060101); G06F 13/16 (20060101); G06F 13/40 (20060101); H01L 27/22 (20060101); H01L 43/08 (20060101); H01L 43/12 (20060101),"[['\n5355528', '\nOctober 1994'], ['\n7884433', '\nFebruary 2011'], ['\n8183061', '\nMay 2012'], ['\n8324698', '\nDecember 2012'], ['\n8772051', '\nJuly 2014'], ['\n8803293', '\nAugust 2014'], ['\n8933542', '\nJanuary 2015'], ['\n9111222', '\nAugust 2015'], ['\n9166143', '\nOctober 2015'], ['\n9437272', '\nSeptember 2016'], ['\n9673388', '\nJune 2017'], ['\n9685604', '\nJune 2017'], ['\n9734880', '\nAugust 2017'], ['\n9767557', '\nSeptember 2017'], ['\n9940534', '\nApril 2018'], ['\n9959500', '\nMay 2018'], ['\n2006/0011958', '\nJanuary 2006'], ['\n2007/0047294', '\nMarch 2007'], ['\n2011/0273926', '\nNovember 2011'], ['\n2012/0294076', '\nNovember 2012'], ['\n2012/0294078', '\nNovember 2012'], ['\n2014/0071741', '\nMarch 2014'], ['\n2016/0218280', '\nJuly 2016'], ['\n2016/0225818', '\nAugust 2016'], ['\n2016/0322090', '\nNovember 2016'], ['\n2017/0039472', '\nFebruary 2017'], ['\n2017/0040531', '\nFebruary 2017'], ['\n2017/0092693', '\nMarch 2017'], ['\n2017/0092851', '\nMarch 2017'], ['\n2017/0103298', '\nApril 2017'], ['\n2017/0103299', '\nApril 2017'], ['\n2017/0110653', '\nApril 2017'], ['\n2017/0125666', '\nMay 2017'], ['\n2017/0148979', '\nMay 2017'], ['\n2017/0324025', '\nNovember 2017'], ['\n2018/0137414', '\nMay 2018'], ['\n2018/0276539', '\nSeptember 2018'], ['\n2018/0285005', '\nOctober 2018'], ['\n2018/0285006', '\nOctober 2018'], ['\n2018/0285713', '\nOctober 2018'], ['\n2018/0285714', '\nOctober 2018'], ['\n2018/0285720', '\nOctober 2018'], ['\n2018/0285722', '\nOctober 2018'], ['\n2018/0285723', '\nOctober 2018']]",[0]," What is claimed is:  1.  A digital integrated circuit for artificial intelligence comprising: a semi-conductor substrate embedded in a single semi-conductor chip, the semi-conductor substrate
containing a plurality of cellular neural networks (CNN) processing units, each CNN processing unit comprising: CNN logic circuits;  and an embedded memory subsystem operatively coupling to the CNN logic circuits;  the embedded memory subsystem further
comprising: a first memory containing an array of first spin transfer torque magnetic random access memory (STT-RAM) cells with each first STT-RAM cell having a first magnetic tunnel junction (MTJ) element;  and a second memory containing an array of
second STT-RAM cells with each second STT-RAM cell having a second MTJ element, wherein the second memory is configured for higher endurance of balanced data read and write operations than the first memory.
 2.  The digital integrated circuit of claim 1, further comprises at least one input/output data bus operatively coupling the plurality of the CNN processing units via the first memory and the second memory and a controller for controlling
operations of the plurality of CNN processing units.
 3.  The digital integrated circuit of claim 2, wherein the semi-conductor substrate comprises a silicon substrate.
 4.  The digital integrated circuit of claim 2, wherein the first memory is configured for requiring data being stored with higher retention rate than the second memory.
 5.  The digital integrated circuit of claim 1, wherein each of the first MTJ element and the second MTJ element is made of two ferromagnetic layers located on either side of an oxide barrier layer orientated vertically.
 6.  The digital integrated circuit of claim 1, wherein each of the first MTJ element and the second MTJ element operates in a Current Perpendicular to Plane configuration.
 7.  The digital integrated circuit of claim 1, wherein the first MTJ element has a circular planar area with a first diameter in a range of 40-120 nm and the second MTJ element has a circular planar area with a second diameter in a range of
30-75 nm.
 8.  The digital integrated circuit of claim 1, wherein the first memory and the second memory have a same pitch.
 9.  The digital integrated circuit of claim 1, wherein the first memory and the second memory have different pitches.
 10.  A digital integrated circuit for artificial intelligence comprising: a semi-conductor substrate embedded in a single semi-conductor chip, the semi-conductor substrate containing a plurality of cellular neural networks (CNN) processing
units, each CNN processing unit comprising: CNN logic circuits;  and an embedded memory subsystem operatively coupling to the CNN logic circuits;  the embedded memory subsystem further comprising: a first memory containing an array of first spin transfer
torque magnetic random access memory (STT-RAM) cells, with each of the first STT-RAM cells having a first perpendicular magnetic tunnel junction (PMTJ) element with a first planar area, the first PMTJ being located on a first transistor;  and a second
memory containing an array of second STT-RAM cells with each of the second STT-RAM cells having a second PMTJ element with a second planar area;  wherein the first planar area is greater than the second planar area, the second PMTJ being located on a
second transistor.
 11.  The digital semi-conductor chip for artificial intelligence of claim 10, wherein the first transistor and the second transistor are independent with each other.  "
"10,331,983","
     June 25, 2019
","Artificial intelligence inference computing device
"," An artificial intelligence inference computing device contains a printed
     circuit board (PCB) and a number of electronic components mounted
     thereon. Electronic components include a wireless communication module, a
     controller module, a memory module, a storage module and at least one
     cellular neural networks (CNN) based integrated circuit (IC) configured
     for performing convolutional operations in a deep learning model for
     extracting features out of input data. Each CNN based IC includes a
     number of CNN processing engines operatively coupled to at least one
     input/output data bus. CNN processing engines are connected in a loop
     with a clock-skew circuit. Wireless communication module is configured
     for transmitting pre-trained filter coefficients of the deep learning
     model, input data and classification results.
",G06V 30/194 (20220101); G06N 3/0454 (20130101); G06N 3/04 (20130101); G06N 3/08 (20130101); G06N 3/063 (20130101); G06N 3/0481 (20130101),G06K 9/00 (20060101); G06N 3/063 (20060101); G06N 3/04 (20060101); G06K 9/66 (20060101),"[['\n5140670', '\nAugust 1992'], ['\n5355528', '\nOctober 1994'], ['\n5764858', '\nJune 1998'], ['\n6047276', '\nApril 2000'], ['\n6754645', '\nJune 2004'], ['\n9418319', '\nAugust 2016'], ['\n10051423', '\nAugust 2018'], ['\n2015/0127327', '\nMay 2015'], ['\n2015/0178246', '\nJune 2015'], ['\n2015/0213302', '\nJuly 2015'], ['\n2016/0019459', '\nJanuary 2016'], ['\n2016/0148078', '\nMay 2016'], ['\n2016/0284346', '\nSeptember 2016'], ['\n2016/0358069', '\nDecember 2016'], ['\n2018/0005344', '\nJanuary 2018'], ['\n2018/0358003', '\nDecember 2018'], ['\n2019/0043203', '\nFebruary 2019']]","[2, '11,335,108', '11,250,461']"," What is claimed is:  1.  An artificial intelligence inference computing device comprising: a printed circuit board (PCB) with a plurality of electronic components mounted thereon;  the plurality
of electronic components including: a wireless communication interface module;  a controller module;  and at least one cellular neural networks (CNN) based integrated circuit (IC) configured for performing convolutional operations in a deep learning
model for extracting features out of input data and each CNN based IC comprising a plurality of CNN processing engines operatively coupled to at least one input/output data bus, the plurality of CNN processing engines being connected in a loop with a
clock-skew circuit, each CNN processing engine including a CNN processing block configured for simultaneously obtaining convolution operations results using corresponding input data and pre-trained filter coefficients;  a first set of memory buffers
operatively coupling to the CNN processing block for storing the corresponding input data;  and a second set of memory buffers operatively coupling to the CNN processing block for storing the pre-trained filter coefficients.
 2.  The artificial intelligence inference computing device of claim 1, wherein the wireless communication interface module is configured for receiving pre-trained filter coefficients of the deep learning model from a smart client device.
 3.  The artificial intelligence inference computing device of claim 2, wherein the wireless communication interface module is configured for receiving input data from the smart client device.
 4.  The artificial intelligence inference computing device of claim 3, wherein the wireless communication interface module is configured for sending classification results to the smart client device.
 5.  The artificial intelligence inference computing device of claim 4, wherein the wireless communication interface module is based on Bluetooth.RTM..
 6.  The artificial intelligence inference computing device of claim 4, wherein the wireless communication interface module is based on WiFi.
 7.  The artificial intelligence inference computing device of claim 4, wherein the smart device comprises a smart phone.
 8.  The artificial intelligence inference computing device of claim 1, wherein the controller module is configured for loading the pre-trained filter coefficients of the deep learning model into said each CNN based IC.
 9.  The artificial intelligence inference computing device of claim 8, wherein the controller module is configured for executing the deep learning model on said each CNN based IC for the received input data.
 10.  The artificial intelligence inference computing device of claim 8, wherein the controller module is configured for performing fully-connected layers from the extracted features out of said each CNN based IC.
 11.  The artificial intelligence inference computing device of claim 1, wherein said each CNN processing engine further performs activation and pooling operations.
 12.  The artificial intelligence inference computing device of claim 1, wherein the imagery data are implemented with a specialized floating point format.
 13.  The artificial intelligence inference computing device of claim 12, wherein the specialized floating point format comprises 5-bit mantissa and 4-bit exponents.
 14.  The artificial intelligence inference computing device of claim 1, wherein the pre-trained filter coefficients are implemented with specialized floating point format.
 15.  The artificial intelligence inference computing device of claim 14, wherein the specialized floating point format comprises 12-bit mantissa and 2-bit exponents.
 16.  The artificial intelligence inference computing device of claim 1, wherein the plurality of electronic components further includes a memory module and a storage module.  "
"10,332,186","
     June 25, 2019
","Method and system for artificial intelligence augmented facility
     interaction
"," Various embodiments herein each include at least one of systems, methods,
     software, and devices for artificial intelligence augmented facility
     interaction. One such embodiment, in the form of a method includes
     identifying a location of a mobile device associated with a customer
     account within a facility. This method further includes determining at
     least one product recommendation based on data stored in a database
     associated with the customer account in view of the identified location
     and transmitting, via a network, the determined product recommendation
     data to the mobile device for presentation on the mobile device.
",G06N 5/022 (20130101); G06Q 30/0631 (20130101),G06Q 30/00 (20120101); G06N 5/02 (20060101); G06Q 30/06 (20120101),"[['\n9760927', '\nSeptember 2017'], ['\n2010/0161400', '\nJune 2010'], ['\n2011/0028160', '\nFebruary 2011'], ['\n2012/0259732', '\nOctober 2012'], ['\n2013/0285855', '\nOctober 2013'], ['\n2014/0094208', '\nApril 2014'], ['\n2014/0249902', '\nSeptember 2014'], ['\n2015/0039461', '\nFebruary 2015'], ['\n2015/0330805', '\nNovember 2015'], ['\n2015/0363943', '\nDecember 2015']]",[0]," What is claimed is:  1.  A method comprising: identifying a location of a mobile device associated with a customer account within a facility, wherein identifying the location of the mobile device
associated with the customer includes: iteratively determining an unfiltered location based on at least three received beacon signals, beacon location data retrieved from a database for the beacons from which the beacon signals were received, and
application of a triangulation algorithm to the beacon location data;  and iteratively determining a filtered location by calculating a location average and standard deviation from at least three most recently determined unfiltered locations and applying
the triangulation algorithm to at least three most recently determined unfiltered locations that are less than the standard deviation from the location average;  determining at least one product recommendation based on data stored in a database
associated with the customer account in view of the identified location;  transmitting, via a network, the determined product recommendation data to the mobile device for presentation on the mobile device;  and wherein: identifying the location of the
mobile device includes generating a path prediction of a path the mobile device is likely to travel based on the identified location and a plurality of previously identified locations;  and determining the at least one product recommendation is further
determined based on the generated path prediction.
 2.  The method of claim 1, wherein: data stored in the database associated with the customer account includes data representative of a plurality of: customer purchase history;  a customer wish list;  a customer shopping list;  customer provided
product preferences;  and customer analytic data generated for the customer.
 3.  The method of claim 2, wherein the at least one product recommendation includes a particular product identified as a product commonly purchased by other customers when the other customers have purchased a product included in at least one of
the customer purchase history, the customer wish list, and the customer shopping list when the identified location is in proximity within the facility of a location of the particular product.
 4.  The method of claim 3, wherein the particular product recommendation is transmitted via the network to the mobile device with location data identifying a location of the product within the facility, the location data renderable within a
facility map presented on the mobile device, the product recommendation data transmitted to the device further including information regarding the particular product renderable on the mobile device.
 5.  The method of claim 4, further comprising: transmitting a command via the network to a product presentation module deployed within the facility at a location of the particular product of the product recommendation, the command instructing
the product presentation module to perform at least one function of activating an attraction feature and presenting the particular product.
 6.  The method of claim 1, wherein generating a path prediction of the path the mobile device is likely to travel based on the identified location includes: iteratively generating a path predication through application of a Kalman filter
algorithm to a plurality of most recently determined unfiltered locations to determine a path having a velocity and a direction over a path period equal in duration to a period over which the plurality of most recently determined unfiltered locations
were determined, calculation of a distance variance from the path by the plurality of most recently determined unfiltered locations, and calculation of the path prediction in a shape starting a distance behind a most recent filtered location and
extending in the direction of the determined path for distance equal to a number of path periods multiplied by the velocity, and the shape having maximum a width at the end of each path period equal to the distance variance multiplied by a number of
elapsed path periods.
 7.  The method of claim 6, wherein the shape is a cone.
 8.  The method of claim 1, wherein identifying the location of the mobile device is iteratively performed and the identified location of the mobile device is stored, the method further comprising: determining a plurality of identified locations
are within a defined facility location area for at least a configured period;  and performing at least one action configured in association with the defined facility location area.
 9.  The method of claim 8, wherein the at least one configured action includes at least one of: transmitting data representative of a promotional offer to the mobile device;  and sending a message to facility personnel to approach a customer
carrying the mobile device to offer assistance.
 10.  The method of claim 1, further comprising: receiving a product query via the network from the mobile device;  retrieving product data from a product database of a product of the product query, the product data including location data of a
location where the product of the product query is located within the facility;  and transmitting the product data including the location data via the network to the mobile device, the location data renderable within a facility map presented on the
mobile device to guide a customer carrying the mobile device to the location of the product of the product query.
 11.  A system comprising: at least one processor, at least one memory device, and at least one network interface device;  a mobile device interaction module stored on the at least one memory device and executable by the at least one processor to
perform data processing activities, the data processing activities comprising: identifying a location of a mobile device associated with a customer account within a facility, wherein identifying the location of the mobile device associated with the
customer includes: iteratively determining an unfiltered location based on at least three received beacon signals, beacon location data retrieved from a database for the beacons from which the beacon signals were received, and application of a
triangulation algorithm to the beacon location data;  and iteratively determining a filtered location by calculating a location average and standard deviation from at least three most recently determined unfiltered locations and applying the
triangulation algorithm to at least three most recently determined unfiltered locations that are less than the standard deviation from the location average;  determining at least one product recommendation based on data stored in a database associated
with the customer account in view of the identified location;  transmitting, via the at least one network interface device to the mobile device, the determined product recommendation data for presentation on the mobile device;  and wherein: identifying
the location of the mobile device includes generating a path prediction of a path the mobile device is likely to travel based on the identified location and a plurality of previously identified locations;  and determining the at least one product
recommendation is further determined based on the generated path prediction.
 12.  The system of claim 11, wherein: data stored in the database associated with the customer account includes data representative of a plurality of: customer purchase history;  a customer wish list;  a customer shopping list;  and the at least
one product recommendation includes a particular product identified as a product commonly purchased by other customers when the other customers have purchased a product included in at least one of the customer purchase history, the customer wish list,
and the customer shopping list when the identified location is in proximity within the facility of a location of the particular product;  and the particular product recommendation is transmitted via the network interface device to the mobile device with
location data identifying a location of the product within the facility, the location data renderable within a facility map presented on the mobile device, the product recommendation data transmitted to the device further including information regarding
the particular product renderable on the mobile device.
 13.  The system of claim 11, wherein generating a path prediction of the path the mobile device is likely to travel based on the identified location includes: iteratively generating a path predication through application of a Kalman filter
algorithm to a plurality of most recently determined unfiltered locations to determine a path having a velocity and a direction over a path period equal in duration to a period over which the plurality of most recently determined unfiltered locations
were determined, calculation of a distance variance from the path by the plurality of most recently determined unfiltered locations, and calculation of the path prediction in a shape starting a distance behind a most recent filtered location and
extending in the direction of the determined path for distance equal to a number of path periods multiplied by the velocity, and the shape having maximum a width at the end of each path period equal to the distance variance multiplied by a number of
elapsed path periods.
 14.  The system of claim 11, wherein identifying the location of the mobile device is iteratively performed and the identified location of the mobile device is stored on the at least one memory device, the data processing activities of the
mobile device interaction module further comprising: determining a plurality of identified locations are within a defined facility location area for at least a configured period;  and performing at least one action configured in association with the
defined facility location area.
 15.  The system of claim 14, wherein the at least one configured action includes at least one of: transmitting data representative of a promotional offer to the mobile device;  and sending a message to facility personnel to approach a customer
carrying the mobile device to offer assistance.
 16.  The system of claim 11, the data processing activities of the mobile device interaction module further comprising: receiving a product query via the network from the mobile device;  retrieving product data from a product database of a
product of the product query, the product data including location data of a location where the product of the product query is located within the facility;  and transmitting the product data including the location data via the network interface device to
the mobile device, the location data renderable within a facility map presented on the mobile device to guide a customer carrying the mobile device to the location of the product of the product query.  "
"10,332,507","
     June 25, 2019
","Method and device for waking up via speech based on artificial
     intelligence
"," A method and a device for waking up via a speech based on artificial
     intelligence are provided in the present disclosure. The method includes:
     acquiring pronunciation information of a customized wake-up word;
     acquiring approximate pronunciation information of the pronunciation
     information; and constructing a network for identifying wake-up words
     according to a preset garbage word list, the pronunciation information
     and the approximate pronunciation information, identifying an input
     speech according to the network to acquire an identified result, and
     determining whether to perform a wake-up operation according to the
     identified result. With embodiments of the present disclosure, different
     networks for identifying the wake-up words may be constructed dynamically
     for different customized wake-up words, thus effectively improving an
     accuracy of waking up, reducing a false alarm rate, improving an
     efficiency of waking up, occupying less memory, and having low power
     consumption.
",G10L 15/22 (20130101); G10L 15/26 (20130101); G10L 15/02 (20130101); G10L 13/04 (20130101); G10L 15/083 (20130101); G10L 15/187 (20130101); G10L 13/00 (20130101); G10L 15/08 (20130101),G10L 13/06 (20130101); G10L 15/187 (20130101); G10L 15/08 (20060101); G10L 13/04 (20130101); G10L 15/26 (20060101); G10L 15/22 (20060101); G10L 15/02 (20060101),"[['\n2001/0012994', '\nAugust 2001'], ['\n2013/0339028', '\nDecember 2013'], ['\n2015/0095032', '\nApril 2015']]","[2, '11,308,955', '10,977,664']"," What is claimed is:  1.  A method for waking up via a speech based on artificial intelligence, comprising: acquiring, by at least one computing device, pronunciation information of a customized
wake-up word;  acquiring, by the at least one computing device, approximate pronunciation information of the pronunciation information;  constructing, by the at least one computing device, a network for identifying wake-up words according to a preset
garbage word list, the pronunciation information and the approximate pronunciation information;  identifying, by the at least one computing device, an input speech according to the network to acquire an identified result;  and determining, by the at
least one computing device, whether to perform a wake-up operation according to the identified result.
 2.  The method according to claim 1, wherein constructing, by the at least one computing device, a network for identifying wake-up words according to a preset garbage word list, the pronunciation information and the approximate pronunciation
information comprises: generating, by the at least one computing device, first pronunciation information according to the preset garbage word list and pronunciation information of a preset word comprised in the customized wake-up word;  and constructing,
by the at least one computing device, the network for identifying the wake-up words according to the preset garbage word list, the pronunciation information, the first pronunciation information and the approximate pronunciation information of the
customized wake-up word.
 3.  The method according to claim 1, wherein before identifying an input speech according to the network, the method further comprises: constructing, by the at least one computing device, a linear decoding network according to the customized
wake-up word;  and obtaining, by the at least one computing device, a summed likelihood score of the linear decoding network by a forced matching between an exemplary speech of the customized wake-up word and the linear decoding network;  and wherein
identifying, by at least one computing device, an input speech according to the network comprises: extracting, by the at least one computing device, acoustic features of the input speech;  analyzing, by the at least one computing device, the acoustic
features according to a preset acoustic model to obtain N states of the input speech and likelihood scores of the N states, wherein N is a positive integer;  correcting, by the at least one computing device, a likelihood score of an i.sup.th state with
the summed likelihood score if the i.sup.th state belongs to a state set of the customized wake-up word, wherein i is a positive integer and not greater than N;  and identifying, by the at least one computing device, the input speech by using a Viterbi
algorithm based on the network and according to the likelihood scores of N states corrected.
 4.  The method according to claim 3, wherein before identifying an input speech according to the network, the method further comprises: determining, by the at least one computing device, the number of phones corresponding to the customized
wake-up word according to the pronunciation information of the customized wake-up word;  adjusting, by the at least one computing device, the number of first active paths used in a preset speech identifying process according to the number of the phones,
to obtain the number of second active paths;  and wherein identifying, by the at least one computing device, the input speech by using a Viterbi algorithm based on the network and according to the likelihood scores of N states corrected comprises:
selecting, by the at least one computing device, an optimum identifying path from the network according to the likelihood scores of N states corrected and the number of the second active paths, to obtain the identified result of the input speech.
 5.  The method according to claim 3, wherein before identifying an input speech according to the network, the method further comprises: acquiring, by the at least one computing device, text length information of the customized wake-up word; 
acquiring, by the at least one computing device, a pronunciation score of the customized wake-up word;  and adjusting, by the at least one computing device, a preset first confidence threshold according to the text length information, the pronunciation
score and the summed likelihood score, to obtain a second confidence threshold;  and wherein determining, by the at least one computing device, whether to perform a wake-up operation according to the identified result comprises: acquiring, by the at
least one computing device, a confidence of the identified result;  performing, by the at least one computing device, the wake-up operation if the confidence is greater than the second confidence threshold;  and refusing, by the at least one computing
device, to perform the wake-up operation if the confidence is not greater than the second confidence threshold.
 6.  The method according to claim 5, wherein acquiring, by the at least one computing device, a pronunciation score of the customized wake-up word comprises: inquiring, by the at least one computing device, a pre-established probability
distribution table of the wake-up words to obtain pronunciation scores of respective syllables of the customized wake-up word.
 7.  The method according to claim 6, wherein the pre-established probability distribution table of the wake-up words is established by: for each of the syllables, counting, by the at least one computing device, a first number of characters
having the pronunciation information comprising the each of the syllables respectively in a word library, counting, by the at least one computing device, a second number of characters having the pronunciation information comprising the each of the
syllables respectively in a preset set of text data, and counting, by the at least one computing device, a third number of syllables of which pronunciation is similar to the each of the syllables;  and for each of the syllables, calculating, by the at
least one computing device, a probability pronunciation score of the each of the syllables according to the first number, the second number and the third number, and establishing, by the at least one computing device, the pre-established probability
distribution table.
 8.  An electronic device comprising: a processor;  a memory, configured to store instructions executable by the processor;  wherein the processor is configured to: acquire pronunciation information of a customized wake-up word;  acquire
approximate pronunciation information of the pronunciation information;  construct a network for identifying wake-up words according to a preset garbage word list, the pronunciation information and the approximate pronunciation information;  identify an
input speech according to the network to acquire an identified result;  and determine whether to perform a wake-up operation according to the identified result.
 9.  The electronic device according to claim 8, wherein the processor is configured to construct a network for identifying wake-up words according to a preset garbage word list, the pronunciation information and the approximate pronunciation
information by acts of: generating first pronunciation information according to the preset garbage word list and pronunciation information of a preset word comprised in the customized wake-up word;  and constructing the network for identifying the
wake-up words according to the preset garbage word list, the pronunciation information, the first pronunciation information and the approximate pronunciation information of the customized wake-up word.
 10.  The electronic device according to claim 8, wherein the processor is further configured to: construct a linear decoding network according to the customized wake-up word;  and obtain a summed likelihood score of the linear decoding network
by a forced matching between an exemplary speech of the customized wake-up word and the linear decoding network;  and the processor is configured to identify an input speech according to the network by acts of: extracting acoustic features of the input
speech;  analyzing the acoustic features according to a preset acoustic model to obtain N states of the input speech and likelihood scores of the N states, wherein N is a positive integer;  correcting a likelihood score of an i.sup.th state with the
summed likelihood score if the i.sup.th state belongs to a state set of the customized wake-up word, wherein i is a positive integer and not greater than N;  and identifying the input speech by using a Viterbi algorithm based on the network and according
to the likelihood scores of N states corrected.
 11.  The electronic device according to claim 10, wherein the processor is further configured to: determine the number of phones corresponding to the customized wake-up word according to the pronunciation information of the customized wake-up
word;  adjust the number of first active paths used in a preset speech identifying process according to the number of the phones, to obtain the number of second active paths;  and the processor is configured to identify the input speech by using a
Viterbi algorithm based on the network and according to the likelihood scores of N states corrected by acts of: selecting an optimum identifying path from the network according to the likelihood scores of N states corrected and the number of the second
active paths, to obtain the identified result of the input speech.
 12.  The electronic device according to claim 10, wherein the processor is further configured to: acquire text length information of the customized wake-up word and acquire a pronunciation score of the customized wake-up word;  and adjust a
preset first confidence threshold according to the text length information, the pronunciation score and the summed likelihood score, to obtain a second confidence threshold;  and the processor is configured to: determine whether to perform a wake-up
operation according to the identified result by acts of: acquiring a confidence of the identified result;  performing the wake-up operation if the confidence is greater than the second confidence threshold;  and refusing to perform the wake-up operation
if the confidence is not greater than the second confidence threshold.
 13.  The electronic device according to claim 12, wherein the processor is configured to acquire a pronunciation score of the customized wake-up word by acts of: inquiring a pre-established probability distribution table of the wake-up words to
obtain pronunciation scores of respective syllables of the customized wake-up word.
 14.  A non-transitory computer readable storage medium comprising instructions, wherein when the instructions are executed by a processor of a device to perform acts of: acquiring pronunciation information of a customized wake-up word; 
acquiring approximate pronunciation information of the pronunciation information;  and constructing a network for identifying wake-up words according to a preset garbage word list, the pronunciation information and the approximate pronunciation
information, identifying an input speech according to the network to acquire an identified result, and determining whether to perform a wake-up operation according to the identified result.
 15.  The non-transitory computer readable storage medium according to claim 14, wherein constructing a network for identifying wake-up words according to a preset garbage word list, the pronunciation information and the approximate pronunciation
information comprises: generating first pronunciation information according to the preset garbage word list and pronunciation information of a preset word comprised in the customized wake-up word;  and constructing the network for identifying the wake-up
words according to the preset garbage word list, the pronunciation information, the first pronunciation information and the approximate pronunciation information of the customized wake-up word.
 16.  The non-transitory computer readable storage medium according to claim 14, wherein when the instructions are executed by a processor of a server to further perform acts of: constructing a linear decoding network according to the customized
wake-up word;  and obtaining a summed likelihood score of the linear decoding network by a forced matching between an exemplary speech of the customized wake-up word and the linear decoding network;  and wherein identifying an input speech according to
the network comprises: extracting acoustic features of the input speech;  analyzing the acoustic features according to a preset acoustic model to obtain N states of the input speech and likelihood scores of the N states, wherein N is a positive integer; 
correcting a likelihood score of an i.sup.th state with the summed likelihood score if the i.sup.th state belongs to a state set of the customized wake-up word, wherein i is a positive integer and not greater than N;  and identifying the input speech by
using a Viterbi algorithm based on the network and according to the likelihood scores of N states corrected.
 17.  The non-transitory computer readable storage medium according to claim 16, wherein when the instructions are executed by a processor of a server to further perform acts of: determining the number of phones corresponding to the customized
wake-up word according to the pronunciation information of the customized wake-up word;  adjusting the number of first active paths used in a preset speech identifying process according to the number of the phones, to obtain the number of second active
paths;  and wherein identifying the input speech by using a Viterbi algorithm based on the network and according to the likelihood scores of N states corrected comprises: selecting an optimum identifying path from the network according to the likelihood
scores of N states corrected and the number of the second active paths, to obtain the identified result of the input speech.
 18.  The non-transitory computer readable storage medium according to claim 16, wherein before identifying an input speech according to the network, the method further comprises: acquiring text length information of the customized wake-up word
and acquiring a pronunciation score of the customized wake-up word;  and adjusting a preset first confidence threshold according to the text length information, the pronunciation score and the summed likelihood score, to obtain a second confidence
threshold;  and wherein determining whether to perform a wake-up operation according to the identified result comprises: acquiring a confidence of the identified result;  performing the wake-up operation if the confidence is greater than the second
confidence threshold;  and refusing to perform the wake-up operation if the confidence is not greater than the second confidence threshold.
 19.  The non-transitory computer readable storage medium according to claim 18, wherein acquiring a pronunciation score of the customized wake-up word comprises: inquiring a pre-established probability distribution table of the wake-up words to
obtain pronunciation scores of respective syllables of the customized wake-up word.
 20.  The non-transitory computer readable storage medium according to claim 19, wherein the pre-established probability distribution table of the wake-up words is established by: for each of the syllables, counting a first number of characters
having the pronunciation information comprising the each of the syllables respectively in a word library, counting a second number of characters having the pronunciation information comprising the each of the syllables respectively in a preset set of
text data, and counting a third number of syllables of which pronunciation is similar to the each of the syllables;  and for each of the syllables, calculating a probability pronunciation score of the each of the syllables according to the first number,
the second number and the third number, and establishing the pre-established probability distribution table.  "
"10,332,624","
     June 25, 2019
","System and methods for an intelligent medical practice system employing a
     learning knowledge base
"," An intelligent care provider medical practice system which learns the
     care provider's preferences and historical diagnosis for predicting and
     treating patients based on provided information. The system makes use of
     one or more medical knowledge bases and utilizes artificial intelligence
     and reasoning to learn the provider's preferences and tendencies. The
     system also automatically generates the provider's notes, treatment
     plans, and other medical practice actions for treating, billing, and
     processing the patient after an exam or visit.
",G16H 70/20 (20180101); G16H 50/20 (20180101); G16H 40/20 (20180101); G16H 10/60 (20180101),G06Q 50/00 (20120101); G16H 40/20 (20180101); G16H 50/20 (20180101); G06Q 10/00 (20120101),"[['\n5583758', '\nDecember 1996'], ['\n8655679', '\nFebruary 2014']]",[1]," What is claimed is:  1.  A medical practice system for treating a patient, the medical practice system comprising: a company server that includes at least one company computer processor, a
company database, a company outgoing folder and a company incoming folder;  a practice server that includes a practice incoming folder and a practice outgoing folder, and the company server in electronic communication, over an electronic network, with
the practice server;  the company database includes (a) at least one medical knowledge database, and (b) a care provider profile;  the at least one company computer processor, using instructions on at least one software application resident on the
computer processor, performing processing including: receiving patient information into the company incoming folder from the practice outgoing folder;  providing at least a portion of said patient information to the at least one medical knowledge
database and to the care provider profile, and wherein the care provider profile is a learned diagnosis profile adjusted, by the company computer processor, based upon a collection of counts including: incrementing a first count based on a plurality of
previously presented diagnosis which were not selected;  incrementing a second count based on a plurality of previously presented diagnosis which were selected, the incrementing the second count performed based on a weighing factor externally input
through interface with a user;  and change in the weighing factor being sensed by the company computer processor so as to manipulate the effect of previously presented diagnosis which were selected, the change in the weighing factor, as processed by the
company computer processor, serves to reflect (a) a decrease in the weighing factor in the situation of non-selection of a prior diagnosis, and (b) an increase in the weighing factor in the situation of selection of a prior diagnosis;  and the company
computer processor inputting a parameter that controls magnitude of weight accorded to a prior diagnosis, so as to control magnitude of impact of a prior diagnosis;  incrementing a third count based on observation of a first observed symptom; 
incrementing a fourth count based on observation of a second observed symptom;  and incrementing a fifth count based on a number of patients;  and subsequent to the at least one company computer processor providing the at least one medical knowledge
database with patient information, providing the care provider profile with patient information, and adjusting the collection of counts, the at least one company computer processor: receiving a plurality of patient health conditions, into the company
incoming folder from the practice outgoing folder, pertaining to a particular patient, each patient health condition constituted by a symptom experienced by the particular patient;  analyzing the plurality of patient health conditions with the at least
one medical knowledge base, in the company database, to identify multiple candidate diagnosis for the particular patient;  calculating a probability of each of the multiple candidate diagnosis;  adjusting the calculated probabilities based upon the care
provider profile, ranking the multiple candidate diagnoses based on the adjusted calculated probability;  selecting the highest ranked diagnosis as a selected diagnosis of the care provider;  and outputting the selected diagnosis from the company
outgoing folder to the practice incoming folder, such outputting including an electronic transmission of the selected diagnosis over the electronic network from the company outgoing folder to the practice incoming folder;  and displaying the selected
diagnosis on a physical display of a user device;  receiving a diagnosis confirmation from the user device;  identifying a treatment plan for the patient based on the confirmed diagnosis and care provider profile, and wherein the company computer
processor includes a rate system parameter;  the rate system parameter controlling a rate at which the company computer processor adjusts the collection of counts, based on observation of the plurality of patient health conditions received from the
practice outgoing folder, so as to evolve the care provider profile for diagnosis of future patients.
 2.  The medical practice system of claim 1, wherein the learned diagnosis profile is based upon many diagnosis over time.
 3.  The medical practice system of claim 2, wherein the medical practice system records the selected diagnosis of the care provider and adjusts the care provider profile so as to evolve the care provider profile for diagnosis of future patients.
 4.  The system of claim 1, wherein the system records the candidate diagnosis, for the particular patient, which was not selected and adjusts the care provider profile.
 5.  The medical practice system of claim 4, wherein the recording the candidate diagnosis which was not selected is performed by adjusting the collection of counts.
 6.  The medical practice system of claim 5, wherein the medical practice system generates, based upon the care provider profile, at least one of: a care provider note, a work order, and billing information.
 7.  A system that executes a computer program to provide a care provider medical diagnosis list for performing a physical step of treating a patient, the system comprising: a company server that includes at least one company computer processor,
a company database, a company outgoing folder and a company incoming folder a practice server that includes a practice incoming folder and a practice outgoing folder, and the company server in electronic communication, over an electronic network, with
the practice server;  the company database storing the computer program thereon, and the at least one company database includes (a) at least one medical information related database and (b) a care provider profile, and the company computer processor:
receives a plurality of patient information, into the company incoming folder from the practice outgoing folder, and provides the patient information to both the at least one medical information related database and the care provider profile, and wherein
the care provider profile is a learned diagnosis profile adjusted, by the processor, based upon a collection of counts including: incrementing a first count based on a plurality of previously presented diagnosis which were not selected;  incrementing a
second count based on a plurality of previously presented diagnosis which were selected;  incrementing a third count based on observation of a first observed symptom;  incrementing a fourth count based on observation of a second observed symptom;  and
incrementing a fifth count based on a number of patients;  and subsequent to the company computer processor providing the at least one medical knowledge database, providing the care provider profile, and adjusting the collection of counts, the company
computer processor: receives a request to run the program;  receives input data containing a plurality of patient health conditions, into the company incoming folder from the practice outgoing folder, pertaining to a particular patient, each patient
health condition constituted by a symptom experienced by the particular patient, executes the program to generate multiple candidate diagnosis based on both the received input data and the at least one medical information related database, wherein each
candidate diagnosis has an associated probability;  adjusts the probability of each candidate diagnosis based on the care provider profile;  ranks the candidate diagnoses based on the adjusted probability to create a care provider specific diagnosis
list;  transmits the care provider specific diagnosis list to a client device over the electronic network, displaying the care provider specific diagnosis list on the client device;  receiving a selected diagnosis confirmation from the client device; 
and identifying a treatment plan for the patient based on the selected diagnosis and care provider profile.
 8.  The system of claim 7, wherein the learned diagnosis profile is based upon many diagnosis over time.
 9.  The system of claim 8, wherein the system records a selected diagnosis of the care provider, for the particular patient, and adjusts the care provider profile based on the selected diagnosis so as to evolve the care provider profile for
diagnosis of future patients.
 10.  The system of claim 7 wherein the system records one or more presented diagnosis, for the particular patient, which was not selected and adjusts the care provider profile.
 11.  The system of claim 7, wherein the system receives a selection of one of the ranked diagnosis in the care provider specific diagnosis list.
 12.  The system of claim 7, wherein the system generates at least one of: a care provider note, a work order, or billing information.
 13.  A method for predicting a diagnosis and treating a patient, based on a care provider profile, the method performed by a company computer processor running at least one computer program, comprising the steps of: providing a company server
that includes the at least one company computer processor, a company database, a company outgoing folder and a company incoming folder providing a practice server that includes a practice incoming folder and a practice outgoing folder and the company
server in electronic communication with the practice server;  receiving a plurality of patient information into the company incoming folder from the practice outgoing folder wherein the at least one company database includes (a) at least one medical
knowledge database, and (b) a care provider profile;  and wherein the care provider profile is a learned diagnosis profile adjusted, by the computer processor, based upon a collection of counts including: incrementing a first count based on a plurality
of previously presented diagnosis which were not selected, incrementing a second count based on a plurality of previously presented diagnosis which were selected, incrementing a third count based on observation of a first observed symptom;  incrementing
a fourth count based on observation of a second observed symptom;  and incrementing a fifth count based on a number of patients;  and subsequent to the company computer processor providing the at least one medical knowledge database, providing the care
provider profile, and adjusting the collection of counts, the company computer processor: receiving patient information, over an electronic network, into the company incoming folder from the practice outgoing folder, pertaining to a particular patient,
wherein the patient information includes a plurality of medical conditions, and each medical condition constituted by a symptom experienced by the particular patient;  analyzing, by the company computer processor, the patient information against the at
least one medical information database in communication with the company computer processor;  identifying, by the company computer processor, a plurality of candidate diagnosis within the knowledge database based upon the analysis, obtaining, by the
company computer processor, the probability of each of the candidate diagnosis;  adjusting, by the company computer processor, the calculated probabilities of each candidate diagnosis based on the care provider profile;  ranking the plurality of
candidate diagnosis, by the company computer processor based upon the adjusted probabilities so as to generate ranked diagnosis;  and displaying the ranked diagnosis on a display of a user device: receiving a selected diagnosis confirmation from the
client device;  and identifying a treatment plan for the patient based on the selected diagnosis and care provider profile.
 14.  The method of claim 13, further comprising the step of transmitting, by the company computer processor, the ranked diagnosis to a client computer associated with the practice server.
 15.  The method of claim 14, further comprising the step of receiving a selection of one of the ranked diagnosis from the client computer, and such selection constituting a selected diagnosis.
 16.  The method of claim 15, further comprising the step of adjusting, by the company computer processor, the care provider profile based upon the selected diagnosis.
 17.  The method of claim 15, further comprising the step of generating, by the program, at least one of: a care provider note, a work order, or billing information.
 18.  A system that executes a computer program to provide a care provider medical diagnosis list for treating a patient, the system comprising: a company server that includes at least one company computer processor, a company database, a company
outgoing folder and a company incoming folder a practice server that includes a practice incoming folder and a practice outgoing folder, and the company server in electronic communication with the practice server;  the company database storing the
computer program thereon, the company database further storing (a) at least one medical information related database and (b) a care provider profile, and the company computer processor: receives a plurality of patient information, into the company
incoming folder from the practice outgoing folder, and provides both the at least one medical information related database and the care provider profile, and wherein the care provider profile is a learned diagnosis profile adjusted, by the company
computer processor, based upon a collection of counts including: incrementing a first count based on a plurality of previously presented diagnosis which were not selected, incrementing a second count based on a plurality of previously presented diagnosis
which were selected, incrementing a third count based on observation of a first observed symptom;  incrementing a fourth count based on observation of a second observed symptom;  and incrementing a fifth count based on a number of patients;  and
subsequent to the processor providing the at least one medical knowledge database, providing the care provider profile, and adjusting the collection of counts, the processor: receives a request to run the program;  receives input data, into the company
incoming folder from the practice outgoing folder, over an electronic network, containing a plurality of patient health conditions pertaining to a particular patient, each patient health condition constituted by a symptom experienced by the particular
patient;  executes the program to generate multiple candidate diagnosis based on both the received input data and the at least one medical information related database, wherein each candidate diagnosis has an associated probability, adjusts the
probability of each candidate diagnosis based on the care provider profile, ranks the candidate diagnoses based on the adjusted probability to create a care provider specific diagnosis list;  and transmits the care provider specific diagnosis list to a
processor based client device via the practice server;  receives a selection from the client device of one of the diagnosis, the selection constituting a selected diagnosis;  and identifies a treatment plan for the patient based on the selected diagnosis
and the care provider profile.
 19.  The system of claim 18, wherein the system records the selected diagnosis of the care provider and adjusts the care provider profile based on the selected diagnosis so as to evolve the care provider profile for diagnosis of future patients.
 20.  The system of claim 18, wherein the system records one or more presented diagnosis, for the particular patient, which was not selected and adjusts the care provider profile.
 21.  The system of claim 18, wherein the system generates at least one of: a care provider note, a work order, or billing information.
 22.  The medical practice system of claim 1, wherein the company outgoing folder, the company incoming folder, the practice incoming folder and the practice outgoing folder are all constituted by respective XML folders. 
"
"10,339,689","
     July 2, 2019
","Intelligent camera
"," Presented here is technology to efficiently process camera images to
     generate artistic images and videos using an artificial intelligence
     module receiving inputs from multiple sensors. Multiple sensors can
     include a depth sensor, a conventional camera, and a motion tracker
     providing inputs to the artificial intelligence module. Based on the
     inputs, the artificial intelligence module can segment the received image
     and/or video into a foreground image and a background image to produce
     portrait imagery by blurring the background image and/or video. The
     artificial intelligence module can select the most aesthetically pleasing
     image from a video. In addition, the artificial intelligence module can
     adjust lighting in an image or video to create artistic lighting effects.
     All the processing can be done in real time due to efficient combination
     of artificial intelligence modules, traditional image processing
     techniques, and use of specialized hardware.
",G06K 9/6274 (20130101); G06N 3/08 (20130101); G06V 30/194 (20220101); G06T 5/002 (20130101); G06N 3/0454 (20130101); G06K 9/6267 (20130101); G06T 7/194 (20170101); G06T 11/60 (20130101); H04N 7/185 (20130101); G06N 3/04 (20130101); G06T 7/11 (20170101); G06T 5/50 (20130101); G06T 2207/10024 (20130101); G06T 2207/20084 (20130101); G06T 2207/30196 (20130101); G06T 2207/20081 (20130101); H04N 7/18 (20130101); G06T 2207/10028 (20130101),G06N 3/08 (20060101); G06T 5/00 (20060101); G06K 9/66 (20060101); G06N 3/04 (20060101); G06T 11/60 (20060101); G06T 7/194 (20170101); H04N 7/18 (20060101); G06K 9/62 (20060101),"[['\n7003136', '\nFebruary 2006'], ['\n8238615', '\nAugust 2012'], ['\n9223781', '\nDecember 2015'], ['\n9704231', '\nJuly 2017'], ['\n9734567', '\nAugust 2017'], ['\n9818150', '\nNovember 2017'], ['\n10147216', '\nDecember 2018'], ['\n2008/0064377', '\nMarch 2008'], ['\n2012/0268612', '\nOctober 2012'], ['\n2015/0101064', '\nApril 2015'], ['\n2016/0098844', '\nApril 2016'], ['\n2017/0155887', '\nJune 2017'], ['\n2018/0039879', '\nFebruary 2018'], ['\n2019/0026609', '\nJanuary 2019'], ['\n2019/0034976', '\nJanuary 2019']]",[0]," The invention claimed is:  1.  A method to create aesthetically pleasing images in real time with a cell phone camera, the method comprising: obtaining from a camera a plurality of images from a
plurality of viewpoints of an environment surrounding the camera, the plurality of images obtained by continually moving the camera through the plurality of viewpoints;  selecting within a centisecond an aesthetically pleasing image with a highest
aesthetic score from the plurality of images by using a neural network trained to assign an aesthetic score to each image in the plurality of images, the neural network running on a processor optimized to execute operations associated with the neural
network, and the neural network comprising a plurality of layers arranged sequentially, each layer in the plurality of layers comprising a plurality of nodes performing a plurality of computations in parallel said selecting within the centisecond
comprising: measuring an amount of time associated with selecting the aesthetically pleasing image with the highest aesthetic score;  when the amount of time exceeds a predetermined amount of time, distributing a first plurality of nodes associated with
a first layer in the plurality of layers across multiple processors associated with a cell phone until the amount of time is below the predetermined amount of time;  and displaying a visual notification along with the aesthetically pleasing image on a
viewfinder associated with the camera, the visual notification indicating to a user to record the aesthetically pleasing image.
 2.  A method comprising: obtaining from a light sensor a plurality of images from a plurality of viewpoints of an environment surrounding the light sensor, the plurality of images obtained by continually moving the light sensor through the
plurality of viewpoints;  selecting within a specified amount of time an aesthetically pleasing image with a highest aesthetic score from the plurality of images by using an artificial intelligence module trained to assign an aesthetic score to each
image in the plurality of images, wherein the artificial intelligence module comprises a plurality of layers arranged sequentially, each layer in the plurality of layers comprising a plurality of nodes performing a plurality of computations in parallel; 
measuring an amount of time associated with selecting the aesthetically pleasing image with the highest aesthetic score;  and when the amount of time exceeds a predetermined amount of time, distributing a first plurality of nodes associated with a first
layer in the plurality of layers across multiple processors associated with a cell phone until the amount of time is below the predetermined amount of time.
 3.  The method of claim 2, wherein the plurality of images comprises an ordered sequence of images from an ordered sequence of viewpoints;  obtaining an amount of time for the artificial intelligence module to assign the aesthetic score to an
initial image in the ordered sequence of images;  and when a number of images in the ordered sequence of images combined with the amount of time exceeds the specified amount of time, achieving the selection within the specified amount of time by dropping
a subset of images in the ordered sequence of images to obtain faster processing.
 4.  The method of claim 2, wherein the plurality of images comprises an ordered sequence of images from an ordered sequence of viewpoints;  obtaining an amount of time for the artificial intelligence module to assign the aesthetic score to an
initial image in the ordered sequence of images;  and when a number of images in the ordered sequence of images combined with the amount of time exceeds the specified amount of time, achieving the selection within the specified amount of time by
utilizing an additional processor associated with the light sensor.
 5.  The method of claim 2, comprising: determining a presence of a vanishing point associated with an image in the plurality of images by detecting converging lines in the image;  determining a presence of a foreground object associated with the
image;  and assigning a high aesthetic score to the image where the foreground object and the vanishing point are proximate to each other and to a center of the image.
 6.  The method of claim 2, comprising: tracking a motion of a plurality of objects associated with the plurality of images;  detecting an object in the plurality of objects with a least amount of motion;  and assigning a high aesthetic score to
an image in the plurality of images where the object with the least amount of motion is proximate to a center of the image.
 7.  The method of claim 2, comprising: segmenting the plurality of images into a foreground object and a background object;  determining a location of the foreground object within each image in the plurality of images;  and assigning a high
aesthetic score to a first image in the plurality of images where the foreground object is proximate to a center of the image or to a second image in the plurality of images were the foreground object is proximate to an edge of the image and
substantially symmetric about the center of the image.
 8.  The method of claim 2, comprising: obtaining from a depth sensor substantially collocated with the light sensor a plurality of depth measurements from the plurality of viewpoints of the environment surrounding the depth sensor, the plurality
of viewpoints obtained by continually moving the depth sensor through the plurality of viewpoints, wherein each depth measurement in the plurality of depth measurements corresponds to an image in the plurality of images;  providing the plurality of depth
measurements in the plurality of images to the artificial intelligence module running on a dedicated processor and trained to assign the aesthetic score to each image in the plurality of images based on the plurality of depth measurements in the
plurality of images;  and selecting within the specified amount of time the aesthetically pleasing image with the highest aesthetic score from the plurality of images using the artificial intelligence module.
 9.  The method of claim 2, comprising: obtaining from a motion tracking sensor substantially collocated with the light sensor a plurality of motion tracking data associated with a plurality of objects in the plurality of images;  providing the
plurality of motion tracking data and the plurality of images to the artificial intelligence module running on a dedicated processor and trained to assign the aesthetic score to each image in the plurality of images based on the plurality of motion
tracking data and the plurality of images;  and selecting within the specified amount of time the aesthetically pleasing image with the highest aesthetic score from the plurality of images using the artificial intelligence module.
 10.  The method of claim 2, wherein the plurality of images comprises a live feed from the light sensor;  selecting the aesthetically pleasing image from the live feed;  and providing a visual notification along with the aesthetically pleasing
image on a display associated with the light sensor, the visual notification indicating to a user to record the aesthetically pleasing image.
 11.  A system comprising: a light sensor to record a plurality of images from a plurality of viewpoints of an environment surrounding the light sensor, the plurality of images obtained by continually moving the light sensor through the plurality
of viewpoints;  an artificial intelligence module trained to assign an aesthetic score to each image in the plurality of images, the artificial intelligence module to receive the plurality of images and to select within a specified amount of time an
aesthetically pleasing image with a highest aesthetic score from the plurality of images, the artificial intelligence module comprising a plurality of layers arrange sequentially, each layer in the plurality of layers comprising a plurality of nodes
performing a plurality of computations in parallel;  a processor to measure an amount of time associated with selecting the aesthetically pleasing image with the highest aesthetic score;  and when the amount of time exceeds a predetermined amount of
time, the processor to distribute a first plurality of nodes associated with a first layer in the plurality of layers across multiple processors associated with a cell phone until the amount of time is below the predetermined amount of time.
 12.  The system of claim 11, the plurality of images comprising an ordered sequence of images from an ordered sequence of viewpoints;  the processor to obtain an amount of time for the artificial intelligence module to assign the aesthetic score
to an initial image in the ordered sequence of images;  and when a number of images in the ordered sequence of images combined with the amount of time exceeds the specified amount of time, the processor to achieve the selection within the specified
amount of time by dropping a subset of images in the ordered sequence of images to obtain faster processing.
 13.  The system of claim 11, comprising: the processor to obtain the plurality of images comprises an ordered sequence of images from an ordered sequence of viewpoints, and to obtain an amount of time for the artificial intelligence module to
assign the aesthetic score to an initial image in the ordered sequence of images;  and when a number of the ordered sequence of images combined with the amount of time exceeds the specified amount of time, the processor to achieve the selection within
the specified amount of time by utilizing an additional processor associated with the light sensor.
 14.  The system of claim 11, comprising: the processor to determine a presence of a vanishing point associated with an image in the plurality of images by detecting converging lines in the image;  the processor to determine a presence of a
foreground object associated with the image;  and the processor to assign a high aesthetic score to the image where the foreground object and the vanishing point are proximate to each other and to a center of the image.
 15.  The system of claim 11, comprising: a motion tracking sensor to track a motion of a plurality of objects associated with the plurality of images;  a processor to detect an object in the plurality of objects with a least amount of motion; 
and the processor to assign a high aesthetic score to an image in the plurality of images where the object with the least amount of motion is proximate to a center of the image.
 16.  The system of claim 11, comprising: a second artificial intelligence module to segment the plurality of images into a foreground object and a background object and to determine a location of the foreground object within each image in the
plurality of images;  and the artificial intelligence module to assign a high aesthetic score to a first image in the plurality of images where the foreground object is proximate to a center of the image or to a second image in the plurality of images
were the foreground object is proximate to an edge of the image and substantially symmetric about the center of the image.
 17.  The system of claim 11, comprising: a depth sensor substantially collocated with the light sensor to record a plurality of depth measurements from the plurality of viewpoints of the environment surrounding the depth sensor, the plurality of
viewpoints obtained by continually moving the depth sensor through the plurality of viewpoints, wherein each depth measurement in the plurality of depth measurements corresponds to an image in the plurality of images;  and the artificial intelligence
module to assign the aesthetic score to each image in the plurality of images based on the plurality of depth measurements and the plurality of images and to select within the specified amount of time the aesthetically pleasing image with the highest
aesthetic score from the plurality of images using the artificial intelligence module.
 18.  A system comprising: a light sensor to record a plurality of images from a plurality of viewpoints of an environment surrounding the light sensor, the plurality of images obtained by continually moving the light sensor through the plurality
of viewpoints;  a motion tracking sensor substantially collocated with the light sensor to collect a plurality of motion tracking data associated with a plurality of objects in the plurality of images;  and the artificial intelligence module to assign
the aesthetic score to each image in the plurality of images based on the plurality of motion tracking data and the plurality of images and to select within the specified amount of time the aesthetically pleasing image with the highest aesthetic score
from the plurality of images using the artificial intelligence module.  "
"10,339,695","
     July 2, 2019
","Content-based medical image rendering based on machine learning
"," An artificial intelligence agent is machine trained and used to provide
     physically-based rendering settings. By using deep learning and/or other
     machine training, settings of multiple rendering parameters may be
     provided for consistent imaging even in physically-based rendering.
",G06N 3/0454 (20130101); G06T 15/06 (20130101); G06N 20/00 (20190101); G06T 15/08 (20130101); G16H 30/20 (20180101); G06K 9/6273 (20130101); G06T 15/506 (20130101); G16H 50/20 (20180101); G16H 30/40 (20180101); G06K 9/6262 (20130101); G06T 15/005 (20130101); G06T 5/00 (20130101); G06N 3/006 (20130101); G06V 2201/03 (20220101); G06N 7/005 (20130101); G06K 2209/05 (20130101); G06T 2210/41 (20130101); G06N 3/02 (20130101),G06T 15/50 (20110101); G06N 3/04 (20060101); G06T 15/00 (20110101); G06T 15/06 (20110101); G06T 15/08 (20110101); G16H 50/20 (20180101); G06K 9/62 (20060101); G06T 5/00 (20060101),"[['\n6463438', '\nOctober 2002'], ['\n6690371', '\nFebruary 2004'], ['\n6999549', '\nFebruary 2006'], ['\n8509525', '\nAugust 2013'], ['\n8520906', '\nAugust 2013'], ['\n8711144', '\nApril 2014'], ['\n8831358', '\nSeptember 2014'], ['\n8983941', '\nMarch 2015'], ['\n9008391', '\nApril 2015'], ['\n9014485', '\nApril 2015'], ['\n9020233', '\nApril 2015'], ['\n9324022', '\nApril 2016'], ['\n2003/0120651', '\nJune 2003'], ['\n2008/0118151', '\nMay 2008'], ['\n2008/0129732', '\nJune 2008'], ['\n2012/0078062', '\nMarch 2012'], ['\n2014/0119672', '\nMay 2014'], ['\n2014/0277939', '\nSeptember 2014'], ['\n2015/0097842', '\nApril 2015'], ['\n2015/0100530', '\nApril 2015'], ['\n2015/0161147', '\nJune 2015'], ['\n2016/0104057', '\nApril 2016'], ['\n2016/0189003', '\nJune 2016'], ['\n2016/0232658', '\nAugust 2016'], ['\n2016/0242740', '\nAugust 2016'], ['\n2016/0321523', '\nNovember 2016'], ['\n2017/0039297', '\nFebruary 2017'], ['\n2017/0161607', '\nJune 2017'], ['\n2017/0169620', '\nJune 2017']]","[6, '11,334,645', '11,328,177', '11,288,602', '10,991,070', '10,973,590', '10,699,410']"," We claim:  1.  A method for content-based rendering based on machine learning in a rendering system, the method comprising: loading, from memory, a medical dataset representing a
three-dimensional region of a patient;  applying, by a machine, the medical dataset to a machine-learnt model, the machine-learned model trained with deep learning to extract features from the medical dataset and trained to output values for two or more
volume rendering parameters that correspond to input of the medical dataset, the two or more volume rendering parameters being settings of a volume renderer, the settings used by the volume renderer to control rendering, from three dimensions to
two-dimensions, an image of the three-dimensional region of the patient;  rendering, by the volume renderer, the image of the three-dimensional region of the patient from the medical dataset using the output values resulting from the applying as the
settings to control the rendering from the medical dataset, the rendering of the medical dataset of the three-dimensional region being to the image in the two-dimensions;  and transmitting the image.
 2.  The method of claim 1 further comprising loading patient information other than the medical dataset representing the three-dimensional region of the patient.
 3.  The method of claim 1 wherein applying comprises applying to output the values for the two or more volume rendering parameters as all of controls for data consistency handling, lighting design, material propriety, and internal renderer
property based on applying user input of viewing design and the medical dataset.
 4.  The method of claim 1 wherein applying comprises applying with the machine-learnt model trained to output the values resulting in the image corresponding to a standard image despite differences in the medical dataset.
 5.  The method of claim 1 wherein applying comprises applying with the machine-learnt model comprising a regression, classification, or reinforcement learnt model.
 6.  The method of claim 1 wherein applying comprises applying with the deep learning as a deep neural network.
 7.  The method of claim 1 wherein applying comprises applying with the machine-learnt model as a deep reinforcement learnt model.
 8.  The method of claim 1 wherein rendering comprises rendering with unbiased path tracing.
 9.  The method of claim 1 wherein transmitting comprises transmitting as part of a diagnostic report, as an initial image of an interactive viewing, or as an overlay in augmented reality.
 10.  The method of claim 1 further comprising: measuring ambient light with a light sensor;  wherein applying comprises applying the ambient light and the medical dataset, the machine-learnt model trained to output the values based in part on
the ambient light.
 11.  A method for machine training for content-based rendering in a machine training system, the method comprising: inputting first volume data of a volume of a patient, a first image of the volume, and first values of rendering parameters to
training of an artificial intelligence, rendering parameters being settings to control rendering from the volume to a two-dimensional image;  machine training, with a machine, the artificial intelligence to output second values of the rendering
parameters for second volume data where the second values control the rendering from the volume to provide a second rendered image of the second volume data modeled on the first image;  and storing the trained artificial intelligence.
 12.  The method of claim 11 wherein inputting comprises inputting the patient non-image information to the training.
 13.  The method of claim 11 wherein training comprises training the artificial intelligence to output the second values as two or more of data consistency, transfer function, lighting, and viewing parameters.
 14.  The method of claim 11 wherein training comprises training the artificial intelligence to output the second values based on user selected viewing camera parameters, the rendering parameters for which second values are to be output being
other than the viewing camera parameters.
 15.  The method of claim 11 wherein training so the second rendered image is modeled after the first rendered image comprises training with a metric of similarity between the first rendered image and the second rendered image.
 16.  The method of claim 11 wherein the first image comprises a reference photograph or video of the patient.
 17.  The method of claim 11 wherein inputting comprises perturbing the rendering parameters, creating a collection of sets of the rendering parameters, and wherein training comprises training based on selection of a sub-set of the sets.
 18.  The method of claim 17 wherein selection of the sub-set comprises selection by a user based on visual examination of images rendered using the sets.
 19.  The method of claim 11 wherein machine training comprises deep learning with regression, classification, or reinforcement learning.
 20.  The method of claim 19 wherein machine training comprises deep reinforcement learning with a similarity of the second rendered image to the first image as a reinforcement.
 21.  The method of claim 20 wherein deep reinforcement learning comprises selecting with a probability distribution of different similarities including the similarity.
 22.  A system for content-based rendering based on machine learning, the system comprising: a medical scanner configured to scan a patient;  a machine configured to output settings for rendering parameters by application of data from the scan to
a machine-learnt model, the rendering parameters being controls for performing rendering from a volume to a two-dimensional image, and the settings learned to provide a first image from the data similar to one or more second images for a same diagnostic
context;  and a graphics processing unit configured to render the first image from the data using the settings output by the application of the data to the machine-learnt model, the first image being a two-dimensional representation.
 23.  The system of claim 22 wherein the machine-learnt model is machine learnt with deep learning.
 24.  The system of claim 22 wherein the rendering parameters comprise material properties, viewing properties, lighting properties, windowing properties, and internal renderer properties, and wherein the graphics processing unit is configured to
render with path tracing using the settings.  "
"10,341,390","
     July 2, 2019
","Aggregation of asynchronous trust outcomes in a mobile device
"," Systems and techniques are provided for aggregation of asynchronous trust
     outcomes in a mobile device. Trust levels may be determined from the
     signals. Each trust level may be determined independently of any other
     trust level. Each trust level may be determined based on applying to the
     signals heuristics, mathematical optimization, decisions trees, machine
     learning systems, or artificial intelligence systems. An aggregated trust
     outcome may be determined by aggregating the trust levels. Aggregating
     the trust levels may include applying heuristics, mathematical
     optimization, decisions trees, machine learning systems, or artificial
     intelligence systems to the trust levels, and wherein the aggregated
     trust outcome; and sending the aggregated trust outcome to be implemented
     by the enabling, disabling, or relaxing of at least one security measure
     based on the aggregated trust outcome.
",H04L 63/20 (20130101); H04W 12/06 (20130101); H04W 12/30 (20210101); H04W 88/02 (20130101); H04W 12/65 (20210101); H04W 12/63 (20210101),H04W 12/06 (20090101); H04L 29/06 (20060101); H04W 88/02 (20090101); H04W 12/00 (20090101),"[['\n8387141', '\nFebruary 2013'], ['\n8412158', '\nApril 2013'], ['\n8713704', '\nApril 2014'], ['\n9633184', '\nApril 2017'], ['\n10148692', '\nDecember 2018'], ['\n2006/0074986', '\nApril 2006'], ['\n2007/0150745', '\nJune 2007'], ['\n2008/0101658', '\nMay 2008'], ['\n2011/0016534', '\nJanuary 2011'], ['\n2012/0007713', '\nJanuary 2012'], ['\n2013/0055348', '\nFebruary 2013'], ['\n2013/0061305', '\nMarch 2013'], ['\n2013/0067566', '\nMarch 2013'], ['\n2013/0227678', '\nAugust 2013'], ['\n2014/0010417', '\nJanuary 2014'], ['\n2014/0033326', '\nJanuary 2014'], ['\n2014/0096231', '\nApril 2014'], ['\n2014/0289820', '\nSeptember 2014'], ['\n2014/0289833', '\nSeptember 2014'], ['\n2014/0380424', '\nDecember 2014']]",[0]," The invention claimed is:  1.  A computer-implemented method performed by a computing device, the method comprising: receiving one or more signals from one or more sensors, the one or more
sensors comprising at least one hardware sensor of the computing device;  determining at least a first trust level and a second trust level from the one or more signals, wherein the first trust level is determined without using the second trust level,
and wherein the second trust level is determined without using the first trust level;  determining a first granular aggregated trust outcome by aggregating at least the first trust level and the second trust level, wherein the first aggregated granular
trust outcome is associated with a first security measure of the computing device;  determining a second granular aggregated trust outcome by aggregating at least the first trust level and the second trust level, wherein the second aggregated granular
trust outcome is associated with a second security measure of the computing device that differs from the first security measure, wherein the first granular aggregated trust outcome is determined independently from the second granular aggregated trust
outcome;  modifying the first security measure based on the first granular aggregated trust outcome, wherein the second granular aggregated trust outcome is not used to modify the first security measure;  and modifying the second security measure based
on the second granular aggregated trust outcome, wherein the first granular aggregated trust outcome is not used to modify the second security measure.
 2.  The computer-implemented method of claim 1, wherein determining the at least the first trust level and the second trust level comprises applying data from at least one state, wherein the data comprises one or more of a historical trust level
and a historical value for one of the one or more signals used to determine the at least the first trust level and the second trust level.
 3.  The computer-implemented method of claim 1, wherein determining the first granular aggregated trust outcome comprises applying at least one configuration setting.
 4.  The computer-implemented method of claim 1, wherein the first granular aggregated trust outcome comprises applying data from a state, wherein the data comprises one or more of a historical trust level and a historical aggregated trust
outcome.
 5.  The computer-implemented method of claim 1, further comprising: determining a global aggregated trust outcome by aggregating at least the first trust level and the second trust level.
 6.  The computer-implemented method of claim 5, wherein the global aggregated trust outcome indicates whether a user of the computing device is authorized to use the computing device.
 7.  The computer-implemented method of claim 6, wherein the global aggregated trust outcome indicates a confidence level that the user of the computing device is authorized to use the computing device.
 8.  The computer-implemented method of claim 1, wherein the first trust level indicates a confidence level that a user of the computing device is an authorized user of the computing device, and wherein the confidence level is based on analyzing
the one or more signals.
 9.  The computer-implemented method of claim 1, wherein the first security measure comprises a request for credentials to unlock the computing device.
 10.  A computer-implemented method performed by a computing device, the method comprising: receiving a plurality of signals from a plurality of sensors, the plurality of sensors comprising hardware and software sensors of the computing device; 
determining a plurality of trust levels from the plurality of signals, wherein each of the plurality of trust levels is determined based on one or more of the plurality of signals independently from any other trust level of the plurality of trust levels,
and wherein the plurality of trust levels comprises a first trust level and a second trust level;  determining a first granular aggregated trust outcome by aggregating at least the first trust level and the second trust level, wherein the first
aggregated granular trust outcome is associated with a first security measure of the computing device;  determining a second granular aggregated trust outcome by aggregating at least the first trust level and the second trust level, wherein the second
aggregated granular trust outcome is associated with a second security measure different from the first security measure, wherein the first granular aggregated trust outcome is determined without using the second granular aggregated trust outcome, and
wherein the second granular aggregated trust outcome is determined without using the first granular aggregated trust outcome;  modifying the first security measure based on the first granular aggregated trust outcome, wherein the first security measure
is not associated with the second granular aggregated trust outcome;  and modifying the second security measure based on the second granular aggregated trust outcome, wherein the second security measure is not associated with the first granular
aggregated trust outcome.
 11.  The computer-implemented method of claim 10, wherein each of the plurality of trust levels is determined by a trustlet.
 12.  The computer-implemented method of claim 11, wherein at least one trustlet determines one of the plurality of trust levels based in part on a state associated with the trustlet.
 13.  The computer-implemented method of claim 10, wherein aggregating the plurality of trust levels to determine the first granular aggregated trust outcome comprises aggregating the plurality of trust levels to determine the first granular
aggregated trust outcome using a trust aggregator.
 14.  The computer-implemented method of claim 13, wherein aggregating the plurality of trust levels to determine the first granular aggregated trust outcome using the trust aggregator comprises aggregating the plurality of trust levels to
determine the first granular aggregated trust outcome based in part on a state of the trust aggregator.
 15.  The computer-implemented method of claim 13, wherein aggregating the plurality of trust levels to determine the first granular aggregated trust outcome using the trust aggregator comprises aggregating the plurality of trust levels to
determine the first granular aggregated trust outcome based in part on configuration settings for the trust aggregator.
 16.  The computer-implemented method of claim 10, wherein each of the plurality of trust levels indicates a confidence level that a user of the computing device is an authorized user of the computing device, and wherein the confidence level is
based on analyzing the plurality of signals.
 17.  The computer-implemented method of claim 10, wherein the first granular aggregated trust outcome comprises a confidence level that the computing device is in a secure environment.
 18.  The computer-implemented method of claim 10, further comprising: enabling at least one security measure of the computing device when the first granular aggregated trust outcome is below a threshold.
 19.  The computer-implemented method of claim 10, further comprising: disabling at least one security measure of the computing device when the first granular aggregated trust outcome is above a threshold.
 20.  A computing device, comprising: one or more sensors, comprising at least one hardware sensor;  one or more processors;  and one or more storage devices storing instructions that are operable, when executed by the one or more processors, to
cause the computing device to perform operations comprising: receiving one or more signals from the one or more sensors;  determining at least a first trust level and a second trust level from the one or more signals, wherein the first trust level is
determined without using the second trust level, and wherein the second trust level is determined without using the first trust level;  determining a first granular aggregated trust outcome by aggregating at least the first trust level and the second
trust level, wherein the first aggregated granular trust outcome is associated with a first security measure of the computing device;  determining a second granular aggregated trust outcome by aggregating at least the first trust level and the second
trust level, wherein the second aggregated granular trust outcome is associated with a second security measure of the computing device that differs from the first security measure, wherein the first granular aggregated trust outcome is determined
independently from the second granular aggregated trust outcome;  modifying the first security measure based on the first granular aggregated trust outcome, wherein the second granular aggregated trust outcome is not used to modify the first security
measure;  and modifying the second security measure based on the second granular aggregated trust outcome, wherein the first granular aggregated trust outcome is not used to modify the second security measure. 
"
"10,341,430","
     July 2, 2019
","System and method for peer group detection, visualization and analysis in
     identity management artificial intelligence systems using cluster based
     analysis of network identity graphs
"," Systems and methods for graph based artificial intelligence systems for
     identity management systems are disclosed. Embodiments of the identity
     management systems disclosed herein may utilize a network graph approach
     to peer grouping of identities of distributed networked enterprise
     computing environment. Specifically, in certain embodiments, data on the
     identities and the respective entitlements assigned to each identity as
     utilized in an enterprise computer environment may be obtained by an
     identity management system. A network identity graph may be constructed
     using the identity and entitlement data. The identity graph can then be
     clustered into peer groups of identities. The peer groups of identities
     may be used by the identity management system and users thereof in risk
     assessment or other identity management tasks.
",G06F 16/355 (20190101); H04L 67/1074 (20130101); G06F 16/345 (20190101); G06F 21/45 (20130101); H04L 67/1044 (20130101); H04L 63/08 (20130101); G06N 5/003 (20130101); H04L 63/104 (20130101); G06F 16/906 (20190101); H04L 41/22 (20130101); H04L 63/10 (20130101); H04L 63/1433 (20130101); H04L 63/20 (20130101); G06F 16/9024 (20190101); G06F 21/57 (20130101); H04L 63/1425 (20130101); G06N 20/00 (20190101),G06F 15/173 (20060101); H04L 29/08 (20060101); G06F 16/906 (20190101); G06F 16/34 (20190101); H04L 29/06 (20060101); G06F 16/901 (20190101); G06F 16/35 (20190101); G06N 20/00 (20190101),"[['\n8065712', '\nNovember 2011'], ['\n8683546', '\nMarch 2014'], ['\n9992230', '\nJune 2018'], ['\n2008/0288330', '\nNovember 2008'], ['\n2010/0274815', '\nOctober 2010'], ['\n2014/0207813', '\nJuly 2014'], ['\n2015/0128211', '\nMay 2015'], ['\n2016/0294645', '\nOctober 2016'], ['\n2016/0294646', '\nOctober 2016'], ['\n2017/0220964', '\nAugust 2017']]","[18, '11,295,241', '11,227,055', '11,201,865', '11,196,804', '11,196,775', '11,122,050', '11,038,927', '10,880,336', '10,862,928', '10,848,499', '10,791,170', '10,749,910', '10,681,056', '10,581,851', '10,554,665', '10,523,682', '10,476,953', '10,476,952']"," What is claimed is:  1.  An identity management system, comprising: a graph data store;  a processor;  a non-transitory, computer-readable storage medium including computer instructions for:
obtaining identity management data from one or more identity management systems in a distributed enterprise computing environment, the identity management data comprising data on a set of identities and a set of entitlements associated with the set of
identities utilized in identity management in the distributed enterprise computing environment;  evaluating the identity management data to determine the set of identities and a set of entitlements associated with the set of identities;  generating a
first identity graph from the identity management data by: creating a node of the first identity graph for each of the determined set of identities, for each first identity and second identity that share at least one entitlement of the set of
entitlements, creating an edge of the first identity graph between a first node representing the first identity and a second node of the identity graph representing the second identity, and generating a similarity weight for each edge of the first
identity graph between each first node and second node based on a number of the set of entitlements shared between the first identity represented by the first node and the second identity represented by the second node;  storing the first identity graph
in the graph data store;  pruning the set of edges of the first identity graph to generate a second identity graph based on the similarity weight associated with each edge of the first identity graph and a pruning threshold;  storing the second identity
graph in the graph data store;  clustering the set of identities represented by the nodes of the second identity graph into a set of peer groups based on the second identity graph, including the nodes of the second identity graph representing the set of
identities, the edges of the second identity graph and the similarity weights of each of the edges of the second identity graph;  associating each of the set of identities with a corresponding peer group;  and generating an interface based on the second
identity graph and the association between each of the set of identities and the corresponding peer group.
 2.  The system of claim 1, wherein the instructions are further for: determining a peer group assessment metric based on the set of peer groups and comparing the peer group assessment metric to a quality threshold, and when the peer group
assessment metric is below the quality threshold: a) adjusting the pruning threshold;  b) pruning the set of edges of the first identity graph to generate the second identity graph based on the similarity weight associated with each edge of the first
identity graph and the adjusted pruning threshold;  c) clustering the set of identities represented by the nodes of the second identity graph into a set of peer groups based on the second identity graph, including the nodes of the second identity graph
representing the set of identities, the edges of the second identity graph and the similarity weights of each of the edges of the second identity graph;  d) determining the peer group assessment metric and comparing the peer group assessment metric to
the quality threshold;  and e) repeating steps a-d until the peer group assessment metric exceeds the quality threshold.
 3.  The system of claim 2, wherein the instructions are further for adjusting the pruning threshold a first amount a first time steps a-d are performed and adjusting the pruning threshold a second amount a second time steps a-d are performed.
 4.  The system of claim 3, wherein the pruning threshold is adjusted up or down.
 5.  The system of claim 1, wherein clustering the second identity graph is performed using Louvain community detection.
 6.  The system of claim 1, wherein the peer group assessment metric is a modularity value resulting from clustering of the set of identities.
 7.  The system of claim 1, wherein the peer group assessment metric is a user specified criteria.
 8.  A method, comprising: obtaining identity management data from one or more identity management systems in a distributed enterprise computing environment, the identity management data comprising data on a set of identities and a set of
entitlements associated with the set of identities utilized in identity management in the distributed enterprise computing environment;  evaluating the identity management data to determine the set of identities and a set of entitlements associated with
the set of identities;  generating a first identity graph from the identity management data by: creating a node of the first identity graph for each of the determined set of identities, for each first identity and second identity that share at least one
entitlement of the set of entitlements, creating an edge of the first identity graph between a first node representing the first identity and a second node of the identity graph representing the second identity, and generating a similarity weight for
each edge of the first identity graph between each first node and second node based on a number of the set of entitlements shared between the first identity represented by the first node and the second identity represented by the second node;  storing
the first identity graph in a graph data store;  pruning the set of edges of the first identity graph to generate a second identity graph based on the similarity weight associated with each edge of the first identity graph and a pruning threshold; 
storing the second identity graph in the graph data store;  clustering the set of identities represented by the nodes of the second identity graph into a set of peer groups based on the second identity graph, including the nodes of the second identity
graph representing the set of identities, the edges of the second identity graph and the similarity weights of each of the edges of the second identity graph;  associating each of the set of identities with a corresponding peer group;  and generating an
interface based on the second identity graph and the association between each of the set of identities and the corresponding peer group.
 9.  The method of claim 8, further comprising: determining a peer group assessment metric based on the set of peer groups and comparing the peer group assessment metric to a quality threshold, and when the peer group assessment metric is below
the quality threshold: a) adjusting the pruning threshold;  b) pruning the set of edges of the first identity graph to generate the second identity graph based on the similarity weight associated with each edge of the first identity graph and the
adjusted pruning threshold;  c) clustering the set of identities represented by the nodes of the second identity graph into a set of peer groups based on the second identity graph, including the nodes of the second identity graph representing the set of
identities, the edges of the second identity graph and the similarity weights of each of the edges of the second identity graph;  d) determining the peer group assessment metric and comparing the peer group assessment metric to the quality threshold; 
and e) repeating steps a-d until the peer group assessment metric exceeds the quality threshold.
 10.  The method of claim 9, further comprising adjusting the pruning threshold a first amount a first time steps a-d are performed and adjusting the pruning threshold a second amount a second time steps a-d are performed.
 11.  The method of claim 10, wherein the pruning threshold is adjusted up or down.
 12.  The method of claim 8, wherein clustering the second identity graph is performed using Louvain community detection.
 13.  The method of claim 8, wherein the peer group assessment metric is a modularity value resulting from clustering of the set of identities.
 14.  The method of claim 8, wherein the peer group assessment metric is a user specified criteria.
 15.  A non-transitory computer readable medium, comprising instructions for: obtaining identity management data from one or more identity management systems in a distributed enterprise computing environment, the identity management data
comprising data on a set of identities and a set of entitlements associated with the set of identities utilized in identity management in the distributed enterprise computing environment;  evaluating the identity management data to determine the set of
identities and a set of entitlements associated with the set of identities;  generating a first identity graph from the identity management data by: creating a node of the first identity graph for each of the determined set of identities, for each first
identity and second identity that share at least one entitlement of the set of entitlements, creating an edge of the first identity graph between a first node representing the first identity and a second node of the identity graph representing the second
identity, and generating a similarity weight for each edge of the first identity graph between each first node and second node based on a number of the set of entitlements shared between the first identity represented by the first node and the second
identity represented by the second node;  storing the first identity graph in a graph data store;  pruning the set of edges of the first identity graph to generate a second identity graph based on the similarity weight associated with each edge of the
first identity graph and a pruning threshold;  storing the second identity graph in the graph data store;  clustering the set of identities represented by the nodes of the second identity graph into a set of peer groups based on the second identity
graph, including the nodes of the second identity graph representing the set of identities, the edges of the second identity graph and the similarity weights of each of the edges of the second identity graph;  associating each of the set of identities
with a corresponding peer group;  and generating an interface based on the second identity graph and the association between each of the set of identities and the corresponding peer group.
 16.  The non-transitory computer readable medium of claim 15, wherein the instructions are further for: determining a peer group assessment metric based on the set of peer groups and comparing the peer group assessment metric to a quality
threshold, and when the peer group assessment metric is below the quality threshold: a) adjusting the pruning threshold;  b) pruning the set of edges of the first identity graph to generate the second identity graph based on the similarity weight
associated with each edge of the first identity graph and the adjusted pruning threshold;  c) clustering the set of identities represented by the nodes of the second identity graph into a set of peer groups based on the second identity graph, including
the nodes of the second identity graph representing the set of identities, the edges of the second identity graph and the similarity weights of each of the edges of the second identity graph;  d) determining the peer group assessment metric and comparing
the peer group assessment metric to the quality threshold;  and e) repeating steps a-d until the peer group assessment metric exceeds the quality threshold.
 17.  The non-transitory computer readable medium of claim 16, wherein the instructions are further for adjusting the pruning threshold a first amount a first time steps a-d are performed and adjusting the pruning threshold a second amount a
second time steps a-d are performed.
 18.  The non-transitory computer readable medium of claim 17, wherein the pruning threshold is adjusted up or down.
 19.  The non-transitory computer readable medium of claim 15, wherein clustering the second identity graph is performed using Louvain community detection.
 20.  The non-transitory computer readable medium of claim 15, wherein the peer group assessment metric is a modularity value resulting from clustering of the set of identities.
 21.  The non-transitory computer readable medium of claim 15, wherein the peer group assessment metric is a user specified criteria.  "
"10,346,782","
     July 9, 2019
","Adaptive augmented decision engine
"," Techniques are described for adaptive and augmented decision making by an
     artificial intelligence (AI) engine, such as an engine that employs
     machine learning techniques. A decision-making process may be executed to
     make a decision regarding operations of the organization, and the AI
     engine may be employed to analyze the various aspects of a decision and
     determine a risk level associated with the decision. The risk level may
     be a combination of the probability of a negative outcome and a magnitude
     of loss that may occur due to a negative outcome. The automated process
     may also determine a confidence level that indicates a degree of
     confidence in the determined risk level. Risk and confidence may be
     independent values. Implementations may enable risk mitigation by
     providing a risk estimate with higher confidence than traditional
     methods.
",G06Q 10/0635 (20130101); G06Q 10/063 (20130101),G06Q 10/06 (20120101),"[['\n2010/0271931', '\nOctober 2010'], ['\n2013/0346496', '\nDecember 2013'], ['\n2015/0229665', '\nAugust 2015'], ['\n2015/0339477', '\nNovember 2015'], ['\n2016/0104163', '\nApril 2016'], ['\n2017/0296085', '\nOctober 2017']]",[0]," What is claimed is:  1.  A computer-implemented method performed by at least one computing device, the method comprising: providing, by the computing device, decision information to an artificial
intelligence (AI) engine executing on the at least one computing device, and receiving a decision that is generated by the AI engine based on the decision information;  providing, by the AI engine executing on the at least one computing device, the
decision information and the decision as input to a risk evaluation model stored on the at least one computing device and receiving from the risk evaluation model: i) a risk level that is calculated by the risk evaluation model and that measures a risk
associated with the decision, and ii) a confidence level that is calculated by the risk evaluation model and that measures a confidence in the calculated risk level;  responsive to determining, by a decision analysis module executing on the at least one
computing device, that the risk level is greater than a risk threshold and the confidence level is less than a confidence threshold: communicating, by the decision analysis module, the decision information and the decision to be reviewed by a first set
of reviewers including multiple reviewers characterized by at least two different expertise key words;  receiving, by the AI engine executing on the at least one computing device, recommendations from the first set of reviewers regarding the decision
information;  and responsive to determining that the recommendations indicate a consensus among the first set of reviewers, modifying, by the AI engine executing on the at least one computing device, the risk evaluation model based on first feedback
information including the recommendations from the first set of reviewers, the risk level, and the confidence level;  and responsive to determining, by the decision analysis module executing on the at least one computing device, that the risk level is
less than the risk threshold and the confidence level is within a predetermined range of the confidence threshold: communicating, by the decision analysis module, a summary of the decision information for the decision to a second set of reviewers;  and
receiving votes from the second set of reviewers recommending a full review and, in response, modifying, by the AI engine, the risk evaluation model based on second feedback information including the risk level, the confidence level, and the votes
recommending the full review.
 2.  The method of claim 1, further comprising: determining, by the at least one processor, a variance among the recommendations from the first set of reviewers;  wherein the first feedback information further includes the variance.
 3.  The method of claim 1, wherein the first feedback information further includes at least one monitored outcome of the decision.
 4.  The method of claim 1, further comprising: generating, by the at least one processor, the summary by performing natural language processing (NLP) on the decision information.
 5.  The method of claim 1, further comprising: receiving, by the at least one processor, from the second set of reviewers, at least one expertise key word that characterizes the reviewers to conduct the full review;  and communicating, by the at
least one processor, the decision information for the decision to be reviewed by at least one reviewer characterized by the one or more expertise key words;  wherein the second feedback information further includes the at least one expertise key word
received from the second set of reviewers.
 6.  The method of claim 1, wherein the second set of reviewers is greater in number than the first set of reviewers.
 7.  A system, comprising: at least one processor;  and a memory communicatively coupled to the at least one processor, the memory storing instructions which, when executed by the at least one processor, cause the at least one processor to
perform operations comprising: providing decision information to an artificial intelligence (AI) engine and receiving a decision that is generated by the AI engine based on the decision information;  providing, by the AI engine, the decision information
and the decision as input to a risk evaluation model and receiving from the risk evaluation model: i) a risk level that is calculated by the risk evaluation model and that measures a risk associated with the decision, and ii) a confidence level that is
calculated by the risk evaluation model and that measures a confidence in the calculated risk level;  responsive to determining, by a decision analysis module, that the risk level is greater than a risk threshold and the confidence level is less than a
confidence threshold: communicating, by the decision analysis module, the decision information and the decision to be reviewed by a first set of reviewers including multiple reviewers characterized by at least two different expertise key words; 
receiving, by the AI engine, recommendations from the first set of reviewers regarding the decision information;  and responsive to determining that the recommendations indicate a consensus among the first set of reviewers, modifying, by the AI engine,
the risk evaluation model based on first feedback information including the recommendations from the first set of reviewers, the risk level, and the confidence level;  and responsive to determining, by the decision analysis module, that the risk level is
less than the risk threshold and the confidence level is within a predetermined range of the confidence threshold: communicating, by the decision analysis module, a summary of the decision information for the decision to a second set of reviewers;  and
receiving votes from the second set of reviewers recommending a full review and, in response, modifying, by the AI engine, the risk evaluation model based on second feedback information including the risk level, the confidence level, and the votes
recommending the full review.
 8.  The system of claim 7, the operations further comprising: determining a variance among the recommendations from the first set of reviewers;  wherein the first feedback information further includes the variance.
 9.  The system of claim 7, wherein the first feedback information further includes at least one monitored outcome of the decision.
 10.  The system of claim 7, the operations further comprising: generating the summary by performing natural language processing (NLP) on the decision information.
 11.  The system of claim 7, the operations further comprising: receiving from the second set of reviewers, at least one expertise key word that characterizes the reviewers to conduct the full review;  and communicating the decision information
for the decision to be reviewed by at least one reviewer characterized by the one or more expertise key words;  wherein the second feedback information further includes the at least one expertise key word received from the second set of reviewers.
 12.  The system of claim 7, wherein the second set of reviewers is greater in number than the first set of reviewers.
 13.  One or more non-transitory computer-readable media storing instructions which, when executed by at least one processor, cause the at least one processor to perform operations comprising: providing decision information to an artificial
intelligence (AI) engine and receiving a decision that is generated by the AI engine based on the decision information;  providing, by the AI engine, the decision information and the decision as input to a risk evaluation model and receiving from the
risk evaluation model: i) a risk level that is calculated by the risk evaluation model and that measures a risk associated with the decision, and ii) a confidence level that is calculated by the risk evaluation model and that measures a confidence in the
calculated risk level;  responsive to determining, by a decision analysis module, that the risk level is greater than a risk threshold and the confidence level is less than a confidence threshold: communicating, by the decision analysis module, the
decision information and the decision to be reviewed by a first set of reviewers including multiple reviewers characterized by at least two different expertise key words;  receiving, by the AI engine, recommendations from the first set of reviewers
regarding the decision information;  and responsive to determining that the recommendations indicate a consensus among the first set of reviewers, modifying, by the AI engine, the risk evaluation model based on first feedback information including the
recommendations from the first set of reviewers, the risk level, and the confidence level;  and responsive to determining, by the decision analysis module, that the risk level is less than the risk threshold and the confidence level is within a
predetermined range of the confidence threshold: communicating, by the decision analysis module, a summary of the decision information for the decision to a second set of reviewers;  and receiving votes from the second set of reviewers recommending a
full review and, in response, modifying, by the AI engine, the risk evaluation model based on second feedback information including the risk level, the confidence level, and the votes recommending the full review.
 14.  The one or more non-transitory computer-readable media of claim 13, the operations further comprising: determining a variance among the recommendations from the first set of reviewers;  wherein the first feedback information further
includes the variance.
 15.  The one or more non-transitory computer-readable media of claim 13, wherein the first feedback information further includes at least one monitored outcome of the decision.
 16.  The one or more non-transitory computer-readable media of claim 13, the operations further comprising: generating the summary by performing natural language processing (NLP) on the decision information.
 17.  The one or more non-transitory computer-readable media of claim 13, the operations further comprising: receiving from the second set of reviewers, at least one expertise key word that characterizes the reviewers to conduct the full review; 
and communicating the decision information for the decision to be reviewed by at least one reviewer characterized by the one or more expertise key words;  wherein the second feedback information further includes the at least one expertise key word
received from the second set of reviewers.  "
"10,347,317","
     July 9, 2019
","Method of self-testing and reusing of reference cells in a memory
     architecture
"," An integrated circuit includes an artificial intelligence (AI) logic and
     an embedded memory coupled to the AI logic and connectable to an external
     processor. The embedded memory includes multiple storage cells and
     multiple reference units. One or more reference units in the memory are
     selected for memory access through configuration at chip packaging level
     by the external processor. The external processor may execute a self-test
     process to select or update the one or more reference units for memory
     access so that the error rate of memory is below a threshold. The
     self-test process may be performed, via a memory initialization
     controller in the memory, to test and reuse the reference cells in the
     memory at chip level. The embedded memory may be a STT-MRAM, SOT, OST
     MRAM, and/or MeRAM memory.
",G11C 11/02 (20130101); G11C 11/4078 (20130101); G11C 29/24 (20130101); G11C 29/08 (20130101); G06N 20/00 (20190101); G11C 29/028 (20130101); G11C 11/16 (20130101); G11C 2029/0401 (20130101); G06N 3/0454 (20130101); G11C 2029/1206 (20130101); G11C 29/46 (20130101); G11C 2029/4402 (20130101),G11C 11/4078 (20060101); G11C 11/02 (20060101); G06N 20/00 (20190101),"[['\n6396742', '\nMay 2002'], ['\n7020022', '\nMarch 2006'], ['\n7773413', '\nAugust 2010'], ['\n8488357', '\nJuly 2013'], ['\n9104815', '\nAugust 2015'], ['\n2004/0136236', '\nJuly 2004'], ['\n2004/0218440', '\nNovember 2004'], ['\n2015/0092469', '\nApril 2015'], ['\n2015/0187440', '\nJuly 2015'], ['\n2017/0186472', '\nJune 2017']]",[1]," The invention claimed is:  1.  A method of testing reference cells in a magnetoresistive random access memory (MRAM) memory comprising a plurality of storage cells and a plurality of reference
units, each reference unit comprising one or more reference cells, the method comprising: (i) determining a number of reference units from the plurality of reference units according to a pattern to form a set of reference cells, wherein the pattern
defines which reference unit in the plurality of reference units is to be selected;  (ii) programming a subset of the set of reference cells with a value of zero and a remaining subset of the set of reference cells with a value of one;  (iii) using the
set of reference cells to test the plurality of storage cells in the MRAM memory to determine an error rate;  (iv) determining whether the error rate exceeds an error threshold;  (v) upon determining that the error rate of the plurality of storage cells
in the MRAM memory exceeds the error threshold: updating the pattern, updating the set of reference cells by selecting a number of reference units from the plurality of reference units according to the updated pattern, and repeating the steps of
(ii)-(v), otherwise: setting the set of reference cells as selected reference cells, and storing reference cell information indicative of which cell in the set of reference cells has a value of zero and which has a value of one.
 2.  The method of claim 1, wherein the error threshold is defined by an artificial intelligence application that executes in the MRAM memory.
 3.  The method of claim 2, wherein the MRAM memory stores a cellular neural network (CNN) for executing the artificial intelligence application.
 4.  The method of claim 2, wherein testing the plurality of storage cells in the MRAM memory to determine the error rate comprises: writing a value of one to the plurality of storage cells and comparing each of the plurality of storage cells
with a reference cell in the set of reference cells to determine a first error rate;  writing a value of zero to the plurality of storage cells and comparing each of the plurality of storage cells with a reference cell in the set of reference cells to
determine a second error rate;  and determining the error rate by adding the first error rate and the second error rate.
 5.  The method of claim 1, wherein the MRAM memory is a spin transfer torque (STT) memory, spin orbit torque (SOT), orthogonal spin transfer (OST) MRAM, magnetoelectric RAM (MeRAM), or a combination thereof.
 6.  The method of claim 1, wherein the reference unit is a reference column, a reference row, or a reference array comprising multiple rows and multiple columns.
 7.  The method of claim 1, wherein programming the subset and the remaining subset of the set of reference cells comprises randomly storing a value of zero or one in each of the set of reference cells so that the subset contains approximately
half of the set of reference cells.
 8.  The method of claim 1, wherein updating the pattern comprises at least one of: shifting a previous pattern by one or more reference units so that the updated pattern and the previous pattern contain identical number of reference units; 
subtracting one or more reference units from the previous pattern;  or adding one or more reference units from the previous pattern.
 9.  The method of claim 1, wherein the steps (i)-(v) are performed at a chip packaging level.
 10.  The method of claim 9, wherein the steps (ii)-(iii) are performed during an initialization process of the MRAM memory via a memory initialization controller of the MRAM memory, the memory initialization controller is configured to: receive
timing signals at one or more trim bits;  receive data signals at one or more trim input bits;  in response to receiving the timing signals, latching the data signals to at least a portion of the plurality of storage cells and/or a portion of the
plurality of reference cells to which the one or more trim input bits correspond.
 11.  The method of claim 1 further comprising reusing reference cells in the MRAM memory by: (i) using the reference cell information to program a subset of the set of reference cells with a value of zero and a remaining subset of the set of
reference cells with a value of one;  (ii) using the set of reference cells to test the plurality of storage cells in the MRAM memory to determine an error rate;  (iii) determining whether the error rate exceeds an error threshold;  and (iv) upon
determining that the error rate of the plurality of storage cells in the MRAM memory exceeds the error threshold: updating the pattern that was used to form the set of reference cells, updating the set of reference cells by selecting a number of
reference units from the plurality of reference units according to the updated pattern, and setting the set of reference cells as selected reference cells.
 12.  An integrated circuit comprising: an artificial intelligence (AI) logic;  and an embedded MRAM memory coupled to the AI logic and connectable to an external processor, the embedded MRAM memory comprising: a plurality of storage cells;  and
a plurality of reference units, each reference unit comprising one or more reference cells, wherein one or more reference units are selected for memory access through configuration at a chip packaging level by the external processor which performs the
steps of: (i) determining a number of reference units from the plurality of reference units according to a pattern to form a set of reference cells, wherein the pattern defines which reference unit in the plurality of reference units is to be selected; 
(ii) programming a subset of the set of reference cells with a value of zero and a remaining subset of the set of reference cells with a value of one;  (iii) using the set of reference cells to test the plurality of storage cells in the MRAM memory to
determine an error rate;  (iv) determining whether the error rate exceeds an error threshold;  (v) upon determining that the error rate of the plurality of storage cells in the MRAM memory exceeds the error threshold: updating the pattern, updating the
set of reference cells by selecting a number of reference units from the plurality of reference units according to the updated pattern, and repeating the steps of (ii)-(v), otherwise: setting the set of reference cells as selected reference cells, and
storing reference cell information indicative of which cell in the set of reference cells has a value of zero and which has a value of one.
 13.  The integrated circuit of claim 12, wherein the error threshold is defined by an artificial intelligence application that executes in the AI logic of the integrated circuit.
 14.  The integrated circuit of claim 12, wherein the MRAM memory stores a cellular neural network (CNN) for executing the artificial intelligence application.
 15.  The integrated circuit of claim 12, wherein the step of testing the plurality of storage cells in configuring the plurality of reference cells in the memory comprises: writing a value of one to the plurality of storage cells and comparing
each of the plurality of storage cells with one or more reference cells to determine a first error rate;  writing a value of zero to the plurality of storage cells and comparing each of the plurality of storage cells with one or more reference cells to
determine a second error rate;  and determining the error rate by adding the first error rate and the second error rate.
 16.  The integrated circuit of claim 12, wherein the embedded MRAM memory is a spin transfer torque (STT), a spin orbit torque (SOT), orthogonal spin transfer (OST) MRAM, magnetoelectric RAM (MeRAM), or a combination thereof.
 17.  The integrated circuit of claim 12, wherein the reference unit is a reference column, a reference row, or a reference array comprising multiple rows and multiple columns.
 18.  The integrated circuit of claim 12, wherein the selected reference cells contain approximately equal number of zero cells and one cells.
 19.  The integrated circuit of claim 12, wherein updating the pattern comprising at least one of: shifting a previous pattern by one or more reference units so that the updated pattern and the previous pattern contain identical number of
reference units;  subtracting one or more reference units from the previous pattern;  or adding one or more reference units from the previous pattern.
 20.  The integrated circuit of claim 12 further comprising a memory initialization controller comprising one or more trim bits and one or more trim input bits, wherein the steps (ii)-(iii) are performed during an initialization process of the
MRAM memory via the memory initialization controller, the memory initialization controller is configured to: receive timing signals at one or more trim bits;  receive data signals at one or more trim input bits;  in response to receiving the timing
signals, latching the data signals to at least a portion of the plurality of storage cells and/or a portion of the plurality of reference cells to which the one or more trim input bits correspond.  "
"10,349,126","
     July 9, 2019
","Method and apparatus for filtering video
"," An artificial intelligence (AI) system for simulating functions such as
     recognition, determination, and so forth of a human brain by using a
     mechanical learning algorithm such as deep learning, or the like, and an
     application thereof are provided. A method of filtering video by a device
     is provided. The method includes selecting at least one previous frame
     preceding a current frame being played from among a plurality of frames
     included in the video, generating metadata regarding the selected at
     least one previous frame, predicting harmfulness of at least one next
     frame to be displayed on the device after playback of the current frame,
     based on the generated metadata, and filtering the next frame based on
     the predicted harmfulness.
",H04N 21/23418 (20130101); H04N 21/44016 (20130101); H04N 21/4755 (20130101); G06V 20/47 (20220101); H04N 21/466 (20130101); G06V 20/41 (20220101); H04N 21/234345 (20130101); G06N 3/0445 (20130101); H04N 21/4532 (20130101); H04N 21/4542 (20130101); G06N 3/08 (20130101); G06N 5/025 (20130101); H04N 21/84 (20130101); H04N 21/44008 (20130101); G06N 5/027 (20130101),H04N 7/16 (20110101); H04N 21/44 (20110101); H04N 5/445 (20110101); G06F 13/00 (20060101); G06F 3/00 (20060101); H04N 21/84 (20110101); H04N 21/475 (20110101); H04N 21/466 (20110101); H04N 21/454 (20110101); H04N 21/45 (20110101); G06N 3/08 (20060101); G06N 5/02 (20060101); H04N 21/234 (20110101); H04N 21/2343 (20110101); G06K 9/00 (20060101),"[['\n2002/0097984', '\nJuly 2002'], ['\n2002/0147782', '\nOctober 2002'], ['\n2007/0168853', '\nJuly 2007'], ['\n2009/0274364', '\nNovember 2009'], ['\n2009/0288131', '\nNovember 2009'], ['\n2015/0221097', '\nAugust 2015'], ['\n2016/0057497', '\nFebruary 2016']]",[1]," What is claimed is:  1.  A method of filtering video by a device, the method comprising: selecting, by at least one processor, at least one previous frame preceding a current frame being played
from among a plurality of frames included in a video being played without filtering the current frame being played;  analyzing, by the at least one processor, at least one of image information or speech information included in the selected at least one
previous frame, while the video is being played;  generating, by the at least one processor, metadata based on the analysis of the selected at least one previous frame, while the video is played;  predicting, by the at least one processor, harmfulness of
at least one next frame to be displayed on the device, after playback of the current frame based on the generated metadata corresponding to the at least one previous frame, while the video is being played;  and filtering, by the at least one processor,
the at least one next frame based on the predicted harmfulness, while the video is being played.
 2.  The method of claim 1, wherein the predicting of the harmfulness of the at least one next frame comprises predicting the harmfulness of the at least one next frame for each of a plurality of categories for predicting the harmfulness.
 3.  The method of claim 2, wherein the predicting of the harmfulness of the at least one next frame further comprises: providing a graphic user interface (GUI) for setting a category for predicting the harmfulness, receiving a user input with
respect to the provided GUI, and setting a new category related to the harmfulness according to the received input.
 4.  The method of claim 1, wherein the filtering of the at least one next frame comprises: comparing a level of the predicted harmfulness with a preset filtering level, and filtering the at least one next frame based on a result of the
comparison.
 5.  The method of claim 4, further comprising: changing, by the at least one processor, the preset filtering level based on a user input, wherein the comparing comprises comparing the level of the predicted harmfulness with the changed filtering
level.
 6.  The method of claim 1, wherein the generating of the metadata comprises: determining a partial region of the selected at least one previous frame as a harmful region comprising harmful content, and generating the metadata for determining
harmfulness of the harmful region.
 7.  The method of claim 1, wherein the generating of the metadata comprises generating the metadata based on text information included in the selected at least one previous frame.
 8.  The method of claim 1, wherein the selecting of the at least one previous frame comprises selecting one or more adjacent previous frames having similarity based on a data variation between the one or more adjacent previous frames included in
the video.
 9.  The method of claim 1, wherein the filtering of the at least one next frame comprises lowering visibility of the at least one next frame or a partial region of the at least one next frame.
 10.  A device for filtering video, the device comprising: a display configured to play the video;  a memory configured to store at least one instruction;  and at least one processor, wherein the at least one processor, by executing the at least
one instruction, is configured to: select at least one previous frame preceding a current frame being played from among a plurality of frames included in a video being played, analyze at least one of image information or speech information included in
the selected at least one previous frame, while the video is being played, generate metadata based on the analysis of the selected at least one previous frame, while the video is played, predict harmfulness of at least one next frame to be displayed on
the device after playback of the current frame;  based on the generated metadata corresponding to the at least one previous frame, while the video is being played, and filter the at least one next frame based on the predicted harmfulness, while the video
is being played.
 11.  The device of claim 10, wherein the at least one processor is further configured to predict the harmfulness of the at least one next frame for each of a plurality of categories for predicting the harmfulness.
 12.  The device of claim 11, wherein the at least one processor is further configured to: provide a graphic user interface (GUI) for setting a category for predicting the harmfulness, receive a user input with respect to the provided GUI, and
set a new category related to the harmfulness according to the received input.
 13.  The device of claim 10, wherein the at least one processor is further configured to: compare a level of the predicted harmfulness with a preset filtering level, and filter the at least one next frame based on a result of the comparison.
 14.  The device of claim 13, wherein the at least one processor is further configured to: change the preset filtering level based on a user input, and compare the level of the predicted harmfulness with the changed filtering level.
 15.  The device of claim 10, wherein the at least one processor is further configured to: determine a partial region of the selected at least one previous frame as a harmful region comprising harmful content, and generate the metadata for
determining harmfulness of the harmful region.
 16.  The device of claim 10, wherein the at least one processor is further configured to generate the metadata based on text information included in the selected at least one previous frame.
 17.  The device of claim 10, wherein the at least one processor is further configured to select adjacent previous frames having similarity based on a data variation between the selected adjacent previous frames included in the video.
 18.  The device of claim 10, wherein the at least one processor is further configured to lower visibility of the at least one next frame or a partial region of the at least one next frame.
 19.  A non-transitory computer-readable recording medium having recorded thereon a program for executing the method of claim 1 on a computer.  "
"10,349,493","
     July 9, 2019
","Artificial intelligence (AI) computing device with one or more lighting
     elements
"," A lighting system including a lighting support frame and lighting fabric
     and a lighting support assembly connected to the lighting support frame
     and lighting fabric, the lighting support frame or the lighting support
     assembly including one or more lighting elements. The lighting system
     further includes an artificial intelligence (AI) device housing coupled
     to the lighting support assembly, the AI device housing including one or
     more microphones, one or more processors, one or more memory devices, and
     computer-readable instructions stored in the one or more memory devices
     and executable by the one or more processors to receive audible commands
     and convert the received audible commands to one or more sound files.
",H05B 47/12 (20200101); F21V 23/045 (20130101); F21S 9/037 (20130101); F21V 23/008 (20130101); G06N 5/04 (20130101); G06F 3/167 (20130101); H05B 47/19 (20200101); Y02B 20/40 (20130101); G10L 15/26 (20130101); F21V 21/26 (20130101),H05B 37/02 (20060101); G06F 3/16 (20060101); G10L 15/26 (20060101); F21S 9/03 (20060101),"[['\n2070045', '\nFebruary 1937'], ['\n2087537', '\nJuly 1937'], ['\n2960094', '\nNovember 1960'], ['\n4174532', '\nNovember 1979'], ['\n4684230', '\nAugust 1987'], ['\n4787019', '\nNovember 1988'], ['\n4915670', '\nApril 1990'], ['\n5007811', '\nApril 1991'], ['\n5029239', '\nJuly 1991'], ['\n5275364', '\nJanuary 1994'], ['\n5321579', '\nJune 1994'], ['\n5349975', '\nSeptember 1994'], ['\n5683064', '\nNovember 1997'], ['\n5979793', '\nNovember 1999'], ['\n5996511', '\nDecember 1999'], ['\n6017188', '\nJanuary 2000'], ['\n6027309', '\nFebruary 2000'], ['\n6113054', '\nSeptember 2000'], ['\n6134103', '\nOctober 2000'], ['\n6138970', '\nOctober 2000'], ['\n6158701', '\nDecember 2000'], ['\n6199570', '\nMarch 2001'], ['\n6298866', '\nOctober 2001'], ['\n6302560', '\nOctober 2001'], ['\n6347776', '\nFebruary 2002'], ['\n6374840', '\nApril 2002'], ['\n6412889', '\nJuly 2002'], ['\n6439249', '\nAugust 2002'], ['\n6446650', '\nSeptember 2002'], ['\n6488254', '\nDecember 2002'], ['\n6511033', '\nJanuary 2003'], ['\n6519144', '\nFebruary 2003'], ['\n6565060', '\nMay 2003'], ['\n6585219', '\nJuly 2003'], ['\n6598990', '\nJuly 2003'], ['\n6636918', '\nOctober 2003'], ['\n6666284', '\nDecember 2003'], ['\n6785789', '\nAugust 2004'], ['\n6840657', '\nJanuary 2005'], ['\n6959996', '\nJuly 2005'], ['\n6961237', '\nNovember 2005'], ['\n7017598', '\nMarch 2006'], ['\nD518629', '\nApril 2006'], ['\n7034902', '\nApril 2006'], ['\n7128076', '\nOctober 2006'], ['\n7134442', '\nNovember 2006'], ['\n7134762', '\nNovember 2006'], ['\n7143501', '\nDecember 2006'], ['\nD539632', '\nApril 2007'], ['\nD558444', '\nJanuary 2008'], ['\n7412985', '\nAugust 2008'], ['\n7493909', '\nFebruary 2009'], ['\n7497225', '\nMarch 2009'], ['\n7497583', '\nMarch 2009'], ['\n7533680', '\nMay 2009'], ['\n7559520', '\nJuly 2009'], ['\n7593220', '\nSeptember 2009'], ['\n7604015', '\nOctober 2009'], ['\n7628164', '\nDecember 2009'], ['\n7650230', '\nJanuary 2010'], ['\n7703464', '\nApril 2010'], ['\n7708022', '\nMay 2010'], ['\n7755970', '\nJuly 2010'], ['\n7778624', '\nAugust 2010'], ['\n7784761', '\nAugust 2010'], ['\n7798161', '\nSeptember 2010'], ['\nD626324', '\nNovember 2010'], ['\n7856996', '\nDecember 2010'], ['\n7861734', '\nJanuary 2011'], ['\n7891367', '\nFebruary 2011'], ['\n7900643', '\nMarch 2011'], ['\n7963293', '\nJune 2011'], ['\n8020572', '\nSeptember 2011'], ['\n8025071', '\nSeptember 2011'], ['\n8061375', '\nNovember 2011'], ['\n8066021', '\nNovember 2011'], ['\n8082935', '\nDecember 2011'], ['\nD660137', '\nMay 2012'], ['\n8166986', '\nMay 2012'], ['\n8205656', '\nJune 2012'], ['\n8251078', '\nAugust 2012'], ['\n8356613', '\nJanuary 2013'], ['\n8555905', '\nOctober 2013'], ['\n8555906', '\nOctober 2013'], ['\n8616226', '\nDecember 2013'], ['\nD697705', '\nJanuary 2014'], ['\n8632045', '\nJanuary 2014'], ['\n8657246', '\nFebruary 2014'], ['\n8919722', '\nJune 2014'], ['\nD719342', '\nDecember 2014'], ['\nD719343', '\nDecember 2014'], ['\n8919361', '\nDecember 2014'], ['\n8960625', '\nFebruary 2015'], ['\nD724309', '\nMarch 2015'], ['\n9030829', '\nMay 2015'], ['\nD731166', '\nJune 2015'], ['\n9078497', '\nJuly 2015'], ['\n9113683', '\nAugust 2015'], ['\nD738609', '\nSeptember 2015'], ['\nD738610', '\nSeptember 2015'], ['\n9125462', '\nSeptember 2015'], ['\n9192215', '\nNovember 2015'], ['\n9220325', '\nDecember 2015'], ['\n9237785', '\nJanuary 2016'], ['\n9241549', '\nJanuary 2016'], ['\n9289039', '\nMarch 2016'], ['\n9510653', '\nDecember 2016'], ['\n9629426', '\nApril 2017'], ['\n2001/0001083', '\nMay 2001'], ['\n2002/0074027', '\nJune 2002'], ['\n2004/0240167', '\nDecember 2004'], ['\n2005/0016571', '\nJanuary 2005'], ['\n2005/0072451', '\nApril 2005'], ['\n2005/0161067', '\nJuly 2005'], ['\n2006/0124122', '\nJune 2006'], ['\n2006/0127034', '\nJune 2006'], ['\n2006/0196532', '\nSeptember 2006'], ['\n2007/0040647', '\nFebruary 2007'], ['\n2007/0070588', '\nMarch 2007'], ['\n2007/0126208', '\nJune 2007'], ['\n2007/0242450', '\nOctober 2007'], ['\n2007/0279856', '\nDecember 2007'], ['\n2007/0286463', '\nDecember 2007'], ['\n2008/0092936', '\nApril 2008'], ['\n2008/0262657', '\nOctober 2008'], ['\n2009/0056775', '\nMarch 2009'], ['\n2009/0071516', '\nMarch 2009'], ['\n2009/0193578', '\nAugust 2009'], ['\n2009/0250982', '\nOctober 2009'], ['\n2009/0277486', '\nDecember 2009'], ['\n2010/0012164', '\nJanuary 2010'], ['\n2010/0097441', '\nApril 2010'], ['\n2010/0204841', '\nAugust 2010'], ['\n2010/0245503', '\nSeptember 2010'], ['\n2010/0295456', '\nNovember 2010'], ['\n2010/0320819', '\nDecember 2010'], ['\n2011/0157801', '\nJune 2011'], ['\n2012/0029704', '\nFebruary 2012'], ['\n2012/0038279', '\nFebruary 2012'], ['\n2013/0073283', '\nMarch 2013'], ['\n2014/0167624', '\nJune 2014'], ['\n2014/0317168', '\nOctober 2014'], ['\n2015/0043202', '\nFebruary 2015'], ['\n2015/0116485', '\nApril 2015'], ['\n2015/0255853', '\nSeptember 2015'], ['\n2015/0362137', '\nDecember 2015'], ['\n2016/0153650', '\nJune 2016'], ['\n2016/0184993', '\nJune 2016'], ['\n2016/0326765', '\nNovember 2016'], ['\n2016/0338457', '\nNovember 2016'], ['\n2018/0020530', '\nJanuary 2018'], ['\n2018/0177029', '\nJune 2018']]",[0]," The invention claimed is:  1.  A lighting assembly, comprising: a lighting support frame and lighting fabric;  a lighting support assembly connected to the lighting support frame and lighting
fabric, the lighting support frame or the lighting support assembly comprising one or more lighting elements;  an artificial intelligence (AI) device housing coupled to the lighting support assembly, the AI device housing comprising: one or more
microphones;  one or more processors;  one or more memory devices;  and computer-readable instructions stored in the one or more memory devices and executable by the one or more processors to: receive audible commands;  and convert the received audible
commands to one or more sound files;  and a wireless transceiver, the computer-readable instructions further executable by the one or more processors to: communicate the one or more sound files, via the wireless transceiver, to an external computing
device for voice recognition;  and receive one or more recognized device commands, via the wireless transceiver, the recognized one or more device commands to be based, at least in part, on the communicated one or more sound files.
 2.  The lighting assembly of claim 1, wherein the received recognized device command is related to operation of a camera, the computer-readable instructions further executable by the one or more processors to: activate the camera to initiate
capture of images or video around the lighting assembly.
 3.  The lighting assembly of claim 1, wherein the received recognized device command is related to operation of one or more sensors, the computer-readable instructions further executable by the one or more processors to activate the one or more
sensors to capture sensor measurements from an area surrounding the lighting assembly.
 4.  The lighting assembly of claim 1, wherein the received recognized device command is related to operation of the one or more lighting elements, the computer-readable instructions further executable by the one or more processors to activate
the one or more lighting elements.
 5.  The lighting assembly of claim 1, wherein the received recognized device commands is related to operation of an audio system, the computer-readable instructions further executable by the one or more processors to activate the audio system to
play audio music files.
 6.  The lighting assembly of claim 5, wherein the audio files are received, via the wireless transceiver, from an external computing device.
 7.  The lighting assembly of claim 1, wherein the one or more microphones are integrated with the AI device housing.
 8.  The lighting assembly of claim 1, wherein the wireless transceiver is a WiFi wireless communications transceiver.
 9.  The lighting assembly of claim 1, wherein the wireless transceiver is a wireless cellular communication transceiver.
 10.  The lighting assembly of claim 1, further including a base assembly coupled to the AI device housing.
 11.  The lighting assembly of claim 10, the base assembly comprising wheels, wherein the base assembly is a movable base assembly.
 12.  The lighting assembly of claim 10, wherein the AI device housing is rotatable with respect to the base assembly.
 13.  The lighting assembly of claim 1, wherein the lighting support assembly is rotatable with respect to AI device housing.
 14.  The lighting assembly of claim 1, the AI device housing further comprising an imaging device to capture images of an area around the lighting assembly.
 15.  A lighting assembly, comprising: an artificial intelligence (AI) body housing, the AI body housing comprising one or more microphones;  one or more processors;  one or more memory devices;  and computer-readable instructions executable by
the one or more processors to receive audible commands;  and convert the received audible commands to one or more sound files;  and;  a cantilever support assembly connected to the AI body housing;  a pivoting assembly connected to the cantilever support
assembly;  and a cantilever lighting assembly connected to the pivoting assembly, the cantilever lighting assembly to comprise one or more lighting elements on a first surface and one or more solar panels on a second surface, wherein the pivoting
assembly to lift the cantilever support assembly to a deployed position;  and a wireless transceiver, the computer-readable instructions further executable by the one or more processors to: communicate the one or more sound files, via the wireless
transceiver, to an external computing device for voice recognition;  and receive one or more recognized device commands, via the wireless transceiver, the recognized one or more device commands to be based, at least in part, on the communicated one or
more sound files.
 16.  The lighting assembly of claim 15, the cantilever support assembly further comprising one or more lighting elements.
 17.  The lighting assembly of claim 15, further comprising a rotation assembly, the rotation assembly connected to the AI body housing and the cantilever support assembly and rotating the cantilever support assembly about the AI body housing.
 18.  A lighting assembly, comprising: a lighting support frame;  a lighting support assembly connected to the lighting support frame and lighting fabric, the lighting support frame or the lighting support assembly comprising one or more lighting
elements;  an artificial intelligence (AI) device housing coupled to the lighting support assembly, the AI device housing comprising: one or more microphones;  one or more processors;  one or more memory devices;  and computer-readable instructions
stored in the one or more memory devices and executable by the one or more processors to: receive audible commands;  and convert the received audible commands to one or more sound files;  and a wireless transceiver, the computer-readable instructions
further executable by the one or more processors to: communicate the one or more sound files, via the wireless transceiver, to an external computing device for voice recognition;  and receive one or more recognized device commands, via the wireless
transceiver, the recognized one or more device commands to be based, at least in part, on the communicated one or more sound files.  "
"10,353,901","
     July 16, 2019
","Systems and methods of using an artificially intelligent database
     management system and interfaces for mobile, embedded, and other
     computing devices
"," The current disclosure generally relates to database management systems
     (DBMSs) and may be generally directed to methods and systems of using
     artificial intelligence (i.e. machine learning and/or anticipation
     functionalities, etc.) to learn a user's use of a DBMS, store this
     ""knowledge"" in a knowledgebase, and anticipate the user's future
     operating intentions. The current disclosure may also be generally
     directed to associative methods and systems of constructing DBMS
     commands. The current disclosure may also be generally directed to
     methods and systems of using a simplified DBMS command language (SDCL)
     for associative DBMS command construction. The current disclosure may
     also be generally directed to artificially intelligent methods and
     systems for associative DBMS command construction. The current disclosure
     may also be generally directed to methods and systems for associative
     DBMS command construction through voice input. Other methods, systems,
     features, elements and/or their embodiments are also disclosed.
",G06F 3/0481 (20130101); G06F 16/284 (20190101); G10L 15/26 (20130101); G06F 16/2379 (20190101); G06F 3/167 (20130101); G06F 3/04842 (20130101); G06F 16/8358 (20190101); G06F 16/21 (20190101); G10L 21/10 (20130101); G06F 16/24575 (20190101); G06N 5/022 (20130101); G06F 16/252 (20190101); G06N 5/02 (20130101),G06N 5/02 (20060101); G06F 3/0484 (20130101); G06F 16/23 (20190101); G06F 16/28 (20190101); G06F 16/25 (20190101); G06F 16/21 (20190101); G06F 16/2457 (20190101); G06F 3/16 (20060101); G10L 15/26 (20060101); G10L 21/10 (20130101); G06F 16/835 (20190101); G06F 3/0481 (20130101),"[['\n4862390', '\nAugust 1989'], ['\n5121470', '\nJune 1992'], ['\n5185857', '\nFebruary 1993'], ['\n5533029', '\nJuly 1996'], ['\n5544222', '\nAugust 1996'], ['\n5598534', '\nJanuary 1997'], ['\n5812117', '\nSeptember 1998'], ['\n5956637', '\nSeptember 1999'], ['\n6087952', '\nJuly 2000'], ['\n6122527', '\nSeptember 2000'], ['\n6128012', '\nOctober 2000'], ['\n6202060', '\nMarch 2001'], ['\n6226665', '\nMay 2001'], ['\n6493717', '\nDecember 2002'], ['\n6549625', '\nApril 2003'], ['\n6636873', '\nOctober 2003'], ['\n6654762', '\nNovember 2003'], ['\n6795706', '\nSeptember 2004'], ['\n6832084', '\nDecember 2004'], ['\n6871068', '\nMarch 2005'], ['\n6879989', '\nApril 2005'], ['\n6889139', '\nMay 2005'], ['\n6895234', '\nMay 2005'], ['\n6959436', '\nOctober 2005'], ['\n7113946', '\nSeptember 2006'], ['\n7117225', '\nOctober 2006'], ['\n7328030', '\nFebruary 2008'], ['\n7444143', '\nOctober 2008'], ['\n7450962', '\nNovember 2008'], ['\n7483905', '\nJanuary 2009'], ['\n7523112', '\nApril 2009'], ['\n7685127', '\nMarch 2010'], ['\n7756525', '\nJuly 2010'], ['\n7756829', '\nJuly 2010'], ['\n7779029', '\nAugust 2010'], ['\n7805397', '\nSeptember 2010'], ['\n7813714', '\nOctober 2010'], ['\n7831554', '\nNovember 2010'], ['\n7865924', '\nJanuary 2011'], ['\n7885635', '\nFebruary 2011'], ['\n7945687', '\nMay 2011'], ['\n7958270', '\nJune 2011'], ['\n8031694', '\nOctober 2011'], ['\n8041372', '\nOctober 2011'], ['\n8099332', '\nJanuary 2012'], ['\n8103272', '\nJanuary 2012'], ['\n8145663', '\nMarch 2012'], ['\n8190645', '\nMay 2012'], ['\n8204911', '\nJune 2012'], ['\n8214409', '\nJuly 2012'], ['\n8266187', '\nSeptember 2012'], ['\n8290898', '\nOctober 2012'], ['\n8291408', '\nOctober 2012'], ['\n8335805', '\nDecember 2012'], ['\n8359223', '\nJanuary 2013'], ['\n8401539', '\nMarch 2013'], ['\n8412150', '\nApril 2013'], ['\n8417740', '\nApril 2013'], ['\n9330165', '\nMay 2016'], ['\n2001/0044732', '\nNovember 2001'], ['\n2002/0078209', '\nJanuary 2002'], ['\n2002/0161735', '\nOctober 2002'], ['\n2002/0174189', '\nNovember 2002'], ['\n2003/0046451', '\nMarch 2003'], ['\n2003/0065662', '\nApril 2003'], ['\n2004/0010699', '\nJanuary 2004'], ['\n2004/0049476', '\nMarch 2004'], ['\n2004/0098394', '\nMay 2004'], ['\n2004/0128327', '\nJuly 2004'], ['\n2004/0203845', '\nOctober 2004'], ['\n2004/0224675', '\nNovember 2004'], ['\n2004/0229595', '\nNovember 2004'], ['\n2004/0249785', '\nDecember 2004'], ['\n2004/0249857', '\nDecember 2004'], ['\n2005/0044165', '\nFebruary 2005'], ['\n2005/0076069', '\nApril 2005'], ['\n2005/0086203', '\nApril 2005'], ['\n2005/0149517', '\nJuly 2005'], ['\n2005/0149542', '\nJuly 2005'], ['\n2005/0289105', '\nDecember 2005'], ['\n2006/0136454', '\nJune 2006'], ['\n2006/0212846', '\nSeptember 2006'], ['\n2007/0136264', '\nJanuary 2007'], ['\n2007/0049246', '\nMarch 2007'], ['\n2007/0291757', '\nDecember 2007'], ['\n2008/0005054', '\nJanuary 2008'], ['\n2008/0039062', '\nFebruary 2008'], ['\n2008/0040782', '\nFebruary 2008'], ['\n2008/0045205', '\nFebruary 2008'], ['\n2008/0086481', '\nApril 2008'], ['\n2008/0091763', '\nApril 2008'], ['\n2008/0126403', '\nMay 2008'], ['\n2008/0172274', '\nJuly 2008'], ['\n2008/0189250', '\nAugust 2008'], ['\n2008/0281825', '\nNovember 2008'], ['\n2009/0070149', '\nMarch 2009'], ['\n2009/0106360', '\nApril 2009'], ['\n2009/0176529', '\nJuly 2009'], ['\n2009/0201908', '\nAugust 2009'], ['\n2009/0217375', '\nAugust 2009'], ['\n2009/0254594', '\nOctober 2009'], ['\n2009/0275331', '\nNovember 2009'], ['\n2010/0023531', '\nJanuary 2010'], ['\n2010/0023541', '\nJanuary 2010'], ['\n2010/0077468', '\nMarch 2010'], ['\n2010/0082536', '\nApril 2010'], ['\n2010/0088316', '\nApril 2010'], ['\n2010/0159903', '\nJune 2010'], ['\n2010/0223228', '\nSeptember 2010'], ['\n2010/0257376', '\nOctober 2010'], ['\n2010/0278162', '\nNovember 2010'], ['\n2011/0010344', '\nJanuary 2011'], ['\n2011/0016089', '\nJanuary 2011'], ['\n2011/0106448', '\nMay 2011'], ['\n2011/0119758', '\nMay 2011'], ['\n2011/0145210', '\nJune 2011'], ['\n2011/0145242', '\nJune 2011'], ['\n2011/0161290', '\nJune 2011'], ['\n2011/0258345', '\nOctober 2011'], ['\n2011/0314482', '\nDecember 2011'], ['\n2012/0016901', '\nJanuary 2012'], ['\n2012/0101993', '\nApril 2012'], ['\n2012/0131116', '\nMay 2012'], ['\n2012/0159393', '\nJune 2012'], ['\n2012/0173485', '\nJuly 2012'], ['\n2012/0183221', '\nJuly 2012'], ['\n2012/0209925', '\nAugust 2012'], ['\n2012/0209948', '\nAugust 2012'], ['\n2013/0036089', '\nFebruary 2013'], ['\n2013/0198171', '\nAugust 2013']]",[0]," The invention claimed is:  1.  An artificially intelligent database management system (DBMS) comprising: one or more processor circuits;  a DBMS;  a memory that stores a first knowledge cell that
includes a plurality of instruction sets for performing one or more operations on the DBMS and a second knowledge cell that includes a plurality of instruction sets for performing one or more operations on the DBMS, wherein the first knowledge cell is
learned in a first learning process and the second knowledge cell is learned in a second learning process;  and a DBMS artificial intelligence unit that: accesses the first knowledge cell and the second knowledge cell in the memory;  determines the
plurality of instruction sets of the second knowledge cell based on at least partial match between at least one portion of a new one or more instruction sets and at least one portion of a subset of the plurality of instruction sets of the second
knowledge cell, or receives a selection of the plurality of instruction sets of the second knowledge cell;  and causes an execution of the plurality of instruction sets of the second knowledge cell at least in response to the determines or the receives
of the DBMS artificial intelligence unit, wherein the execution of the plurality of instruction sets of the second knowledge cell is performed in a temporal order in which the plurality of instruction sets of the second knowledge cell are learned in the
second learning process, and wherein the DBMS or another DBMS performs the one or more operations defined by the plurality of instruction sets of the second knowledge cell.
 2.  The system of claim 1, wherein the second knowledge cell includes an instruction set for creating an element of the DBMS, and wherein the element of the DBMS is not: a query, a portion of a query, an instruction, a portion of an instruction,
an instruction set, or a portion of an instruction set.
 3.  The system of claim 1, wherein the DBMS artificial intelligence unit determines the plurality of instruction sets of the second knowledge cell based on the at least partial match between the at least one portion of the new one or more
instruction sets and the at least one portion of the subset of the plurality of instruction sets of the second knowledge cell, and wherein the determines the plurality of instruction sets of the second knowledge cell based on the at least partial match
between the at least one portion of the new one or more instruction sets and the at least one portion of the subset of the plurality of instruction sets of the second knowledge cell includes: determining that a number of at least partially matching
portions of the new one or more instruction sets and portions of the subset of the plurality of instruction sets of the second knowledge cell exceeds a threshold number, or determining that a percentage of at least partially matching portions of the new
one or more instruction sets and portions of the subset of the plurality of instruction sets of the second knowledge cell exceeds a threshold percentage.
 4.  The system of claim 1, wherein the second knowledge cell includes an instruction set for deleting an element of the DBMS, and wherein the element of the DBMS is not: a query, a portion of a query, an instruction, a portion of an instruction,
an instruction set, or a portion of an instruction set.
 5.  The system of claim 1, wherein the DBMS includes an Oracle DBMS, a Microsoft SQL Server DBMS, a MySQL DBMS, an IBM DB2 DBMS, a relational DBMS, an application that uses the DBMS, or an interface for the DBMS, and wherein the another DBMS
includes an Oracle DBMS, a Microsoft SQL Server DBMS, a MySQL DBMS, an IBM DB2 DBMS, a relational DBMS, an application that uses the another DBMS, or an interface for the another DBMS.
 6.  The system of claim 1, wherein the first learning process and the second learning process use a same learning algorithm.
 7.  The system of claim 1, wherein the DBMS artificial intelligence unit further: alters at least one instruction set of the second knowledge cell or a copy of the at least one instruction set of the second knowledge cell based on a pattern
recognized from at least one portion of the new one or more instruction sets.
 8.  The system of claim 1, wherein the DBMS artificial intelligence unit further: alters at least one instruction set of the second knowledge cell or a copy of the at least one instruction set of the second knowledge cell in response to
receiving a user selection to modify the at least one instruction set of the second knowledge cell or the copy of the at least one instruction set of the second knowledge cell.
 9.  The system of claim 1, wherein the DBMS artificial intelligence unit determines the plurality of instruction sets of the second knowledge cell based on the at least partial match between the at least one portion of the new one or more
instruction sets and the at least one portion of the subset of the plurality of instruction sets of the second knowledge cell, and wherein the causes the execution of the plurality of instruction sets of the second knowledge cell includes causing the
execution of the plurality of instruction sets of the second knowledge cell with no user input.
 10.  The system of claim 1, wherein the first learning process includes receiving at least one instruction set of the first knowledge cell from a first application that uses the DBMS, and wherein the second learning process includes receiving at
least one instruction set of the second knowledge cell from a second application that uses the DBMS.
 11.  The system of claim 1, wherein the DBMS includes at least one of: a hardware element that is included in the one or more processor circuits, a hardware element that is included in another one or more processor circuits, an application
operating on the one or more processor circuits, an application operating on another one or more processor circuits, or an element coupled to the one or more processor circuits, and wherein the DBMS artificial intelligence unit includes at least one of:
a hardware element that is included in the one or more processor circuits, a hardware element that is included in another one or more processor circuits, an application operating on the one or more processor circuits, an application operating on another
one or more processor circuits, or an element coupled to the one or more processor circuits.
 12.  The system of claim 1, wherein the DBMS artificial intelligence unit determines the plurality of instruction sets of the second knowledge cell based on the at least partial match between the at least one portion of the new one or more
instruction sets and the at least one portion of the subset of the plurality of instruction sets of the second knowledge cell, and wherein the determines the plurality of instruction sets of the second knowledge cell based on the at least partial match
between the at least one portion of the new one or more instruction sets and the at least one portion of the subset of the plurality of instruction sets of the second knowledge cell includes determining the plurality of instruction sets of the second
knowledge cell based on at least partial match between at least one portion of two or more instruction sets of the new one or more instruction sets and at least one portion of the subset of the plurality of instruction sets of the second knowledge cell.
 13.  The system of claim 1, wherein the causes the execution of the plurality of instruction sets of the second knowledge cell is performed at least in response to receiving a user confirmation of the execution of the plurality of instruction
sets of the second knowledge cell.
 14.  The system of claim 1, wherein the DBMS artificial intelligence unit receives the selection of the plurality of instruction sets of the second knowledge cell, and wherein the receives the selection of the plurality of instruction sets of
the second knowledge cell includes receiving a user selection of the plurality of instruction sets of the second knowledge cell.
 15.  The system of claim 1, wherein the DBMS is not a search engine, and wherein the another DBMS is not a search engine, and wherein at least one instruction set of the second knowledge cell is not a search query, and wherein at least one
instruction set of the second knowledge cell is not a select Structured Query Language (SQL) query.
 16.  The system of claim 1, wherein the subset of the plurality of instruction sets of the second knowledge cell includes one or more instruction sets of the second knowledge cell.
 17.  The system of claim 1, wherein the execution of the plurality of instruction sets of the second knowledge cell is performed by at least one from a list of: the one or more processor circuits, another one or more processor circuits, the
DBMS, and the another DBMS.
 18.  The system of claim 1, wherein the first knowledge cell is a data structure for storing the plurality of instruction sets of the first knowledge cell, and wherein the second knowledge cell is a data structure for storing the plurality of
instruction sets of the second knowledge cell.
 19.  A non-transitory machine readable medium having a program stored thereon that when executed by one or more processor circuits causes the one or more processor circuits to perform operations comprising: accessing a memory that stores a first
knowledge cell that includes a plurality of instruction sets for performing one or more operations on a DBMS and a second knowledge cell that includes a plurality of instruction sets for performing one or more operations on the DBMS, wherein the first
knowledge cell is learned in a first learning process and the second knowledge cell is learned in a second learning process;  determining the plurality of instruction sets of the second knowledge cell based on at least partial match between at least one
portion of a new one or more instruction sets and at least one portion of a subset of the plurality of instruction sets of the second knowledge cell, or receiving a selection of the plurality of instruction sets of the second knowledge cell;  and causing
an execution of the plurality of instruction sets of the second knowledge cell at least in response to the determining or the receiving, wherein the execution of the plurality of instruction sets of the second knowledge cell is performed in a temporal
order in which the plurality of instruction sets of the second knowledge cell are learned in the second learning process, and wherein the DBMS or another DBMS performs the one or more operations defined by the plurality of instruction sets of the second
knowledge cell.
 20.  A method comprising: (a) accessing, by one or more processor circuits, a memory that stores a first knowledge cell that includes a plurality of instruction sets for performing one or more operations on a DBMS and a second knowledge cell
that includes a plurality of instruction sets for performing one or more operations on the DBMS, wherein the first knowledge cell is learned in a first learning process and the second knowledge cell is learned in a second learning process;  (b1)
determining, by the one or more processor circuits, the plurality of instruction sets of the second knowledge cell based on at least partial match between at least one portion of a new one or more instruction sets and at least one portion of a subset of
the plurality of instruction sets of the second knowledge cell, or (b2) receiving, by the one or more processor circuits, a selection of the plurality of instruction sets of the second knowledge cell;  (c) executing the plurality of instruction sets of
the second knowledge cell at least in response to the determining of (b1) or the receiving of (b2), wherein the executing the plurality of instruction sets of the second knowledge cell is performed in a temporal order in which the plurality of
instruction sets of the second knowledge cell are learned in the second learning process, and (d) performing, by the DBMS or by another DBMS, the one or more operations defined by the plurality of instruction sets of the second knowledge cell. 
"
"10,354,217","
     July 16, 2019
","Constraint-based complex dynamic route sequencing route sequencing for
     multi-vehicle fleets
"," Methods and systems to dynamically and optimally sequence routes based on
     historical and real-time traffic conditions, and to predict anticipated
     traffic conditions along the dynamically generated route are disclosed.
     Route sequencing may be based on a set of predefined constraints, e.g.,
     distance, time, time with traffic, or any objective cost function. The
     system of the present invention may be implemented in a vehicle fleet
     comprising one or more vehicles with one or more depots, or with no
     depots. An optimization server obtains real-time, historical and/or
     predicted future traffic, weather, hazard, and avoidance-zone data on
     road segments to generate a route, while staying within parameters and
     constraints set by an automatic machine learning process, an artificial
     intelligence program, or a human administrator. The platform may be
     coupled to sensors positioned on roads, e.g., speed radar or camera, and
     sensors positioned in vehicles, e.g., GPS system or on-board diagnostic
     sensor.
",G01C 21/3697 (20130101); G08G 1/0145 (20130101); G05D 1/0287 (20130101); G08G 1/0112 (20130101); G01C 21/3415 (20130101); G06Q 10/083 (20130101); G08G 1/0116 (20130101); G08G 1/207 (20130101); G08G 1/012 (20130101); G08G 1/0133 (20130101); G08G 1/202 (20130101); G01C 21/3492 (20130101); G01C 21/3469 (20130101); G08G 1/0129 (20130101); G05D 1/0088 (20130101); G08G 1/096811 (20130101); G01C 21/3461 (20130101),G01C 21/34 (20060101); G05D 1/00 (20060101); G01C 21/36 (20060101); G05D 1/02 (20060101); G06Q 10/08 (20120101); G08G 1/01 (20060101); G08G 1/00 (20060101); G08G 1/0968 (20060101),"[['\n5958012', '\nSeptember 1999'], ['\n6256577', '\nJuly 2001'], ['\n6317686', '\nNovember 2001'], ['\n8275508', '\nSeptember 2012'], ['\n8374781', '\nFebruary 2013'], ['\n8825395', '\nSeptember 2014'], ['\n8977481', '\nMarch 2015'], ['\n9140567', '\nSeptember 2015'], ['\n2004/0030572', '\nFebruary 2004'], ['\n2005/0093720', '\nMay 2005'], ['\n2006/0031007', '\nFebruary 2006'], ['\n2007/0155404', '\nJuly 2007'], ['\n2009/0276153', '\nNovember 2009'], ['\n2010/0205022', '\nAugust 2010'], ['\n2010/0312466', '\nDecember 2010'], ['\n2010/0332121', '\nDecember 2010'], ['\n2014/0095309', '\nApril 2014'], ['\n2015/0032366', '\nJanuary 2015'], ['\n2016/0117871', '\nApril 2016'], ['\n2016/0379168', '\nDecember 2016'], ['\n2018/0017401', '\nJanuary 2018']]",[0]," What is claimed is:  1.  A machine-implemented method, comprising: receiving for a segment of a road at least one of a traffic data, a weather data, a hazard data and an avoidance zone data,
wherein the at least one of a traffic data, a weather data, a hazard data, and an avoidance zone data comprise at least one of a real-time data, a historical data, and a predictive data, wherein the historical data matches with a current circumstance; 
determining a current traffic condition and a future traffic condition based on the at least one of a traffic data, a weather data, a hazard data, and an avoidance zone data;  sequencing one or more driving routes comprising a plurality of user vehicles
and a plurality of destinations based on one of a shortest travel time, a shortest distance, a shortest distance based on the current traffic condition and the future traffic condition, and an objective cost function, wherein the driving route obeys one
or more predetermined constraints;  and generating one or more driving directions based on the sequenced one or more driving routes.
 2.  The method of claim 1, further comprising: wherein the avoidance zone data comprises an avoidance zone, a traffic signal schedule, and a bridge schedule.
 3.  The method of claim 1, further comprising: wherein the avoidance zone data comprises a driver restriction constraint, a vehicle-type restriction constraint and a prohibited maneuver constraint.
 4.  The method of claim 3, further comprising: wherein the prohibited maneuver constraint comprises a left-turn restriction, a right-turn restriction, and a U-turn restriction.
 5.  The method of claim 3, further comprising: wherein the prohibited maneuver constraint is determined by a country detection process.
 6.  The method of claim 1, further comprising: wherein the one or more predetermined constraints comprise at least one of an operational constraint, an asset management constraint, an inventory constraint, a business constraint, a regulatory
constraint, and a natural world constraint.
 7.  The method of claim 6, further comprising: wherein the asset management constraint comprises a pre-approved road constraint and a vehicle capacity constraint.
 8.  The method of claim 6, further comprising: wherein the inventory constraint comprises one or more persons.
 9.  The method of claim 8, further comprising: wherein at least one destination is dynamically tracked through a mobile device for delivery of an inventory in an immediate vicinity of the destination.
 10.  The method of claim 9, further comprising: wherein the plurality of destinations is not at fixed geospatial coordinate or physical location.
 11.  The method of claim 9, further comprising: wherein the geospatial position of the plurality of destinations is dynamic.
 12.  The method of claim 8, further comprising: wherein a position of the plurality of destinations is dynamically tracked through at least one position detection device, and wherein the dynamic tracking of the position of the plurality of
destinations is followed by repeating the steps of receiving data, predicting the current traffic condition and the future traffic condition, sequencing the driving route and generating the driving direction.
 13.  The method of claim 6, further comprising: wherein the business constraint comprises a tracked user behavioral constraint, a customer behavior, a vehicle maneuver constraint, and an external machine constraint.
 14.  The method of claim 6, further comprising: wherein the at least one of an operational constraint, an asset management constraint, an inventory constraint, a business constraint, a regulatory constraint, and a natural world constraint are
set at predetermined priority levels when two or more constraints are obeyed.
 15.  The method of claim 6, further comprising: wherein the inventory constraint comprises an inventory amount constraint, a package size constraint, and an inventory processing time constraint, and wherein the inventory constraint is dynamic.
 16.  The method of claim 1, further comprising: wherein the driving route is based on a future time that the driving route will be implemented.
 17.  The method of claim 1, further comprising: determining an intersection point between the user vehicle and an inventory replenishment vehicle, and wherein the inventory replenishment vehicle comprises one or more inventory for the user
vehicle.
 18.  The method of claim 17, further comprising: wherein the one or more inventory comprises a person inventory.
 19.  The method of claim 1, further comprising: presenting one or more information to the user vehicle, wherein the information is at least one of a data and a metadata about the driving route, wherein the data and the metadata are based on a
cost computation calculated between two destinations of the driving route, and wherein the cost computation is based on the least one of a traffic data, a weather data, a hazard data and an avoidance zone data.
 20.  The method of claim 1, further comprising: wherein the plurality of destinations comprises at least one of a customer location and a position detection device attached to an object.  "
"10,354,342","
     July 16, 2019
","Adaptive livestock growth modeling using machine learning approaches to
     predict growth and recommend livestock management operations and
     activities
"," A method and system is provided in an adaptive framework for modeling
     livestock growth. The adaptive framework processes input data relative to
     livestock growth in an ensemble of one or more models and an artificial
     intelligence layer configured to select the most appropriate or primary
     model to optimize, predict, and recommend livestock feed operations based
     upon environmental, physiological, location and time variables within
     such input data. The adaptive framework also optimizes workflow by pen
     and by producer, based upon historical performance, gender and breed and
     the management practices of the producer.
",G06N 20/20 (20190101); G06N 20/00 (20190101); G16H 50/30 (20180101); G06Q 10/0639 (20130101); G16H 50/20 (20180101); G06N 7/00 (20130101); A01K 29/005 (20130101); G06Q 50/02 (20130101),G06Q 50/02 (20120101); G06Q 10/06 (20120101); G06N 7/00 (20060101); A01K 29/00 (20060101); G06N 20/00 (20190101),"[['\n5960105', '\nSeptember 1999'], ['\n2005/0153317', '\nJuly 2005'], ['\n2006/0069023', '\nMarch 2006'], ['\n2007/0093965', '\nApril 2007'], ['\n2009/0055243', '\nFebruary 2009'], ['\n2012/0299731', '\nNovember 2012'], ['\n2014/0088939', '\nMarch 2014'], ['\n2015/0359199', '\nDecember 2015']]",[0]," The invention claimed is:  1.  A method, comprising: initializing and retrieving input data impacting a livestock growth rate over time, the input data at least including livestock weight data
that includes an initial weight and a target weight, a livestock gender, a livestock type, a location, and a genetic breed;  analyzing the input data in a plurality of data processing modules within a computing environment in which the plurality of data
processing modules are executed in conjunction with at least one processor, the data processing modules configured to develop a model for continuously evaluating an average daily gain over time in a convergence-based ensemble of adaptive livestock growth
models, by identifying a plurality of variables in the input data that are expected to impact the average daily gain over time, and assigning weights to the plurality of variables in the input data, calculating a performance coefficient representing a
maintenance ratio during a livestock feeding period, the maintenance ratio based on weighted variables in the plurality of variables that are indicative of a body condition score for livestock as the livestock feeding period progresses, continuously
performing the convergence-based ensemble of adaptive livestock growth models to analyze a weight gain allowance from energy intake and a growing cattle energy requirement for maintenance, selecting a particular livestock growth model from the
convergence-based ensemble of adaptive livestock growth models to calculate the average daily gain to achieve the target weight over the livestock feeding period, predicting the livestock growth rate over time from the average daily gain, validating the
selection of the particular livestock growth model, by returning an actual closeout weight and actual scale data to the convergence-based ensemble of the adaptive livestock growth models to confirm the predicted livestock growth rate, and initiating one
or more management actions based on the predicted livestock growth rate and at least one of the livestock gender and the genetic breed, the one or more management actions including at least one of a sale of livestock, an adjustment of a rate of livestock
feed, a delivery of a type of livestock feed, an application of a nutritional treatment to livestock, and a delivery of a veterinary treatment to livestock.
 2.  The method of claim 1, wherein the input data includes historical and current weather information, and further includes one or more of temperature, precipitation, wind speed, solar radiation, and humidity throughout the livestock feeding
period.
 3.  The method of claim 1, wherein the input data further comprises a nutrition plan, a facility type, and a feedlot location.
 4.  The method of claim 3, further comprising obtaining a feedlot location from processing one or more GPS data points representing geographical coordinates associated with livestock with a GPS receiver, and determining the geographical
coordinates of the feedlot location from the one or more GPS data points.
 5.  The method of claim 1, wherein the performance coefficient is further calculated from animal-specific variables influencing the body condition score, and that include one or more of coat insulation value, lower critical temperature, tissue
insulation, hair depth, hide thickness, heat production, external insulation, dry matter intake, calorie intake, implants, antibiotics, biological additives, and food sources.
 6.  The method of claim 1, further comprising automatically adjusting the weights assigned to the plurality of variables as the livestock feeding period progresses.
 7.  The method of claim 1, wherein the convergence-based ensemble of adaptive livestock growth models include one or both of standardized models, and models customized according to particular formulas for analyzing the livestock growth rate.
 8.  The method of claim 1, wherein the convergence-based ensemble of adaptive livestock growth models include a net energy gain model.
 9.  The method of claim 1, wherein the convergence-based ensemble of adaptive livestock growth models include a net energy maintenance model.
 10.  The method of claim 1, further comprising calculating one or more additional weights, and assigning the one or more additional weights to variables for the input data that was not included in the convergence-based ensemble of adaptive
livestock growth models for subsequent modeling of the average daily gain over time.
 11.  A system, comprising: a data initialization component configured to retrieve input data impacting a livestock growth rate over time for a convergence-based ensemble of adaptive livestock growth models, the input data at least including a
livestock weight that includes an initial weight and a target weight, a livestock gender, a livestock type, a location, and a genetic breed;  a plurality of data processing components configured to analyze the input data by developing a model for
continuously evaluating an average daily gain in the convergence-based ensemble of livestock growth models by identifying a plurality of variables in the input data that are expected to impact the average daily gain over time, and assigning weights to
the plurality of variables in the input data, and calculating a performance coefficient representing a maintenance ratio during a livestock feeding period, the maintenance ratio based on weighted variables in the plurality of variables that are
indicative of a body condition score for livestock as the livestock feeding period progresses, an artificial intelligence component configured to continuously perform the convergence-based ensemble of adaptive livestock growth models to analyze a weight
gain allowance from energy intake and a growing cattle energy requirement for maintenance, select a particular livestock growth model from the convergence-based ensemble of adaptive livestock growth models, and predict the livestock growth rate over time
from the average daily gain;  a feedback component configured to validate the selection of the particular livestock growth model by returning an actual closeout weight and actual scale data to the convergence-based ensemble of the adaptive livestock
growth models to confirm the predicted livestock growth rate;  and an output data component configured to initiate one or more management actions based on the predicted livestock growth rate and at least one of the livestock gender and the genetic breed,
the one or more management actions including at least one of a sale of livestock, an adjustment of a rate of livestock feed, a delivery of a type of livestock feed, an application of a nutritional treatment, to livestock, and a delivery of a veterinary
treatment to livestock.
 12.  The system of claim 11, wherein the input data includes historical and current weather information, and further includes one or more of temperature, precipitation, wind speed, solar radiation, and humidity throughout the livestock feeding
period.
 13.  The system of claim 11, wherein the input data further comprises a nutrition plan, a facility type, and a feedlot location.
 14.  The system of claim 13, wherein the plurality of data processing components are further configured to obtain a feedlot location from processing one or more GPS data points representing geographical coordinates associated with livestock with
a GPS receiver, and determine the geographical coordinates of the feedlot location from the one or more GPS data points.
 15.  The system of claim 11, wherein the performance coefficient is further calculated from animal-specific variables influencing the body condition score, and that include one or more of coat insulation value, lower critical temperature, tissue
insulation, hair depth, hide thickness, heat production, external insulation, dry matter intake, calorie intake, implants, antibiotics, biological additives, and food sources.
 16.  The system of claim 11, wherein the artificial intelligence component is further configured to automatically adjusting the weights assigned to the plurality of variables as the livestock feeding period progresses.
 17.  The system of claim 11, wherein the convergence-based ensemble of adaptive livestock growth models include one or both of standardized models, and models customized according to particular formulas for analyzing the livestock growth rate.
 18.  The system of claim 11, wherein the convergence-based ensemble of adaptive livestock growth models include a net energy gain model.
 19.  The system of claim 11, wherein the convergence-based ensemble of adaptive livestock growth models include a net energy maintenance model.
 20.  The system of claim 11, wherein the plurality of data processing components are further configured to calculate one or more additional weights, and assign the one or more additional weights to variables for the input data that was not
included in the convergence-based ensemble of adaptive livestock growth models for subsequent modeling of the average daily gain over time.
 21.  A method, comprising: determining an average daily gain in a plurality of data processing functions that perform a convergence-based ensemble of livestock growth models with input data that is expressed as a plurality of variables impacting
livestock growth, the input data at least relative to a livestock weight that includes an initial weight and a target weight, a livestock gender, a livestock type, a location, and a genetic breed, by selecting a particular model in the convergence-based
ensemble of livestock growth models, by assigning weights to the plurality of variables that are identified as having an impact on the average daily gain over time, calculating a body condition score for livestock as the livestock feeding period
progresses, and a performance coefficient representing a maintenance ratio indicative of the body condition score, and continuously analyzing a weight gain allowance from energy intake and a growing cattle energy requirement for maintenance;  predicting
a livestock growth rate based on the average daily gain from the selected particular model;  confirming the predicted livestock growth rate by returning an actual closeout weight and actual scale data to the convergence-based ensemble of the adaptive
livestock growth models;  and initiating one or more management actions based on the predicted livestock growth rate and at least one of the livestock gender and the genetic breed, the one or more management actions including at least one of a sale of
livestock, an adjustment of a rate of livestock feed, a delivery of a type of livestock feed, an application of a nutritional treatment to livestock, and a delivery of a veterinary treatment to livestock.
 22.  The method of claim 21, wherein the input data includes historical and current weather information, and further includes one or more of temperature, precipitation, wind speed, solar radiation, and humidity throughout the livestock feeding
period.
 23.  The method of claim 1, wherein the input data further comprises a nutrition plan, a facility type, and a feedlot location.
 24.  The method of claim 23, further comprising obtaining a feedlot location from processing one or more GPS data points representing geographical coordinates associated with livestock with a GPS receiver, and determining the geographical
coordinates of the feedlot location from the one or more GPS data points.
 25.  The method of claim 21, wherein the performance coefficient is further calculated from animal-specific variables influencing the body condition score, and that include one or more of coat insulation value, lower critical temperature, tissue
insulation, hair depth, hide thickness, heat production, external insulation, dry matter intake, calorie intake, implants, antibiotics, biological additives, and food sources.
 26.  The method of claim 21, further comprising automatically adjusting the weights assigned to the plurality of variables as the livestock feeding period progresses.
 27.  The method of claim 21, wherein the convergence-based ensemble of adaptive livestock growth models include one or both of standardized models, and models customized according to particular formulas for analyzing the livestock growth rate.
 28.  The method of claim 21, wherein the convergence-based ensemble of adaptive livestock growth models include a net energy gain model.
 29.  The method of claim 21, wherein the convergence-based ensemble of adaptive livestock growth models include a net energy maintenance model.
 30.  The method of claim 21, further comprising calculating one or more additional weights, and assigning the one or more additional weights to variables for the input data that was not included in the convergence-based ensemble of adaptive
livestock growth models for subsequent modeling of the average daily gain over time.  "
"10,354,644","
     July 16, 2019
","System and method for encoding data in a voice recognition integrated
     circuit solution
"," Methods of encoding voice data for loading into an artificial
     intelligence (AI) integrated circuit are provided. The AI integrated
     circuit may have an embedded cellular neural network for implementing AI
     tasks based on the loaded voice data. An encoding method may generate a
     two-dimensional (2D) frequency-time array from an audio waveform, use the
     2D frequency-time array to generate a set of 2D arrays to approximate the
     2D frequency-time array, load the set of 2D arrays into the AI integrated
     circuit, execute programming instructions contained in the AI integrated
     circuit to feed the set of 2D arrays into the embedded cellular neural
     network in the AI integrated circuit to generate a voice recognition
     result, and output the voice recognition result. The encoding method also
     trains a convolution neural network (CNN) and loads the weights of the
     CNN into the AI integrated circuit for implementing the AI tasks.
",G10L 15/02 (20130101); G06N 3/04 (20130101); G10L 25/18 (20130101); G06N 3/0454 (20130101); G10L 15/22 (20130101); G10L 15/16 (20130101); G06N 3/08 (20130101); G06N 3/063 (20130101); G10L 15/063 (20130101),G06N 3/04 (20060101); G10L 15/16 (20060101); G06N 3/08 (20060101); G10L 15/06 (20130101); G10L 15/22 (20060101); G10L 25/18 (20130101),"[['\n2016/0196343', '\nJuly 2016'], ['\n2018/0182377', '\nJune 2018']]",[0]," We claim:  1.  A method of encoding voice data for loading into an artificial intelligence (AI) integrated circuit, the method comprising: receiving, by a processor, voice data comprising at
least a segment of an audio waveform;  generating, by the processor, a two-dimensional (2D) frequency-time array comprising a plurality of pixels, each pixel having a value that represents an audio intensity of the segment of the audio waveform at a time
in the segment and a frequency in the audio waveform, wherein the 2D frequency-time array is a 2D spectrogram;  using, by the processor, the 2D frequency-time array to generate a set of 2D arrays comprising a number of 2D arrays, each 2D array having a
plurality of pixels, each pixel having a value and corresponding to one of the plurality of pixels in the 2D frequency-time array, wherein the value of each pixel in the 2D frequency-time array is approximated by an average of the values of corresponding
pixels in the set of 2D arrays, and wherein generating the set of 2D arrays comprises, for each pixel in the 2D spectrogram: determining an integer part and a fraction part from the value of each pixel in the 2D spectrogram;  determining a value of zero
or one for corresponding pixels in the set of 2D arrays, wherein an average value of the corresponding pixels in the set of 2D arrays is approximate to the fraction part;  and updating the values of each of the corresponding pixels in the set of 2D
arrays by adding thereto the integer part;  loading the set of 2D arrays into the AI integrated circuit;  executing one or more programming instructions contained in the AI integrated circuit to feed the set of 2D arrays into an embedded cellular neural
network architecture in the AI integrated circuit;  generating a voice recognition result from the embedded cellular neural network architecture based on the set of 2D arrays;  and outputting the voice recognition result.
 2.  The method of claim 1, further comprising: receiving a set of sample training voice data comprising at least one sample segment of an audio waveform;  using the set of sample training voice data to generate one or more sample 2D
frequency-time arrays each comprising a plurality of pixels, each pixel having a value that represents an audio intensity of the sample segment of the audio waveform at a time in the sample segment and a frequency in the audio waveform;  using each of
the one or more sample 2D frequency-time arrays to generate a set of 2D training arrays, each 2D training array having a plurality of pixels, each pixel having a value and corresponding to one of the plurality of pixels in each sample 2D frequency-time
array, wherein the value of each pixel in each 2D frequency-time array is approximated by a combination of the values of corresponding pixels in the set of 2D arrays generated by that 2D frequency-time array;  using the set of 2D training arrays to train
one or more weights of a convolutional neural network;  and loading the one or more trained weights into the embedded cellular neural network architecture of the AI integrated circuit.
 3.  The method of claim 1, further comprising sampling the 2D frequency-time array before using the 2D frequency-time array to generate the set of 2D arrays.
 4.  The method of claim 1, further comprising normalizing the value of each pixel of the 2D frequency-time array to a real number in a range from zero to a depth of each channel in the AI integrated circuit before generating the set of 2D
arrays.
 5.  The method of claim 1, wherein generating the set of 2D arrays comprises, for each pixel in the 2D frequency-time array: generating a sequence of random values, wherein an average of the random values in the sequence is approximate to the
value of the pixel;  and using the sequence of random values to determine the values of the corresponding pixels in each of the set of 2D arrays.
 6.  A method encoding voice data for loading into an artificial intelligence (AI) integrated circuit, the method comprising: receiving, by a processor, voice data comprising at least a segment of an audio waveform;  generating, by the processor,
a two-dimensional (2D) frequency-time array comprising a plurality of pixels, each pixel having a value that represents an audio intensity of the segment of the audio waveform at a time in the segment and a frequency in the audio waveform;  using, by the
processor, the 2D frequency-time array to generate a set of 2D arrays comprising a number of 2D arrays, each 2D array having a plurality of pixels, each pixel having a value and corresponding to one of the plurality of pixels in the 2D frequency-time
array, wherein the value of each pixel in the 2D frequency-time array is approximated by a combination of the values of corresponding pixels in the set of 2D arrays;  loading the set of 2D arrays into the AI integrated circuit;  executing one or more
programming instructions contained in the AI integrated circuit to feed the set of 2D arrays into an embedded cellular neural network architecture in the AI integrated circuit;  generating a voice recognition result from the embedded cellular neural
network architecture based on the set of 2D arrays;  and outputting the voice recognition result;  wherein the 2D frequency-time array is a 2D spectrogram and generating the set of 2D arrays comprises, for each pixel in the 2D spectrogram: determining a
significant fraction part and an exponent part from the value of each pixel;  using the significant fraction part to determine a first subset of 2D arrays comprising a first number of 2D arrays;  and using the exponent part to determine a second subset
of 2D arrays comprising a second number of 2D arrays;  wherein a sum of the first and second numbers is the number of arrays in the set of 2D arrays.
 7.  The method of claim 6, wherein determining the first subset of 2D arrays comprises: determining the first subset of 2D arrays so that the significant fraction part for each pixel in the 2D spectrogram is approximate to an average of the
values of corresponding pixels across all of the 2D arrays in the first subset.
 8.  The method of claim 7, wherein determining the first subset of 2D arrays comprises, for each pixel in the 2D spectrogram: determining an integer part and a fraction part from the significant fraction part of the value of each pixel;  using
the fraction part to determine a value of zero or one for a corresponding pixel in each of the first subset of 2D arrays, wherein an average value of corresponding pixels in the first subset of 2D arrays is approximate to the fraction part;  and updating
the values for the corresponding pixels in each of the first subset of 2D arrays by adding thereto the integer part.
 9.  The method of claim 6, further comprising taking a log of the value of each pixel in the 2D spectrogram before determining the significant fraction part and the exponent part from the value of each pixel.
 10.  A system for encoding voice data for loading into an artificial intelligence (AI) integrated circuit, the system comprising: a processor;  and a non-transitory computer readable medium containing programming instructions that, when
executed, will cause the processor to: receive voice data comprising at least a segment of an audio waveform, generate a two-dimensional (2D) frequency-time array comprising a plurality of pixels, each pixel having a value that represents an audio
intensity of the segment of the audio waveform at a time in the segment and a frequency in the audio waveform, wherein the 2D frequency-time array is a 2D spectrogram, use the 2D frequency-time array to generate a set of 2D arrays comprising a number of
2D arrays, each 2D array having a plurality of pixels, each pixel having a value and corresponding to one of the plurality of pixels in the 2D frequency-time array, wherein the value of each pixel in the 2D frequency-time array is approximated by an
average of the values of corresponding pixels in the set of 2D arrays, and load the set of 2D arrays into the AI integrated circuit;  wherein programming instructions for generating the set of 2D arrays comprises programming instructions configured to,
for each pixel in the 2D spectrogram: determine an integer part and a fraction part from the value of each pixel in the 2D spectrogram;  determine a value of zero or one for corresponding pixels in the set of 2D arrays, wherein an average value of the
corresponding pixels in the set of 2D arrays is approximate to the fraction part;  and update the values of each of the corresponding pixels in the set of 2D arrays by adding thereto the integer part;  wherein the AI integrated circuit comprises: an
embedded cellular neural network architecture, and one or more programming instructions configured to: feed the set of 2D arrays into the embedded cellular neural network architecture in the AI integrated circuit;  generate a voice recognition result
from the embedded cellular neural network architecture based on the set of 2D arrays;  and output the voice recognition result.
 11.  The system of claim 10, further comprising additional programming instructions configured to cause the processor to: receive a set of sample training voice data comprising at least one sample segment of an audio waveform;  use the set of
sample training voice data to generate one or more sample 2D frequency-time arrays each comprising a plurality of pixels, each pixel having a value that represents an audio intensity of the sample segment of the audio waveform at a time in the sample
segment and a frequency in the audio waveform;  use each of the one or more sample 2D frequency-time arrays to generate a set of 2D training arrays, each 2D training array having a plurality of pixels, each pixel having a value and corresponding to one
of the plurality of pixels in each sample 2D frequency-time array, wherein the value of each pixel in each 2D frequency-time array is approximated by a combination of the values of corresponding pixels in the set of 2D arrays generated by that 2D
frequency-time array;  use the set of 2D training arrays to train one or more weights of a convolutional neural network;  and load the one or more trained weights into the embedded cellular neural network architecture of the AI integrated circuit.
 12.  The system of claim 10, further comprising additional programming instructions configured to sample the 2D frequency-time array before using the 2D frequency-time array to generate the set of 2D arrays.
 13.  The system of claim 10, further comprising additional programming instructions configured to normalize the value of each pixel of the 2D frequency-time array to a real number in a range from zero to a depth of each channel in the AI
integrated circuit before generating the set of 2D arrays.
 14.  The system of claim 10, wherein programming instructions for generating the set of 2D arrays comprises programming instructions configured to, for each pixel in the 2D frequency-time array: generate a sequence of random values, wherein an
average of the random values in the sequence is approximate to the value of the pixel;  and use the sequence of random values to determine the values of the corresponding pixels in each of the set of 2D arrays.
 15.  A system for encoding voice data for loading into an artificial intelligence (AI) integrated circuit, the system comprising: a processor;  and a non-transitory computer readable medium containing programming instructions that, when
executed, will cause the processor to: receive voice data comprising at least a segment of an audio waveform, generate a two-dimensional (2D) frequency-time array comprising a plurality of pixels, each pixel having a value that represents an audio
intensity of the segment of the audio waveform at a time in the segment and a frequency in the audio waveform, use the 2D frequency-time array to generate a set of 2D arrays comprising a number of 2D arrays, each 2D array having a plurality of pixels,
each pixel having a value and corresponding to one of the plurality of pixels in the 2D frequency-time array, wherein the value of each pixel in the 2D frequency-time array is approximated by a combination of the values of corresponding pixels in the set
of 2D arrays, and load the set of 2D arrays into the AI integrated circuit;  wherein the AI integrated circuit comprises: an embedded cellular neural network architecture, and one or more programming instructions configured to: feed the set of 2D arrays
into the embedded cellular neural network architecture in the AI integrated circuit;  generate a voice recognition result from the embedded cellular neural network architecture based on the set of 2D arrays;  and output the voice recognition result;  and
wherein the 2D frequency-time array is a 2D spectrogram and programming instructions for generating the set of 2D arrays comprise programming instructions configured to, for each pixel in the 2D spectrogram: determine a significant fraction part and an
exponent part from the value of each pixel;  use the significant fraction part to determine a first subset of 2D arrays comprising a first number of 2D arrays;  and use the exponent part to determine a second subset of 2D arrays comprising a second
number of 2D arrays;  wherein a sum of the first and second numbers is the number of arrays in the set of 2D arrays.
 16.  The system of claim 15, wherein programming instructions for determining the first subset of 2D arrays comprise programming instructions configured to: determine the first subset of 2D arrays so that the significant fraction part for each
pixel in the 2D spectrogram is approximate to an average of the values of corresponding pixels across all of the 2D arrays in the first subset.
 17.  The system of claim 16, wherein programming instructions for determining the first subset of 2D arrays comprise programming instructions configured to, for each pixel in the 2D spectrogram: determine an integer part and a fraction part from
the significant fraction part of the value of each pixel;  use the fraction part to determine a value of zero or one for a corresponding pixel in each of the first subset of 2D arrays, wherein an average value of corresponding pixels in the first subset
of 2D arrays is approximate to the fraction part;  and update the values for the corresponding pixels in each of the first subset of 2D arrays by adding thereto the integer part.
 18.  The system of claim 15, further comprising programming instructions configured to take a log of the value of each pixel in the 2D spectrogram before determining the significant fraction part and the exponent part from the value of each
pixel.  "
"10,356,499","
     July 16, 2019
","Artificial intelligence sound output apparatus, hub for communication
     network, method of manufacturing the apparatus, and grille for the
     apparatus
"," Disclosed is an artificial intelligence hub including: a main body having
     a speaker for outputting sound; a communication module for wirelessly
     communicating with a surround ding device; a base arranged below the main
     body to support the main body; a cover coupled to a top of the main body,
     the cover having an upper surface for displaying a screen based on
     information exchanged via the communication module; and a grille having a
     vertically elongated cylindrical shape and provided with a plurality of
     through-holes therein, the main body being arranged within the grille,
     the grille being coupled at an upper end thereof to the cover housing and
     coupled at a lower end thereof to the base.
",G06F 3/167 (20130101); H04R 1/025 (20130101); H04R 1/023 (20130101); G06F 3/165 (20130101); H04R 2420/07 (20130101); H04R 2430/01 (20130101); H04R 31/00 (20130101); G10L 15/00 (20130101); H04R 1/24 (20130101); H04R 2201/029 (20130101),H04R 1/02 (20060101); G06F 3/16 (20060101); G10L 15/00 (20130101),"[['\n3483945', '\nDecember 1969'], ['\n4974698', '\nDecember 1990'], ['\n5444198', '\nAugust 1995'], ['\n5504502', '\nApril 1996'], ['\n5652413', '\nJuly 1997'], ['\n6104819', '\nAugust 2000'], ['\n6186269', '\nFebruary 2001'], ['\n6790400', '\nSeptember 2004'], ['\n7429707', '\nSeptember 2008'], ['\nD632265', '\nFebruary 2011'], ['\n8003886', '\nAugust 2011'], ['\n8068618', '\nNovember 2011'], ['\nD687009', '\nJuly 2013'], ['\nD700904', '\nMarch 2014'], ['\n9036858', '\nMay 2015'], ['\n9131301', '\nSeptember 2015'], ['\nD743819', '\nNovember 2015'], ['\nD746166', '\nDecember 2015'], ['\nD747984', '\nJanuary 2016'], ['\n9280141', '\nMarch 2016'], ['\n9304736', '\nApril 2016'], ['\n9319795', '\nApril 2016'], ['\n9453655', '\nSeptember 2016'], ['\nD784963', '\nApril 2017'], ['\n9633197', '\nApril 2017'], ['\n9641919', '\nMay 2017'], ['\n9659577', '\nMay 2017'], ['\n9791839', '\nOctober 2017'], ['\n9820024', '\nNovember 2017'], ['\n9830005', '\nNovember 2017'], ['\n9843851', '\nDecember 2017'], ['\n9851728', '\nDecember 2017'], ['\n9857961', '\nJanuary 2018'], ['\n2006/0182299', '\nAugust 2006'], ['\n2006/0250762', '\nNovember 2006'], ['\n2011/0148812', '\nJune 2011'], ['\n2011/0303520', '\nDecember 2011'], ['\n2012/0035020', '\nFebruary 2012'], ['\n2012/0051015', '\nMarch 2012'], ['\n2013/0001971', '\nJanuary 2013'], ['\n2013/0338839', '\nDecember 2013'], ['\n2014/0049886', '\nFebruary 2014'], ['\n2014/0074283', '\nMarch 2014'], ['\n2014/0086431', '\nMarch 2014'], ['\n2014/0110237', '\nApril 2014'], ['\n2014/0139047', '\nMay 2014'], ['\n2014/0219491', '\nAugust 2014'], ['\n2014/0315620', '\nOctober 2014'], ['\n2015/0010194', '\nJanuary 2015'], ['\n2015/0104042', '\nApril 2015'], ['\n2015/0136572', '\nMay 2015'], ['\n2015/0201255', '\nJuly 2015'], ['\n2015/0264461', '\nSeptember 2015'], ['\n2015/0334480', '\nNovember 2015'], ['\n2015/0334859', '\nNovember 2015'], ['\n2016/0071440', '\nMarch 2016'], ['\n2016/0173977', '\nJune 2016'], ['\n2016/0259376', '\nSeptember 2016'], ['\n2016/0335423', '\nNovember 2016'], ['\n2016/0345086', '\nNovember 2016'], ['\n2017/0006374', '\nJanuary 2017'], ['\n2017/0163437', '\nJune 2017'], ['\n2017/0180850', '\nJune 2017'], ['\n2017/0242412', '\nAugust 2017'], ['\n2017/0259983', '\nSeptember 2017'], ['\n2017/0288447', '\nOctober 2017'], ['\n2017/0330429', '\nNovember 2017'], ['\n2017/0345420', '\nNovember 2017'], ['\n2018/0052218', '\nFebruary 2018']]",[0]," What is claimed is:  1.  A hub comprising: a main body including a housing defining a cavity to receive a speaker that generates sound;  communication circuitry coupled to the housing of the main
body, the communication circuitry exchanging data with another device;  a base coupled to a lower surface of the housing of the main body;  a cover including a cover housing, the cover housing including a lower surface coupled to a top of the housing of
the main body, and an upper surface to receive a display panel to output visual content associated with the data exchanged with the other device via the communication circuitry;  and a grille having a surface defining a cylinder with a central cavity, an
upper opening and a lower opening, the upper opening of the grille being coupled to the cover housing, the lower opening of the grill being coupled to the base, and the main body being positioned within the central cavity to be encapsulated by the base,
the cover, and the grille, the surface of the grille including a plurality of through-holes to pass the sound generated by the speaker, wherein: an upper surface of the cover housing defines an opening;  the display panel is positioned in the opening of
the cover housing;  and a window is positioned over the display panel and in the opening to cover the display panel, and wherein the cover further includes: a display Printed Circuit Board (PCB) provided in the cover housing and having an upper surface
on which the display panel is installed;  and a window support provided below the window to support the window, wherein the window support includes: a window support plate provided above the display panel to support the window and having an opening to
expose the display panel;  and at least one support boss extending downward and away from the window support plate and coupled to the cover housing.
 2.  The hub according to claim 1, wherein the upper surface of the cover housing inclines the window and the display panel with respect to a horizontal plane.
 3.  The hub according to claim 2, wherein the window has a circular shape.
 4.  The hub according to claim 3, wherein a horizontal cross section of the grille is an ellipse.
 5.  The hub according to claim 1, wherein the opening in the window support plate has a rectangular shape with a left-to-right length that is longer than a front-to-rear length, and wherein the at least one support boss is included in a
plurality of support bosses, one of the support bosses is formed in a first area at a rear side of the opening in the window support plate, and another of the support bosses is formed in a second area at a front side of the opening in the window support
plate.
 6.  The hub according to claim 5, wherein the display PCB has at least one through-hole through which at least one of the support bosses extends.
 7.  The hub according to claim 1, further comprising a volume switch positioned on the display PCB to a volume of the speaker, wherein the cover housing includes: an opening formed in a sidewall thereof;  and a volume button positioned in the
opening in the sidewall, and wherein the volume button is activated in response to a force applied to the volume switch.
 8.  The hub according to claim 1, wherein the cover includes an inclined window over the display panel, and wherein the grille has a horizontal cross-sectional shape corresponding to a shape associated with orthogonally projecting the window on
the horizontal plane.
 9.  The hub according to claim 8, wherein the window has a circular shape.
 10.  The hub according to claim 1, wherein the base includes: a base body having an open upper surface, the base body defining a predetermined space therein;  and a support rubber fixed on a bottom surface of the base body.
 11.  The hub according to claim 10, wherein the base body includes: a bottom surface that includes an insertion groove to receive a support pad;  a base outer wall extending upward from a periphery of the bottom, the base outer wall extending
below the grille;  and a lower-end holding extension upwardly extending from the base outer wall and inserted into the grille, the lower-end holding extension contacting an inner surface of the lower opening of the grille.
 12.  The hub according to claim 1, wherein the communication circuitry includes WiFi.RTM.  circuitry and Bluetooth.RTM.  circuitry, wherein the WiFi.RTM.  circuitry and the Bluetooth.RTM.  circuitry are coupled to the main body, and wherein
antennas for the WiFi.RTM.  circuitry and the Bluetooth.RTM.  circuitry are positioned in the cover above the grille.
 13.  The hub according to claim 1, wherein the communication circuitry includes a Zigbee.RTM.  circuitry.
 14.  A hub comprising: a main body including a housing defining a cavity to receive a speaker that generates sound;  communication circuitry coupled to the housing of the main body, the communication circuitry exchanging data with another
device;  a base coupled to a lower surface of the housing of the main body;  a cover including a cover housing, a cover housing including a lower surface coupled to a top of the housing of the main body, and an upper surface to receive a display panel to
output visual content associated with the data exchanged with the other device via the communication circuitry;  and a grille having a surface defining a cylinder with a central cavity, an upper opening and a lower opening, the upper opening of the
grille being coupled to the cover housing, the lower opening of the grill being coupled to the base, and the main body being positioned within the central cavity to be encapsulated by the base, the cover, and the grille, the surface of the grille
including a plurality of through-holes to pass the sound generated by the speaker, wherein an upper surface of the cover housing defines an opening, the display panel is positioned in the opening of the cover housing, and a window is positioned over the
display panel and in the opening to cover the display panel, wherein the cover further includes: a display Printed Circuit Board (PCB) provided in the cover housing and having an upper surface on which the display panel is installed;  and a window
support provided below the window to support the window, wherein the window support includes: a window support plate provided above the display panel to support the window and having an opening to expose the display panel;  and at least one support boss
extending downward and away from the window support plate and coupled to the cover housing, wherein the cover housing further includes: a cylindrical sidewall extending in a vertical direction;  and a partition extending from an inner circumferential
surface of the sidewall, the partition dividing an inside of the sidewall into upper and lower regions, wherein the display PCB is provided at an upper surface of the partition within an upper region of the cover housing, and wherein the partition
includes at least one insertion boss to receive the at least one support boss, respectively.
 15.  The hub according to claim 14, wherein the window support plate inclines the window and the partition extends parallel to the inclined window.
 16.  A hub comprising: a main body including a housing defining a cavity to receive a speaker that generates sound;  communication circuitry coupled to the housing of the main body, the communication circuitry exchanging data with another
device;  a base coupled to a lower surface of the housing of the main body;  a cover including a cover housing, the cover housing including a lower surface coupled to a top of the housing of the main body, and an upper surface to receive a display panel
to output visual content associated with the data exchanged with the other device via the communication circuitry;  and a grille having a surface defining a cylinder with a central cavity, an upper opening and a lower opening, the upper opening of the
grille being coupled to the cover housing, the lower opening of the grill being coupled to the base, and the main body being positioned within the central cavity to be encapsulated by the base, the cover, and the grille, the surface of the grille
including a plurality of through-holes to pass the sound generated by the speaker, wherein: an upper surface of the cover housing defines an opening;  the display panel is positioned in the opening of the cover housing;  and a window is positioned over
the display panel and in the opening to cover the display panel, wherein the cover further includes: a display Printed Circuit Board (PCB) provided in the cover housing and having an upper surface on which the display panel is installed;  and a window
support provided below the window to support the window, wherein the window support includes: a window support plate provided above the display panel to support the window and having an opening to expose the display panel;  and at least one support boss
extending downward and away from the window support plate and coupled to the cover housing, wherein contact switches are provided on the upper surface of the display PCB, wherein the window support includes protrusions protruding downward from the window
support plate at respective positions corresponding to the contact switches, and wherein pressure applied on the window support plate through the window activates one or more of the contact switches.
 17.  A hub comprising: a main body including a housing defining a cavity to receive a speaker that generates sound;  communication circuitry coupled to the housing of the main body, the communication circuitry exchanging data with another
device;  a base coupled to a lower surface of the housing of the main body;  a cover including a cover housing, the cover housing including a lower surface coupled to a top of the housing of the main body, and an upper surface to receive a display panel
to output visual content associated with the data exchanged with the other device via the communication circuitry;  and a grille having a surface defining a cylinder with a central cavity, an upper opening and a lower opening, the upper opening of the
grille being coupled to the cover housing, the lower opening of the grill being coupled to the base, and the main body being positioned within the central cavity to be encapsulated by the base, the cover, and the grille, the surface of the grille
including a plurality of through-holes to pass the sound generated by the speaker, wherein: an upper surface of the cover housing defines an opening;  the display panel is positioned in the opening of the cover housing;  and a window is positioned over
the display panel and in the opening to cover the display panel, wherein the upper surface of the cover housing inclines the window and the display panel with respect to a horizontal plane, wherein the window has a circular shape, wherein a horizontal
cross section of the grille is an ellipse, wherein the cover housing includes at least one fastening boss extending in a lateral direction from the upper-end holding extension and coupled to the main body, wherein the main body includes a speaker case to
receive the speaker therein, wherein the speaker case has at least one boss-insertion recess for insertion of the at least one fastening boss, wherein the speaker case includes: a front case and a rear case that combine to define the cavity, wherein the
at least one fastening boss is included in a plurality of fastening bosses, and one of the fastening bosses protrudes rearward from a front surface of the upper-end holding extension, wherein another one of the fastening bosses protrudes forward from a
rear surface of the upper-end holding extension, wherein at least one of the boss-insertion recesses is indented rearward from a front surface of the front case, and the front case has an opening formed in an upper surface thereof so that the fastening
boss, protruding rearward from the front surface of the upper-end holding extension, is inserted into the boss-insertion recess from a top side, and wherein at least another one of the boss-insertion recesses is indented forward from a rear surface of
the rear case, and the rear case has an opening formed in an upper surface thereof so that the fastening boss, protruding forward from the rear surface of the upper-end holding extension, is inserted into the boss-insertion recess from a top side.
 18.  The hub according to claim 17, wherein the speaker case includes at least one fastening boss protruding downward from a lower surface thereof, and wherein the base includes at least one insertion boss protruding upward from an inner surface
thereof for insertion of the respective at least one fastening boss.
 19.  The hub according to claim 17, wherein the cover further includes a display PCB provided in the cover housing and having an upper surface to support the display panel, and wherein the hub further comprises a main PCB positioned between a
lower surface of the speaker case and the base and electronically connected to a display PCB.  "
"10,360,214","
     July 23, 2019
","Ensuring reproducibility in an artificial intelligence infrastructure
"," Ensuring reproducibility in an artificial intelligence infrastructure
     that includes one or more storage systems and one or more graphical
     processing unit (`GPU`) servers, including: identifying, by a unified
     management plane, one or more transformations applied to a dataset by the
     artificial intelligence infrastructure, wherein applying the one or more
     transformations to the dataset causes the artificial intelligence
     infrastructure to generate a transformed dataset; storing, within the one
     or more storage systems, information describing the dataset, the one or
     more transformations applied to the dataset, and the transformed dataset;
     identifying, by the unified management plane, one or more machine
     learning models executed by the artificial intelligence infrastructure
     using the transformed dataset as input; and storing, within the one or
     more storage systems, information describing one or more machine learning
     models executed using the transformed dataset as input.
",G06F 3/061 (20130101); G06F 3/0647 (20130101); G06T 1/60 (20130101); G06N 3/08 (20130101); G06F 3/0629 (20130101); G06F 16/2255 (20190101); G06F 16/24534 (20190101); G06T 1/20 (20130101); G06N 20/00 (20190101); G06F 3/06 (20130101); G06T 2200/28 (20130101),G06F 16/00 (20190101); G06T 1/60 (20060101); G06N 20/00 (20190101); G06N 3/08 (20060101); G06T 1/20 (20060101); G06F 3/06 (20060101); G06F 16/2453 (20190101); G06F 16/22 (20190101),"[['\n5706210', '\nJanuary 1998'], ['\n5799200', '\nAugust 1998'], ['\n5933598', '\nAugust 1999'], ['\n6012032', '\nJanuary 2000'], ['\n6085333', '\nJuly 2000'], ['\n6643641', '\nNovember 2003'], ['\n6647514', '\nNovember 2003'], ['\n6789162', '\nSeptember 2004'], ['\n7089272', '\nAugust 2006'], ['\n7107389', '\nSeptember 2006'], ['\n7146521', '\nDecember 2006'], ['\n7334124', '\nFebruary 2008'], ['\n7437530', '\nOctober 2008'], ['\n7493424', '\nFebruary 2009'], ['\n7669029', '\nFebruary 2010'], ['\n7689609', '\nMarch 2010'], ['\n7743003', '\nJune 2010'], ['\n7743191', '\nJune 2010'], ['\n7899780', '\nMarch 2011'], ['\n8042163', '\nOctober 2011'], ['\n8086585', '\nDecember 2011'], ['\n8200887', '\nJune 2012'], ['\n8271700', '\nSeptember 2012'], ['\n8387136', '\nFebruary 2013'], ['\n8437189', '\nMay 2013'], ['\n8465332', '\nJune 2013'], ['\n8527544', '\nSeptember 2013'], ['\n8566546', '\nOctober 2013'], ['\n8578442', '\nNovember 2013'], ['\n8613066', '\nDecember 2013'], ['\n8620970', '\nDecember 2013'], ['\n8751463', '\nJune 2014'], ['\n8762642', '\nJune 2014'], ['\n8769622', '\nJuly 2014'], ['\n8800009', '\nAugust 2014'], ['\n8812860', '\nAugust 2014'], ['\n8850546', '\nSeptember 2014'], ['\n8898346', '\nNovember 2014'], ['\n8909854', '\nDecember 2014'], ['\n8931041', '\nJanuary 2015'], ['\n8949863', '\nFebruary 2015'], ['\n8984602', '\nMarch 2015'], ['\n8990905', '\nMarch 2015'], ['\n9081713', '\nJuly 2015'], ['\n9124569', '\nSeptember 2015'], ['\n9134922', '\nSeptember 2015'], ['\n9189334', '\nNovember 2015'], ['\n9209973', '\nDecember 2015'], ['\n9250823', '\nFebruary 2016'], ['\n9300660', '\nMarch 2016'], ['\n9311182', '\nApril 2016'], ['\n9444822', '\nSeptember 2016'], ['\n9507532', '\nNovember 2016'], ['\n9632870', '\nApril 2017'], ['\n2002/0013802', '\nJanuary 2002'], ['\n2003/0145172', '\nJuly 2003'], ['\n2003/0191783', '\nOctober 2003'], ['\n2003/0225961', '\nDecember 2003'], ['\n2004/0080985', '\nApril 2004'], ['\n2004/0111573', '\nJune 2004'], ['\n2004/0153844', '\nAugust 2004'], ['\n2004/0193814', '\nSeptember 2004'], ['\n2004/0260967', '\nDecember 2004'], ['\n2005/0160416', '\nJuly 2005'], ['\n2005/0188246', '\nAugust 2005'], ['\n2005/0216800', '\nSeptember 2005'], ['\n2006/0015771', '\nJanuary 2006'], ['\n2006/0129817', '\nJune 2006'], ['\n2006/0161726', '\nJuly 2006'], ['\n2006/0230245', '\nOctober 2006'], ['\n2006/0239075', '\nOctober 2006'], ['\n2007/0022227', '\nJanuary 2007'], ['\n2007/0028068', '\nFebruary 2007'], ['\n2007/0055702', '\nMarch 2007'], ['\n2007/0109856', '\nMay 2007'], ['\n2007/0150689', '\nJune 2007'], ['\n2007/0168321', '\nJuly 2007'], ['\n2007/0220227', '\nSeptember 2007'], ['\n2007/0294563', '\nDecember 2007'], ['\n2007/0294564', '\nDecember 2007'], ['\n2008/0005587', '\nJanuary 2008'], ['\n2008/0077825', '\nMarch 2008'], ['\n2008/0162674', '\nJuly 2008'], ['\n2008/0195833', '\nAugust 2008'], ['\n2008/0270678', '\nOctober 2008'], ['\n2008/0282045', '\nNovember 2008'], ['\n2009/0077340', '\nMarch 2009'], ['\n2009/0100115', '\nApril 2009'], ['\n2009/0198889', '\nAugust 2009'], ['\n2010/0052625', '\nMarch 2010'], ['\n2010/0211723', '\nAugust 2010'], ['\n2010/0246266', '\nSeptember 2010'], ['\n2010/0257142', '\nOctober 2010'], ['\n2010/0262764', '\nOctober 2010'], ['\n2010/0325345', '\nDecember 2010'], ['\n2010/0332754', '\nDecember 2010'], ['\n2011/0072290', '\nMarch 2011'], ['\n2011/0125955', '\nMay 2011'], ['\n2011/0131231', '\nJune 2011'], ['\n2011/0167221', '\nJuly 2011'], ['\n2012/0023144', '\nJanuary 2012'], ['\n2012/0054264', '\nMarch 2012'], ['\n2012/0079318', '\nMarch 2012'], ['\n2012/0131253', '\nMay 2012'], ['\n2012/0303919', '\nNovember 2012'], ['\n2012/0311000', '\nDecember 2012'], ['\n2013/0007845', '\nJanuary 2013'], ['\n2013/0031414', '\nJanuary 2013'], ['\n2013/0036272', '\nFebruary 2013'], ['\n2013/0071087', '\nMarch 2013'], ['\n2013/0145447', '\nJune 2013'], ['\n2013/0191555', '\nJuly 2013'], ['\n2013/0198459', '\nAugust 2013'], ['\n2013/0205173', '\nAugust 2013'], ['\n2013/0219164', '\nAugust 2013'], ['\n2013/0227201', '\nAugust 2013'], ['\n2013/0290607', '\nOctober 2013'], ['\n2013/0311434', '\nNovember 2013'], ['\n2013/0318297', '\nNovember 2013'], ['\n2013/0332614', '\nDecember 2013'], ['\n2014/0020083', '\nJanuary 2014'], ['\n2014/0074850', '\nMarch 2014'], ['\n2014/0082715', '\nMarch 2014'], ['\n2014/0086146', '\nMarch 2014'], ['\n2014/0090009', '\nMarch 2014'], ['\n2014/0096220', '\nApril 2014'], ['\n2014/0101434', '\nApril 2014'], ['\n2014/0164774', '\nJune 2014'], ['\n2014/0173232', '\nJune 2014'], ['\n2014/0195636', '\nJuly 2014'], ['\n2014/0201512', '\nJuly 2014'], ['\n2014/0201541', '\nJuly 2014'], ['\n2014/0208155', '\nJuly 2014'], ['\n2014/0215590', '\nJuly 2014'], ['\n2014/0229654', '\nAugust 2014'], ['\n2014/0230017', '\nAugust 2014'], ['\n2014/0258526', '\nSeptember 2014'], ['\n2014/0282983', '\nSeptember 2014'], ['\n2014/0285917', '\nSeptember 2014'], ['\n2014/0325262', '\nOctober 2014'], ['\n2014/0351627', '\nNovember 2014'], ['\n2014/0373104', '\nDecember 2014'], ['\n2014/0373126', '\nDecember 2014'], ['\n2015/0026387', '\nJanuary 2015'], ['\n2015/0074463', '\nMarch 2015'], ['\n2015/0089569', '\nMarch 2015'], ['\n2015/0095515', '\nApril 2015'], ['\n2015/0113203', '\nApril 2015'], ['\n2015/0121137', '\nApril 2015'], ['\n2015/0134920', '\nMay 2015'], ['\n2015/0149822', '\nMay 2015'], ['\n2015/0193169', '\nJuly 2015'], ['\n2015/0378888', '\nDecember 2015'], ['\n2016/0071017', '\nMarch 2016'], ['\n2016/0098323', '\nApril 2016'], ['\n2016/0350009', '\nDecember 2016'], ['\n2016/0352720', '\nDecember 2016'], ['\n2016/0352830', '\nDecember 2016'], ['\n2016/0352834', '\nDecember 2016'], ['\n2017/0091670', '\nMarch 2017'], ['\n2017/0220949', '\nAugust 2017'], ['\n2017/0344910', '\nNovember 2017'], ['\n2018/0189674', '\nJuly 2018']]","[2, '11,347,613', '11,093,666']"," What is claimed is:  1.  A method of ensuring reproducibility in an artificial intelligence infrastructure that includes one or more storage systems and one or more graphical processing unit
(Gal) servers, the method comprising: identifying, by a unified management plane, one or more transformations applied to a dataset by the artificial intelligence infrastructure, wherein applying the one or more transformations to the dataset causes the
artificial intelligence infrastructure to generate a transformed dataset;  storing, within the one or more storage systems, information describing the dataset, the one or more transformations applied to the dataset, and the transformed dataset; 
identifying, by the unified management plane, one or more machine learning models executed by the artificial intelligence infrastructure using the transformed dataset as input;  storing, within the one or more storage systems, information describing one
or more machine learning models executed using the transformed dataset as input;  determining, by the artificial intelligence infrastructure, whether data related to a previously executed machine learning model should be tiered off of the one or more
storage systems;  and responsive to determining that the data related to the previously executed machine learning model should be tiered off of the one or more storage systems: storing the data related to the previously executed machine learning model in
lower-tier storage;  and removing, from the one or more storage systems, the data related to the previously executed machine learning model.
 2.  The method of claim 1 wherein storing, within the one or more storage systems, information describing the dataset, the one or more transformations applied to the dataset, and the transformed dataset further comprises: generating, by the
artificial intelligence infrastructure applying a predetermined hash function to the dataset, the one or more transformations applied to the dataset, and the transformed dataset, a hash value;  and storing, within the one or more storage systems, the
hash value.
 3.  The method of claim 1 wherein storing, within the one or more storage systems, information describing one or more machine learning models executed using the transformed dataset as input further comprises: generating, by the artificial
intelligence infrastructure applying a predetermined hash function to the one or more machine learning models and the transformed dataset, a hash value;  and storing, within the one or more storage systems, the hash value.
 4.  The method of claim 1 further comprising: identifying, by the unified management plane, differences between a machine learning model and a machine learning model previously executed by the artificial intelligence infrastructure;  and
storing, within the one or more storage systems, only the portion of the machine learning model that differs from the machine learning models previously executed by the artificial intelligence infrastructure.
 5.  The method of claim 1 further comprising identifying, from amongst a plurality of machine learning models, a preferred machine learning model.
 6.  The method of claim 1 further comprising tracking the improvement of a particular machine learning model over time.
 7.  An artificial intelligence infrastructure that includes one or more storage systems and one or more graphical processing unit (GMT) servers, the artificial intelligence infrastructure configured to carry out the steps of: identifying, by a
unified management plane, one or more transformations applied to a dataset by the artificial intelligence infrastructure, wherein applying the one or more transformations to the dataset causes the artificial intelligence infrastructure to generate a
transformed dataset;  storing, within the one or more storage systems, information describing the dataset, the one or more transformations applied to the dataset, and the transformed dataset;  identifying, by the unified management plane, one or more
machine learning models executed by the artificial intelligence infrastructure using the transformed dataset as input;  storing, within the one or more storage systems, information describing one or more machine learning models executed using the
transformed dataset as input;  determining, by the artificial intelligence infrastructure, whether data related to a previously executed machine learning model should be tiered off of the one or more storage systems;  and responsive to determining that
the data related to the previously executed machine learning model should be tiered off of the one or more storage systems: storing the data related to the previously executed machine learning model in lower-tier storage;  and removing, from the one or
more storage systems, the data related to the previously executed machine learning model.
 8.  The artificial intelligence infrastructure of claim 7 wherein storing, within the one or more storage systems, information describing the dataset, the one or more transformations applied to the dataset, and the transformed dataset further
comprises: generating, by the artificial intelligence infrastructure applying a predetermined hash function to the one or more transformations applied to the dataset and the transformed dataset, a hash value;  and storing, within the one or more storage
systems, the hash value.
 9.  The artificial intelligence infrastructure of claim 7 wherein storing, within the one or more storage systems, information describing one or more machine learning models executed using the transformed dataset as input further comprises:
generating, by the artificial intelligence infrastructure applying a predetermined hash function to the one or more machine learning models, a hash value;  and storing, within the one or more storage systems, the hash value.
 10.  The artificial intelligence infrastructure of claim 7 wherein the artificial intelligence infrastructure is further configured to carry out the steps of: identifying, by the unified management plane, differences between a machine learning
model and a machine learning model previously executed by the artificial intelligence infrastructure;  and storing, within the one or more storage systems, only the portion of the machine learning model that differs from the machine learning models
previously executed by the artificial intelligence infrastructure.
 11.  The artificial intelligence infrastructure of claim 7 wherein the artificial intelligence infrastructure is further configured to carry out the step of identifying, from amongst a plurality of machine learning models, a preferred machine
learning model.
 12.  The artificial intelligence infrastructure of claim 7 wherein the artificial intelligence infrastructure is further configured to carry out the step of tracking the improvement of a particular machine learning model over time.
 13.  An apparatus for ensuring reproducibility in an artificial intelligence infrastructure that includes one or more storage systems and one or more graphical processing unit (Gal) servers, the apparatus comprising a computer processor, a
computer memory operatively coupled to the computer processor, the computer memory having disposed within it computer program instructions that, when executed by the computer processor, cause the apparatus to carry out the steps of: identifying, by a
unified management plane, one or more transformations applied to a dataset by the artificial intelligence infrastructure, wherein applying the one or more transformations to the dataset causes the artificial intelligence infrastructure to generate a
transformed dataset;  storing, within the one or more storage systems, information describing the dataset, the one or more transformations applied to the dataset, and the transformed dataset;  identifying, by the unified management plane, one or more
machine learning models executed by the artificial intelligence infrastructure using the transformed dataset as input;  storing, within the one or more storage systems, information describing one or more machine learning models executed using the
transformed dataset as input;  determining, by the artificial intelligence infrastructure, whether data related to a previously executed machine learning model should be tiered off of the one or more storage systems;  and responsive to determining that
the data related to the previously executed machine learning model should be tiered off of the one or more storage systems: storing the data related to the previously executed machine learning model in lower-tier storage;  and removing, from the one or
more storage systems, the data related to the previously executed machine learning model.
 14.  The apparatus of claim 13 further comprising computer program instructions that, when executed by the computer processor, cause the apparatus to carry out the steps of: identifying, by the unified management plane, differences between a
machine learning model and a machine learning model previously executed by the artificial intelligence infrastructure;  and storing, within the one or more storage systems, only the portion of the machine learning model that differs from the machine
learning models previously executed by the artificial intelligence infrastructure.
 15.  The apparatus of claim 13 further comprising computer program instructions that, when executed by the computer processor, cause the apparatus to carry out the step of identifying, from amongst a plurality of machine learning models, a
preferred machine learning model.
 16.  The apparatus of claim 13 further comprising computer program instructions that, when executed by the computer processor, cause the apparatus to carry out the step of tracking the improvement of a particular machine learning model over
time.
 17.  The apparatus of claim 13 further comprising computer program instructions that, when executed by the computer processor, cause the apparatus to carry out the steps of: generating, by the artificial intelligence infrastructure applying a
predetermined hash function to the one or more machine learning models, a hash value;  and storing, within the one or more storage systems, the hash value.  "
